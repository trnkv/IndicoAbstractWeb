<?xml version="1.0" encoding="utf-8"?>
<AbstractBook>
<Conference>
The 8th International Conference "Distributed Computing and Grid-technologies in Science and Education" (GRID 2018)
</Conference>

<abstract>
<Id>180</Id>
<Title>
Грид и облачная инфраструктура дата-центра Института Физики НАН Азербайджана
</Title>
<Content>
Основными направлениями развития дата-центра Института физики НАН Азербайджана являются грид и облачные технологии. Пользуясь тем что грид-сегмент дата-центра интегрирован в инфраструктуру EGI/WLCG в качестве грид-сайта уровня Tier3, пользователи дата-центра получают возможность участвовать в таких международных проектах как ATLAS (CERN). Сотрудничество с международными научными центрами ОИЯИ, ЦЕРН в области информационных технологий способствует эффективному развитию дата центра, а также помогает решать задачи пользователей в таких научных областях, как физика высоких энергий, физика твердого тела и т. д.
</Content>
<field id="content">
Основными направлениями развития дата-центра Института физики НАН Азербайджана являются грид и облачные технологии. Пользуясь тем что грид-сегмент дата-центра интегрирован в инфраструктуру EGI/WLCG в качестве грид-сайта уровня Tier3, пользователи дата-центра получают возможность участвовать в таких международных проектах как ATLAS (CERN). Сотрудничество с международными научными центрами ОИЯИ, ЦЕРН в области информационных технологий способствует эффективному развитию дата центра, а также помогает решать задачи пользователей в таких научных областях, как физика высоких энергий, физика твердого тела и т. д.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>Aleksey</FirstName>
    <FamilyName>Bondyakov</FamilyName>
    <Email>aleksey@jinr.ru</Email>
    <Affiliation>JINR (Joint Institute For Nuclear Research)</Affiliation>
</PrimaryAuthor>
<Speaker>
    <FirstName>Aleksey</FirstName>
    <FamilyName>Bondyakov</FamilyName>
    <Email>aleksey@jinr.ru</Email>
    <Affiliation>JINR (Joint Institute For Nuclear Research)</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track></Track>
</abstract>

<abstract>
<Id>181</Id>
<Title>
Clustering methods for energy consumption forecasting in smart grids
</Title>
<Content>
Clustering is a well-known machine learning algorithm which enables the determination of underlying groups in datasets. In electric power systems it has been traditionally utilized for different purposes like defining consumer individual profiles, tariff designs and improving load forecasting. A new age in power systems structure such as smart grids determined the wide investigations op applications and benefits of clustering methods for smart meter data analysis. This paper presents an improvement of energy consumption forecasting methods by performing cluster analysis. For clustering the centroid based method K-means with K-means++ centroids was used. Various forecasting methods were applied to find the most effective ones with clustering procedure application. Used smart meter data have an hourly measurements of energy consumption time series of Russian central region customers. In our computer modeling investigations we have obtained significant improvements due to carrying out the cluster analysis for consumption forecasting.
</Content>
<field id="content">
Clustering is a well-known machine learning algorithm which enables the determination of underlying groups in datasets. In electric power systems it has been traditionally utilized for different purposes like defining consumer individual profiles, tariff designs and improving load forecasting. A new age in power systems structure such as smart grids determined the wide investigations op applications and benefits of clustering methods for smart meter data analysis. This paper presents an improvement of energy consumption forecasting methods by performing cluster analysis. For clustering the centroid based method K-means with K-means++ centroids was used. Various forecasting methods were applied to find the most effective ones with clustering procedure application. Used smart meter data have an hourly measurements of energy consumption time series of Russian central region customers. In our computer modeling investigations we have obtained significant improvements due to carrying out the cluster analysis for consumption forecasting.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>eugene</FirstName>
    <FamilyName>shchetinin</FamilyName>
    <Email>riviera-molto@mail.ru</Email>
    <Affiliation>fa</Affiliation>
</PrimaryAuthor>
<Speaker>
    <FirstName>eugene</FirstName>
    <FamilyName>shchetinin</FamilyName>
    <Email>riviera-molto@mail.ru</Email>
    <Affiliation>fa</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>

<abstract>
<Id>182</Id>
<Title>
Different Approaches for Elastic Imaging using Multiprocessor Computing Systems
</Title>
<Content>
At present, oil and natural gas form the basis of energy throughout the world. In view of significant depletion of reserves, the task of prospecting and exploration of new deposits is becoming increasingly important. In the industry the specific migration procedure is used to find contrast interfaces between geological layers with different properties. It should be noted that algorithms developed to date are constructed in the acoustic approximation of the medium, which leads to defects in migration images. In particular, subvertical boundaries are practically not restored. Prospective in terms of overcoming these shortcomings is the transition to a full elastic formulation of the tasks of seismic survey process. Despite rising computational complexity of the problem the goal can be achieved with the usage of modern HPC systems. The work is the continuation of the previously reported research about the usage of Born approximation as a new elastic imaging method. In this study different fundamental approaches were used. Born approximation and Kirchhoff approach were adopted to 2D and 3D elastic problems in the case of homogeneous background medium. The research software in C++/Mathematica was developed, and a set of calculations for simple geological models were carried out using multicore shared memory system. The assessment of the scalability shows high effectiveness. The research was supported by the grant of the President of the Russian Federation No. MK- 1831.2017.9.
</Content>
<field id="content">
At present, oil and natural gas form the basis of energy throughout the world. In view of significant depletion of reserves, the task of prospecting and exploration of new deposits is becoming increasingly important. In the industry the specific migration procedure is used to find contrast interfaces between geological layers with different properties. It should be noted that algorithms developed to date are constructed in the acoustic approximation of the medium, which leads to defects in migration images. In particular, subvertical boundaries are practically not restored. Prospective in terms of overcoming these shortcomings is the transition to a full elastic formulation of the tasks of seismic survey process. Despite rising computational complexity of the problem the goal can be achieved with the usage of modern HPC systems. The work is the continuation of the previously reported research about the usage of Born approximation as a new elastic imaging method. In this study different fundamental approaches were used. Born approximation and Kirchhoff approach were adopted to 2D and 3D elastic problems in the case of homogeneous background medium. The research software in C++/Mathematica was developed, and a set of calculations for simple geological models were carried out using multicore shared memory system. The assessment of the scalability shows high effectiveness. The research was supported by the grant of the President of the Russian Federation No. MK- 1831.2017.9.
</field>
<field id="summary">
The research software based on the new elastic imaging methods using different approaches: Born approximation, Kirchhoff approximation and full-wave simulation method was developed. The assessment of the scalability on multi-cores shared memory system shows approximately 90 % of effectiveness.
</field>
<PrimaryAuthor>
    <FirstName>Vasily</FirstName>
    <FamilyName>Golubev</FamilyName>
    <Email>w.golubev@mail.ru</Email>
    <Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</PrimaryAuthor>
<Co-Author>
    <FirstName>Alena</FirstName>
    <FamilyName>Favorskaya</FamilyName>
    <Email>aleanera@yandex.ru</Email>
    <Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</Co-Author>
<Speaker>
    <FirstName>Vasily</FirstName>
    <FamilyName>Golubev</FamilyName>
    <Email>w.golubev@mail.ru</Email>
    <Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>

<abstract>
<Id>183</Id>
<Title>
Clouds of JINR, University of Sofia and INRNE - current state of the project
</Title>
<Content>
JINR established a cloud based on OpenNebula. It is open for integration with the clouds from the member states. The paper presents current (first year) state of the 3 years project that aims to create a cloud backbone in Bulgaria. University of Sofia and INRNE participate in that initiative. This is a target project funded by JINR based on the research plan of the institute.
</Content>
<field id="content">
JINR established a cloud based on OpenNebula. It is open for integration with the clouds from the member states. The paper presents current (first year) state of the 3 years project that aims to create a cloud backbone in Bulgaria. University of Sofia and INRNE participate in that initiative. This is a target project funded by JINR based on the research plan of the institute.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Dimitrov</FamilyName>
    <Email>cht@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia</Affiliation>
</PrimaryAuthor>
<Co-Author>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Korenkov</FamilyName>
    <Email>korenkov@jinr.ru</Email>
    <Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Nikita</FirstName>
    <FamilyName>Balashov</FamilyName>
    <Email>balashov.nikita@gmail.com</Email>
    <Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Nikolay</FirstName>
    <FamilyName>Kutovskiy</FamilyName>
    <Email>kut@jinr.ru</Email>
    <Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Kouzma</FirstName>
    <FamilyName>Kouzmov</FamilyName>
    <Email>kouzma@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Radoslava</FirstName>
    <FamilyName>Hristova</FamilyName>
    <Email>radoslava@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia / JINR</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Hristov</FirstName>
    <FamilyName>Svetoslav</FamilyName>
    <Email>stc@inrne.bas.bg</Email>
    <Affiliation>INRNE</Affiliation>
</Co-Author>
<Speaker>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Dimitrov</FamilyName>
    <Email>cht@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>

<abstract>
<Id>184</Id>
<Title>
Problems of date and time data types in relational model of data
</Title>
<Content>
Several years after the initial announcement of the relational model of data, Codd published a review on the model, so called Version 2. This review is based on the experience of relational database systems implementation in the intermediate period. One of the main corrections are recommendation on date and time data types. This paper reinvestigate the topic from the nowadays point of view.
</Content>
<field id="content">
Several years after the initial announcement of the relational model of data, Codd published a review on the model, so called Version 2. This review is based on the experience of relational database systems implementation in the intermediate period. One of the main corrections are recommendation on date and time data types. This paper reinvestigate the topic from the nowadays point of view.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Dimitrov</FamilyName>
    <Email>cht@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia</Affiliation>
</PrimaryAuthor>
<Speaker>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Dimitrov</FamilyName>
    <Email>cht@fmi.uni-sofia.bg</Email>
    <Affiliation>University of Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>

<abstract>
<Id>185</Id>
<Title>
Molecular dynamic simulation of water vapor interaction with various types of pores using hybrid computing structures
</Title>
<Content>
Theoretical and experimental investigations of water vapor interaction with porous materials are very needful for various fields of science and tech-nology. Not only studies of the interaction of water vapor and porous material as a continuous medium, but also the study of the interaction of water vapor with individual pore is very important in these researches. Mathematical mod-eling occupies an important place in these investigations.Conventional ap-proaches to solve problems of mathematical research of the processes of inter-action of water vapor with individual pore are the following. The first ap-proach is based on the use of diffusion equation for description of interaction of water vapor with a pore. It is so called macro approach.The second ap-proach is based on various particle methods like, for example, molecular dy-namics (MD). These methods essentially consider the microstructure of the in-vestigated system consisting of water vapor and a pore. This second approach can be called a micro approach. At the macro level, the influence of the arrangement structure of individ-ual pores on the processes of water vapor interaction with porous material as a continuous medium is studied. At the micro level, it is very interesting to in-vestigate the dependence of the characteristics of the water vapor interaction with porous media on the geometry and dimensions of the individual pore. Both approaches require the most efficient calculation methods as far as pos-sible with the current level of development of computational technologies. Us-age of efficient calculation methods is necessary because the degree of approx-imation for simulating system is largely determined by the dimensionality of the system of equation being solved at every time step. Number of time steps is also quite large. In this work, a study of efficiency of various implementations algorithms for MD simulation of water vapor interaction with individual pore is carried out. A great disadvantage of MD is its requirement of a relatively large compu-tational effort and long time in simulations. These problems can be drastically reduced by parallel calculations. In this work we investigate dependence of time required for simulations on different parameters, like number of particles in the system, shape of pores, and so on. The results of parallel calculations are compared with the results obtained by serial calculations. Keywords: porous media, molecular dynamics, macroscopic diffusion model, parallel calculations This work was supported by the JINR project No. 05-6-1118-2014/2019, protocol No. 4596-6-17/19, and used HybriLIT resources.
</Content>
<field id="content">
Theoretical and experimental investigations of water vapor interaction with porous materials are very needful for various fields of science and tech-nology. Not only studies of the interaction of water vapor and porous material as a continuous medium, but also the study of the interaction of water vapor with individual pore is very important in these researches. Mathematical mod-eling occupies an important place in these investigations.Conventional ap-proaches to solve problems of mathematical research of the processes of inter-action of water vapor with individual pore are the following. The first ap-proach is based on the use of diffusion equation for description of interaction of water vapor with a pore. It is so called macro approach.The second ap-proach is based on various particle methods like, for example, molecular dy-namics (MD). These methods essentially consider the microstructure of the in-vestigated system consisting of water vapor and a pore. This second approach can be called a micro approach. At the macro level, the influence of the arrangement structure of individ-ual pores on the processes of water vapor interaction with porous material as a continuous medium is studied. At the micro level, it is very interesting to in-vestigate the dependence of the characteristics of the water vapor interaction with porous media on the geometry and dimensions of the individual pore. Both approaches require the most efficient calculation methods as far as pos-sible with the current level of development of computational technologies. Us-age of efficient calculation methods is necessary because the degree of approx-imation for simulating system is largely determined by the dimensionality of the system of equation being solved at every time step. Number of time steps is also quite large. In this work, a study of efficiency of various implementations algorithms for MD simulation of water vapor interaction with individual pore is carried out. A great disadvantage of MD is its requirement of a relatively large compu-tational effort and long time in simulations. These problems can be drastically reduced by parallel calculations. In this work we investigate dependence of time required for simulations on different parameters, like number of particles in the system, shape of pores, and so on. The results of parallel calculations are compared with the results obtained by serial calculations. Keywords: porous media, molecular dynamics, macroscopic diffusion model, parallel calculations This work was supported by the JINR project No. 05-6-1118-2014/2019, protocol No. 4596-6-17/19, and used HybriLIT resources.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>Eduard</FirstName>
    <FamilyName>Nikonov</FamilyName>
    <Email>e.nikonov@jinr.ru</Email>
    <Affiliation>LIT JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
    <FirstName>Vladimir</FirstName>
    <FamilyName>Korenkov</FamilyName>
    <Email>korenkov@jinr.ru</Email>
    <Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
    <FirstName>Maria</FirstName>
    <FamilyName>Popovičová</FamilyName>
    <Email>maria.popovicova@unipo.sk</Email>
    <Affiliation>University of Prešov, Slovakia</Affiliation>
</Co-Author>
<Speaker>
    <FirstName>Eduard</FirstName>
    <FamilyName>Nikonov</FamilyName>
    <Email>e.nikonov@jinr.ru</Email>
    <Affiliation>LIT JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>

<abstract>
<Id>186</Id>
<Title>
Electronic, Dynamical and Thermodynamic Properties of DNA
</Title>
<Content>
The idea to use DNA molecule as a base element for nanobioelectronics is discussed. It could be cosidered as some molecular wire where a typical charge transfer/transport pattern can physically be viewed as a polaron and/or soliton which mobility can be very low. A computer experiment demonstrates that mobile breather excited near one of the ends of DNA can trap the polarons. The formed quasiparticle can move along the molecule for a long distance and does not require the electric field. The dynamics of charge migration was modeled to calculate temperature dependencies of its thermodynamic equilibrium values such as energy, electronic heat capacity and reaction constants for different nucleotide sequences. The mechanism of charge transfer for a long distance due to polaron melting is considered. Special attention is given to: dynamical behavior of electrons in regular polynucleotide chains, dynamics of polaron states formation in Peyrard – Bishop- Dauxois chain, polaron motion in an electric field, the role of dispersion, Bloch oscillations and breather states. The work was supported by RSF project 16-11-10163.
</Content>
<field id="content">
The idea to use DNA molecule as a base element for nanobioelectronics is discussed. It could be cosidered as some molecular wire where a typical charge transfer/transport pattern can physically be viewed as a polaron and/or soliton which mobility can be very low. A computer experiment demonstrates that mobile breather excited near one of the ends of DNA can trap the polarons. The formed quasiparticle can move along the molecule for a long distance and does not require the electric field. The dynamics of charge migration was modeled to calculate temperature dependencies of its thermodynamic equilibrium values such as energy, electronic heat capacity and reaction constants for different nucleotide sequences. The mechanism of charge transfer for a long distance due to polaron melting is considered. Special attention is given to: dynamical behavior of electrons in regular polynucleotide chains, dynamics of polaron states formation in Peyrard – Bishop- Dauxois chain, polaron motion in an electric field, the role of dispersion, Bloch oscillations and breather states. The work was supported by RSF project 16-11-10163.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Lakhno</FamilyName>
<Email>lak@impb.ru</Email>
<Affiliation>Institute of Mathematical Problems of Biology RAS, Keldysh Institute of Applied Mathematics of Russian Academy of Sciences</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Lakhno</FamilyName>
<Email>lak@impb.ru</Email>
<Affiliation>Institute of Mathematical Problems of Biology RAS, Keldysh Institute of Applied Mathematics of Russian Academy of Sciences</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
</abstract>
<abstract>
<Id>187</Id>
<Title>
IMPROVING THE EFFICIENCY OF SMART GRIDS OF ENERGY CONSUMPTION WITH APPLICATION OF SYSTEMS OF ARTIFICIAL INTELLECT
</Title>
<Content>
Clustering is a well-known machine learning algorithm which enables the determination of underlying groups in datasets. In electric power systems it has been traditionally utilized for different purposes like defining consumer individual profiles, tariff designs and improving load forecasting.A new age in power systems structure such as smart grids determined the wide investigations of applications and benefits of clustering methods for smart meter data analysis. This paper presents an improvement of energy consumption forecasting methods by performing cluster analysis. For clustering the centroid based method K-means with K-means++ centroids was used. Various forecasting methods were applied to find the most effective ones with clustering procedure application. Used smart meter data have an hourly measurements of energy consumption time series of russian central region customers. In our computer modeling investigations we have obtained significant improvement due to carrying out the cluster analysis for consumption forecasting.
</Content>
<field id="content">
Clustering is a well-known machine learning algorithm which enables the determination of underlying groups in datasets. In electric power systems it has been traditionally utilized for different purposes like defining consumer individual profiles, tariff designs and improving load forecasting.A new age in power systems structure such as smart grids determined the wide investigations of applications and benefits of clustering methods for smart meter data analysis. This paper presents an improvement of energy consumption forecasting methods by performing cluster analysis. For clustering the centroid based method K-means with K-means++ centroids was used. Various forecasting methods were applied to find the most effective ones with clustering procedure application. Used smart meter data have an hourly measurements of energy consumption time series of russian central region customers. In our computer modeling investigations we have obtained significant improvement due to carrying out the cluster analysis for consumption forecasting.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>eugene</FirstName>
<FamilyName>Shchetinin</FamilyName>
<Email>riviera-molto@mail.ru</Email>
<Affiliation>Financial University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikchail</FirstName>
<FamilyName>Berezhkov</FamilyName>
<Email>berezhkov@gmail.com</Email>
<Affiliation>Stankin</Affiliation>
</Co-Author>
<Speaker>
<FirstName>eugene</FirstName>
<FamilyName>Shchetinin</FamilyName>
<Email>riviera-molto@mail.ru</Email>
<Affiliation>Financial University</Affiliation>
</Speaker>
<Speaker>
<FirstName>Mikchail</FirstName>
<FamilyName>Berezhkov</FamilyName>
<Email>berezhkov@gmail.com</Email>
<Affiliation>Stankin</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>188</Id>
<Title>
О методах и технологиях интеллектуального энергосбережения в коммерческих зданиях
</Title>
<Content>
Интеллектуальные технологии энергосбережения и энергоэффективности являются со-временным масштабным мировым трендом не только в развитии энергетических систем, но и в строительном, девелоперском бизнесе. Спрос на «умные» здания растет не только в мире, но и в России, прежде всего на рынке строительства и эксплуатации крупных бизнес-центров, торгово-развлекательных центров и др. строительных деловых проектов. Точные оценки экономии важны для продвижения строительных проектов в области энергоэффективности и демонстрации их экономической эффективности. Растущее количество современной измерительной инфраструктуры в коммерческих зданиях привело к повышению доступности данных высокой частоты. Эти данные можно использовать для обнаружения неисправностей и диагностики оборудования, отопления, вентиляции, и оптимизации кондиционирования воздуха. Это также обусловило применение современных и эффективных методов машинного обучения, которые предоставляют перспективные возможности для получения точных прогнозов базового энергопотребления здания, и, таким образом, точные оценки экономии. В настоящей работе для моделирования временных высокочастотных серий энергопо-требления был применен алгоритм градиентного бустинга, мощный алгоритм машинного обучения в широком диапазоне применения в анализе больших данных. На его основе предложен метод моделирования дневного профиля энергопотребления и разработан численный алгоритм, его реализующий. Для оценки его эффективности были использованы данные о энергопотреблении 380 коммерческих зданий. Периоды обучения модели были различными, и для оценки эффективности модели использовались несколько показателей точности прогнозирования. Результаты показали, что использование модели градиентного бустинга улучшило точность прогнозирования более чем в 80 процентах случаев по сравнению с моделями промышленных зданий, использующих линейную регрессию и алгоритмом случайного леса.
</Content>
<field id="content">
Интеллектуальные технологии энергосбережения и энергоэффективности являются со-временным масштабным мировым трендом не только в развитии энергетических систем, но и в строительном, девелоперском бизнесе. Спрос на «умные» здания растет не только в мире, но и в России, прежде всего на рынке строительства и эксплуатации крупных бизнес-центров, торгово-развлекательных центров и др. строительных деловых проектов. Точные оценки экономии важны для продвижения строительных проектов в области энергоэффективности и демонстрации их экономической эффективности. Растущее количество современной измерительной инфраструктуры в коммерческих зданиях привело к повышению доступности данных высокой частоты. Эти данные можно использовать для обнаружения неисправностей и диагностики оборудования, отопления, вентиляции, и оптимизации кондиционирования воздуха. Это также обусловило применение современных и эффективных методов машинного обучения, которые предоставляют перспективные возможности для получения точных прогнозов базового энергопотребления здания, и, таким образом, точные оценки экономии. В настоящей работе для моделирования временных высокочастотных серий энергопо-требления был применен алгоритм градиентного бустинга, мощный алгоритм машинного обучения в широком диапазоне применения в анализе больших данных. На его основе предложен метод моделирования дневного профиля энергопотребления и разработан численный алгоритм, его реализующий. Для оценки его эффективности были использованы данные о энергопотреблении 380 коммерческих зданий. Периоды обучения модели были различными, и для оценки эффективности модели использовались несколько показателей точности прогнозирования. Результаты показали, что использование модели градиентного бустинга улучшило точность прогнозирования более чем в 80 процентах случаев по сравнению с моделями промышленных зданий, использующих линейную регрессию и алгоритмом случайного леса.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>eugene</FirstName>
<FamilyName>shchetinin</FamilyName>
<Email>riviera-molto@mail.ru</Email>
<Affiliation>Financial University under the Government of the Russian Federation</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgenia</FirstName>
<FamilyName>Popova</FamilyName>
<Email>evgenia.popova1397@gmail.com</Email>
<Affiliation>Financial University under the Government of the Russian Federation</Affiliation>
</Co-Author>
<Speaker>
<FirstName>eugene</FirstName>
<FamilyName>shchetinin</FamilyName>
<Email>riviera-molto@mail.ru</Email>
<Affiliation>Financial University under the Government of the Russian Federation</Affiliation>
</Speaker>
<Speaker>
<FirstName>Evgenia</FirstName>
<FamilyName>Popova</FamilyName>
<Email>evgenia.popova1397@gmail.com</Email>
<Affiliation>Financial University under the Government of the Russian Federation</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>189</Id>
<Title>
NRV web knowledge base: scientific and educational applications
</Title>
<Content>
The NRV web knowledge base on low-energy nuclear physics has been created in the Joint Institute for Nuclear Research. This knowledge base working through the Internet integrates a large amount of digitized experimental data on the properties of nuclei and nuclear reaction cross sections with a wide range of computational programs for modeling of nuclear properties and various processes of nuclear dynamics which run directly in the browser of a remote user. Today, the NRV knowledge base is both a powerful tool for nuclear physics research and an educational resource. The system is widely used, as evidenced by the large number of user queries to its resources and the number of references to the knowledge base in the articles published in scientific journals. The basic principles of the NRV knowledge base are covered, and a brief description of its structure is given. The practical usage of the NRV knowledge base for both scientific and educational applications is demonstrated in detail.
</Content>
<field id="content">
The NRV web knowledge base on low-energy nuclear physics has been created in the Joint Institute for Nuclear Research. This knowledge base working through the Internet integrates a large amount of digitized experimental data on the properties of nuclei and nuclear reaction cross sections with a wide range of computational programs for modeling of nuclear properties and various processes of nuclear dynamics which run directly in the browser of a remote user. Today, the NRV knowledge base is both a powerful tool for nuclear physics research and an educational resource. The system is widely used, as evidenced by the large number of user queries to its resources and the number of references to the knowledge base in the articles published in scientific journals. The basic principles of the NRV knowledge base are covered, and a brief description of its structure is given. The practical usage of the NRV knowledge base for both scientific and educational applications is demonstrated in detail.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Karpov</FamilyName>
<Email>karpov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Denikin</FamilyName>
<Email>denikin@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Naumenko</FamilyName>
<Email>anaumenko@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>alekseev_ap@mail.ru</Email>
<Affiliation>Chuvash State University, Cheboksary, Russia</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Rachkov</FamilyName>
<Email>rachkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Viacheslav</FirstName>
<FamilyName>Samarin</FamilyName>
<Email>samarin@jinr.ru</Email>
<Affiliation>
Joint Institute for Nuclear Research, Flerov Laboratory of Nuclear Reactions
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vyacheslav</FirstName>
<FamilyName>Saiko</FamilyName>
<Email>saiko@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Rachkov</FamilyName>
<Email>rachkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>5. Distributed computing in education</Track>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>190</Id>
<Title>
Deep machine learning and pattern/face recognition based on quantum neural networks and quantum genetic algorithm
</Title>
<Content>
In report a new approach for deep machine learning and pattern recognition based on quantum neural network and quantum genetic algorithm is described. The structure of quantum fuzzy neural network is considered. Examples of pattern recognition is described. The method of global optimization in control problems is considered on example of quantum genetic algorithm. The structure on quantum genetic algorithm is introduced. Information technology of intelligent control system design based on quantum soft computing is presented. Example of quantum genetic algorithm application for control of nonlinear “carte-pole” system is described. Application of modified Grover quantum search algorithm in unstructured big database is discussed. Quantum soft computing optimizer of knowledge bases is presented. This report discusses the development of robust intelligent control systems. Special attention is paid to the algorithm of quantum fuzzy inference, in particular to the stage of determining the type and form of quantum correlation. Automating the choice of the type of quantum correlation can be done with the help of a quantum genetic algorithm whose analysis and choice are considered in this report.
</Content>
<field id="content">
In report a new approach for deep machine learning and pattern recognition based on quantum neural network and quantum genetic algorithm is described. The structure of quantum fuzzy neural network is considered. Examples of pattern recognition is described. The method of global optimization in control problems is considered on example of quantum genetic algorithm. The structure on quantum genetic algorithm is introduced. Information technology of intelligent control system design based on quantum soft computing is presented. Example of quantum genetic algorithm application for control of nonlinear “carte-pole” system is described. Application of modified Grover quantum search algorithm in unstructured big database is discussed. Quantum soft computing optimizer of knowledge bases is presented. This report discusses the development of robust intelligent control systems. Special attention is paid to the algorithm of quantum fuzzy inference, in particular to the stage of determining the type and form of quantum correlation. Automating the choice of the type of quantum correlation can be done with the help of a quantum genetic algorithm whose analysis and choice are considered in this report.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey V.</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>Doctor of Science in mathematics and physics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Reshetnikov</FamilyName>
<Email>agreshetnikov@gmail.com</Email>
<Affiliation>Ph.D.</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Ryabov</FamilyName>
<Email>ryabov_nv95@mail.ru</Email>
<Affiliation>Ph.D. student</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey V.</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>Doctor of Science in mathematics and physics</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>191</Id>
<Title>
Применение сети Хопфилда для автоматизированной подборки КПЭ
</Title>
<Content>
Для решения задач распознавания образов применяют методы машинного обучения, одним из которых являются искусственные нейронные сети (ИНС). Их реализация была подсмотрена у природы и похожа на сети нервных клеток живого организма. Работа ИНС повторяет некоторые функции головного мозга и делится на два этапа: обучение нейронной сети, которое позволяет ей выстраивать собственные правила с помощью весовых коэффициентов и распознавание, построенное на основе собранных данных (опыта). Одной из сетей является автоассоциативная рекуррентная сеть Хопфилда, с помощью которой была произведена автоматизация подбора ключевых показателей эффективности для руководителей предприятий.
</Content>
<field id="content">
Для решения задач распознавания образов применяют методы машинного обучения, одним из которых являются искусственные нейронные сети (ИНС). Их реализация была подсмотрена у природы и похожа на сети нервных клеток живого организма. Работа ИНС повторяет некоторые функции головного мозга и делится на два этапа: обучение нейронной сети, которое позволяет ей выстраивать собственные правила с помощью весовых коэффициентов и распознавание, построенное на основе собранных данных (опыта). Одной из сетей является автоассоциативная рекуррентная сеть Хопфилда, с помощью которой была произведена автоматизация подбора ключевых показателей эффективности для руководителей предприятий.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Денис</FirstName>
<FamilyName>Кравченко</FamilyName>
<Email>kradenis@gmail.com</Email>
<Affiliation>Михайлович</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Эдуард</FirstName>
<FamilyName>Никонов</FamilyName>
<Email>e.nikonov@jinr.ru</Email>
<Affiliation>Германович</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Денис</FirstName>
<FamilyName>Кравченко</FamilyName>
<Email>kradenis@gmail.com</Email>
<Affiliation>Михайлович</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>192</Id>
<Title>
A new approach to the development of provenance metadata management systems for large scientific experiments
</Title>
<Content>
Provenance metadata (PMD) contain key information that is necessary to determine the origin, authorship and quality of corresponding data, their proper storage, correct using, and for interpretation and confirmation of relevant scientific results. The need for PMD is especially essential when big data are jointly processed by several research teams, which is a very common practice in many scientific areas of late. This requires a wide and intensive exchange of data and programs for their processing and analysis, covering long periods of time, during which both the data sources and the algorithms for their processing may be modified. Although a number of projects have been implemented in recent years to create management systems for such metadata, but the vast majority of implemented solutions are centralized, which is poorly suited to current trends of working in distributed environments, open data access models, and the use of metadata by organizationally unrelated or loosely coupled communities of researchers. We propose to solve this problem by using a new approach to creating a distributed registry of provenance metadata based on blockchain technology and smart contracts. In this work, the functional requirements for the PMD management system were formulated. Based on these requirements, we investigated the problem of the optimal choice of the type of blockchain for such a system, as well as the optimal choice of consensus algorithm for records ordering within the blockchain without participation of third-party trusted bodies. The architecture and algorithms of the system operation, as well as its interaction with the distributed storage resources management systems, are proposed. Specific use cases for the PMD management system are considered. A number of existing blockchain platforms are considered and the most preferable one is selected. The results of this work are of particular importance in the big data era, when a full analysis of the results of experiments is often not possible for one team, so that many independent teams take part in their analysis. The suggested approach is currently under implementation in SINP MSU in the framework of the project supported by the Russian Science Foundation (grant No 18-11-00075).
</Content>
<field id="content">
Provenance metadata (PMD) contain key information that is necessary to determine the origin, authorship and quality of corresponding data, their proper storage, correct using, and for interpretation and confirmation of relevant scientific results. The need for PMD is especially essential when big data are jointly processed by several research teams, which is a very common practice in many scientific areas of late. This requires a wide and intensive exchange of data and programs for their processing and analysis, covering long periods of time, during which both the data sources and the algorithms for their processing may be modified. Although a number of projects have been implemented in recent years to create management systems for such metadata, but the vast majority of implemented solutions are centralized, which is poorly suited to current trends of working in distributed environments, open data access models, and the use of metadata by organizationally unrelated or loosely coupled communities of researchers. We propose to solve this problem by using a new approach to creating a distributed registry of provenance metadata based on blockchain technology and smart contracts. In this work, the functional requirements for the PMD management system were formulated. Based on these requirements, we investigated the problem of the optimal choice of the type of blockchain for such a system, as well as the optimal choice of consensus algorithm for records ordering within the blockchain without participation of third-party trusted bodies. The architecture and algorithms of the system operation, as well as its interaction with the distributed storage resources management systems, are proposed. Specific use cases for the PMD management system are considered. A number of existing blockchain platforms are considered and the most preferable one is selected. The results of this work are of particular importance in the big data era, when a full analysis of the results of experiments is often not possible for one team, so that many independent teams take part in their analysis. The suggested approach is currently under implementation in SINP MSU in the framework of the project supported by the Russian Science Foundation (grant No 18-11-00075).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Demichev</FamilyName>
<Email>demichev@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Demichev</FamilyName>
<Email>demichev@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>193</Id>
<Title>
Анализ параллельной структуры популяционных алгоритмов оптимизации
</Title>
<Content>
Работа посвящена вопросам крупно-блочной параллельной реализации методов эволюционной и роевой оптимизации на примере решения задачи минимизации функций действительного аргумента. Рассматривается субпопуляционная схема распараллеливания. Предлагается классификация паттернов параллельного взаимодействия субпопуляций, выполненная на основе анализа ряда методов рассматриваемого класса. Описывается программная реализация в форме библиотеки шаблонных функций, реализающих данные паттерны, предлагающая пользователю высокоуровневое средство описания популяционных алгоритмов оптимизации. Рассматриваются вопросы параметризации реализуемых алгоритмов с целью исследования эффективности их распараллеливания.
</Content>
<field id="content">
Работа посвящена вопросам крупно-блочной параллельной реализации методов эволюционной и роевой оптимизации на примере решения задачи минимизации функций действительного аргумента. Рассматривается субпопуляционная схема распараллеливания. Предлагается классификация паттернов параллельного взаимодействия субпопуляций, выполненная на основе анализа ряда методов рассматриваемого класса. Описывается программная реализация в форме библиотеки шаблонных функций, реализающих данные паттерны, предлагающая пользователю высокоуровневое средство описания популяционных алгоритмов оптимизации. Рассматриваются вопросы параметризации реализуемых алгоритмов с целью исследования эффективности их распараллеливания.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Ershov</FamilyName>
<Email>ershovnm@gmail.com</Email>
<Affiliation>Moscow State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Poluyan</FamilyName>
<Email>svpoluyan@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Poluyan</FamilyName>
<Email>svpoluyan@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>194</Id>
<Title>
MODERN E - INFRASTRUCTURE FOR SCIENCE AND EDUCATION IN MOLDOVA BASED ON THE RENAM-GEANT PLATFORM
</Title>
<Content>
The article is devoted to the analysis of approaches and solutions for the development and use of the electronic infrastructure for offering srvices to science and education in the Republic of Moldova. In the paper is considering of trends in the development of the electronic infrastructure and services in the national network RENAM, which provides effective information support for basic scientific and educational processes. The prospects of creating new optical CBF (Cross Border Fibers) links and other components of the electronic platform RENAM-GEANT development on the basis of the UE EaPConnect project are described. The strategy of development of modern regional e-Infrastructure resources and the provision of services that are focused on support of scientific and educational communities in the countries included in the European EaP program are considered.
</Content>
<field id="content">
The article is devoted to the analysis of approaches and solutions for the development and use of the electronic infrastructure for offering srvices to science and education in the Republic of Moldova. In the paper is considering of trends in the development of the electronic infrastructure and services in the national network RENAM, which provides effective information support for basic scientific and educational processes. The prospects of creating new optical CBF (Cross Border Fibers) links and other components of the electronic platform RENAM-GEANT development on the basis of the UE EaPConnect project are described. The strategy of development of modern regional e-Infrastructure resources and the provision of services that are focused on support of scientific and educational communities in the countries included in the European EaP program are considered.
</field>
<field id="summary">
Key words: IT infrastructure, information systems, electronic resources and services
</field>
<PrimaryAuthor>
<FirstName>Grigore</FirstName>
<FamilyName>Secrieru</FamilyName>
<Email>secrieru@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Peter</FirstName>
<FamilyName>Bogatencov</FamilyName>
<Email>bogatencov@renam.md</Email>
<Affiliation>RENAM, Moldova</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nicolai</FirstName>
<FamilyName>Iliuha</FamilyName>
<Email>nicolai.iliuha@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nichita</FirstName>
<FamilyName>Degteariov</FamilyName>
<Email>ndegteariov@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Grigore</FirstName>
<FamilyName>Secrieru</FamilyName>
<Email>secrieru@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>195</Id>
<Title>DDS – The Dynamic Deployment System</Title>
<Content>
The Dynamic Deployment System (DDS) is a tool-set that automates and significantly simplifies a deployment of user-defined processes and their dependencies on any resource management system (RMS) using a given topology. DDS is a part of the ALFA framework. A number of basic concepts are taken into account in DDS. DDS implements a single responsibility principle command line tool-set and API. The system treats users’ task as a black box – it can be an executable or a script. It also provides a watchdogging and a rule-based execution of tasks. DDS implements a plug-in system to abstract from RMS. Additionally it ships an SSH and a localhost plug-ins which can be used when no RMS is available. DDS doesn’t require pre-installation and pre-configuration on the worker nodes. It deploys private facilities on demand with isolated sandboxes. The system provides a key-value property propagation engine. That engine can be used to configure tasks at runtime. DDS also provides a lightweight API for tasks to exchange messages, so-called, custom commands. In this report a detailed description, current status and future development plans of the DDS will be highlighted.
</Content>
<field id="content">
The Dynamic Deployment System (DDS) is a tool-set that automates and significantly simplifies a deployment of user-defined processes and their dependencies on any resource management system (RMS) using a given topology. DDS is a part of the ALFA framework. A number of basic concepts are taken into account in DDS. DDS implements a single responsibility principle command line tool-set and API. The system treats users’ task as a black box – it can be an executable or a script. It also provides a watchdogging and a rule-based execution of tasks. DDS implements a plug-in system to abstract from RMS. Additionally it ships an SSH and a localhost plug-ins which can be used when no RMS is available. DDS doesn’t require pre-installation and pre-configuration on the worker nodes. It deploys private facilities on demand with isolated sandboxes. The system provides a key-value property propagation engine. That engine can be used to configure tasks at runtime. DDS also provides a lightweight API for tasks to exchange messages, so-called, custom commands. In this report a detailed description, current status and future development plans of the DDS will be highlighted.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Lebedev</FamilyName>
<Email>andrey.lebedev@gsi.de</Email>
<Affiliation>GSI, Darmstadt</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anar</FirstName>
<FamilyName>Manafov</FamilyName>
<Email>a.manafov@gsi.de</Email>
<Affiliation>GSI, Darmstadt</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Lebedev</FamilyName>
<Email>andrey.lebedev@gsi.de</Email>
<Affiliation>GSI, Darmstadt</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>196</Id>
<Title>
Sensitivity Analysis in a problem of ReaxFF molecular-dynamic force field optimization
</Title>
<Content>
In a wide range of modern problems, it is required to estimate an influence of uncertainty of input parameters on uncertainty of an output value of a modeling function. In this contribution, we present algorithms for analyzing the sensitivity of a target function with respect to parameters in the problem of optimization of ReaxFF molecular-dynamic force field. In this particular case it allows one to effectively decrease the number of simultaneously optimized parameters. We compare the Sobol's global sensitivity indexes (SI) approach and the correlation analysis. Both methods are based on computations of the target function value on the set of pseudo- or quasi-randomly distributed points. The distribution derived is used for further computations of SI using Monte-Carlo technique and correlation coefficients. In the case of optimized ReaxFF force field one may spend up to several seconds to compute a value of the target function in a particular point. That is why it is important to perform calculations in parallel for multiple points. A parallel algorithm has been implemented in C++ using MPI. We compute Sobol's SI and coefficients of correlation of parameters variation and target function values variation while we optimize the force field for molecules and crystals of zinc hydroxide. We show that using of parameter set sorted by influence allows one to significantly increase convergence speed of the optimization algorithm and even completely exclude those parameters with relatively small influence.
</Content>
<field id="content">
In a wide range of modern problems, it is required to estimate an influence of uncertainty of input parameters on uncertainty of an output value of a modeling function. In this contribution, we present algorithms for analyzing the sensitivity of a target function with respect to parameters in the problem of optimization of ReaxFF molecular-dynamic force field. In this particular case it allows one to effectively decrease the number of simultaneously optimized parameters. We compare the Sobol's global sensitivity indexes (SI) approach and the correlation analysis. Both methods are based on computations of the target function value on the set of pseudo- or quasi-randomly distributed points. The distribution derived is used for further computations of SI using Monte-Carlo technique and correlation coefficients. In the case of optimized ReaxFF force field one may spend up to several seconds to compute a value of the target function in a particular point. That is why it is important to perform calculations in parallel for multiple points. A parallel algorithm has been implemented in C++ using MPI. We compute Sobol's SI and coefficients of correlation of parameters variation and target function values variation while we optimize the force field for molecules and crystals of zinc hydroxide. We show that using of parameter set sorted by influence allows one to significantly increase convergence speed of the optimization algorithm and even completely exclude those parameters with relatively small influence.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Shefov</FamilyName>
<Email>k.s.shefov@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Stepanova</FirstName>
<FamilyName>Margarita</FamilyName>
<Email>mstep@mms.nw.ru</Email>
<Affiliation>SPbSU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Stepanova</FirstName>
<FamilyName>Margarita</FamilyName>
<Email>mstep@mms.nw.ru</Email>
<Affiliation>SPbSU</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>197</Id>
<Title>
Разработка децентрализованной платежной системы на основе технологии blockchain с учетом специфики мобильных платформ
</Title>
<Content>
Интернет находится в центре революций: централизованные проприетарные сервисы подвергаются замене на их децентрализованные аналоги со свободными лицензиями; доверенные третьи стороны юридических и финансовых договоров заменяются верифицируемыми вычислениями; неэффективные монолитные сервисы уступают место одноранговым алгоритмическим рынкам. Bitcoin, Ethereum и другие сети, фундаментом которых является технология blockchain, доказали полезность децентрализованных регистров транзакций. Имея в основе децентрализованные, открытые базы данных, они поддерживают выполнение сложных «умных» контрактов (smart contracts) и обслуживают крипто-активы стоимостью десятки миллиардов долларов. Эти системы являются первыми экземплярами межсетевых открытых сервисов, в которых участники образуют децентрализованную сеть, предоставляющую полезные услуги для коммерции, без централизованного управления или доверенных лиц. Парадигма открытости и децентрализации коснулась не только мира коммерции, но и систем хранения и обработки больших объемов данных. InterPlanetary File System показала полезность адресации контента путем децентрализации самой всемирной паутины, обслуживая миллиарды файлов, используемых в глобальной одноранговой сети. Но достижение децентрализации и ухода от доверенных третьих лиц обернулось высокими требованиями к ресурсоемкости узлов сети и потерей масштабируемости, что препятствует массовой адаптации данных систем. В особенности данная проблема проявляется в виде обхода стороной блокчейн-технологиями мобильных платформ. Между тем, в октябре 2016 использование интернета мобильными и планшетными устройствами впервые превысило ПК по всем�латежей, таких как Visa, которая обес� от независимой веб-аналитической компании StatCounter1. В дальнейшем тенденция роста числа мобильных узлов в сети будет сохраняться. Проблема масштабируемости не дает возможностей конкурировать с централизованными системами электронных платежей, таких как Visa, которая обеспечивает обработку порядка 65000 транзакций в секунду2. В частности, это ограничивает интеграцию с «интернетом вещей» ‒ перспективного направления цифровой экономики. Целью данной работы является разработка распределенной сети на основе blockchain для мобильных платформ. Выдвигается концепт для преодоления вышеуказанных ограничений текущих blockchain-проектов. Уход от таких механизмов верификации как в Ethereum и Bitcoin, использующих для достижения консенсуса между участниками сети сложные вычислительные алгоритмы. В качестве их замены выступают ресурсоэффективные алгоритмы консенсуса Proof-of-Stack. Проблемы бесконечного роста цепочки блоков находится в плоскости организации распределенного хранения данных: эффективного алгоритма выбора массива блоков для хранения узлом с учетом необходимого коэффициента репликации. Подсети на основе системы каналов «узел-узел» для микроплатежей призваны решить проблему масштабируемости. Ключевые слова: blockchain, децентрализованные системы, криптография.
</Content>
<field id="content">
Интернет находится в центре революций: централизованные проприетарные сервисы подвергаются замене на их децентрализованные аналоги со свободными лицензиями; доверенные третьи стороны юридических и финансовых договоров заменяются верифицируемыми вычислениями; неэффективные монолитные сервисы уступают место одноранговым алгоритмическим рынкам. Bitcoin, Ethereum и другие сети, фундаментом которых является технология blockchain, доказали полезность децентрализованных регистров транзакций. Имея в основе децентрализованные, открытые базы данных, они поддерживают выполнение сложных «умных» контрактов (smart contracts) и обслуживают крипто-активы стоимостью десятки миллиардов долларов. Эти системы являются первыми экземплярами межсетевых открытых сервисов, в которых участники образуют децентрализованную сеть, предоставляющую полезные услуги для коммерции, без централизованного управления или доверенных лиц. Парадигма открытости и децентрализации коснулась не только мира коммерции, но и систем хранения и обработки больших объемов данных. InterPlanetary File System показала полезность адресации контента путем децентрализации самой всемирной паутины, обслуживая миллиарды файлов, используемых в глобальной одноранговой сети. Но достижение децентрализации и ухода от доверенных третьих лиц обернулось высокими требованиями к ресурсоемкости узлов сети и потерей масштабируемости, что препятствует массовой адаптации данных систем. В особенности данная проблема проявляется в виде обхода стороной блокчейн-технологиями мобильных платформ. Между тем, в октябре 2016 использование интернета мобильными и планшетными устройствами впервые превысило ПК по всем�латежей, таких как Visa, которая обес� от независимой веб-аналитической компании StatCounter1. В дальнейшем тенденция роста числа мобильных узлов в сети будет сохраняться. Проблема масштабируемости не дает возможностей конкурировать с централизованными системами электронных платежей, таких как Visa, которая обеспечивает обработку порядка 65000 транзакций в секунду2. В частности, это ограничивает интеграцию с «интернетом вещей» ‒ перспективного направления цифровой экономики. Целью данной работы является разработка распределенной сети на основе blockchain для мобильных платформ. Выдвигается концепт для преодоления вышеуказанных ограничений текущих blockchain-проектов. Уход от таких механизмов верификации как в Ethereum и Bitcoin, использующих для достижения консенсуса между участниками сети сложные вычислительные алгоритмы. В качестве их замены выступают ресурсоэффективные алгоритмы консенсуса Proof-of-Stack. Проблемы бесконечного роста цепочки блоков находится в плоскости организации распределенного хранения данных: эффективного алгоритма выбора массива блоков для хранения узлом с учетом необходимого коэффициента репликации. Подсети на основе системы каналов «узел-узел» для микроплатежей призваны решить проблему масштабируемости. Ключевые слова: blockchain, децентрализованные системы, криптография.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Андрей</FirstName>
<FamilyName>Илюхин</FamilyName>
<Email>a.iluhin@nordavind.ru</Email>
<Affiliation>Dubna International University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Eduard</FirstName>
<FamilyName>Nikonov</FamilyName>
<Email>e.nikonov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Андрей</FirstName>
<FamilyName>Илюхин</FamilyName>
<Email>a.iluhin@nordavind.ru</Email>
<Affiliation>Dubna International University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>198</Id>
<Title>The ATLAS EventIndex: a catalog of physics data</Title>
<Content>
The ATLAS experiment produced hundreds of petabytes of data and expects to have one order of magnitude more in the future. This data are spread among hundreds of computing Grid sites around the world. The EventIndex catalogues the basic elements of these data - real and simulated events. It provides the means to select and access event data in the ATLAS distributed storage system and is also used for completeness and consistency checks and data overlap studies. The EventIndex employs various data handling technologies like Hadoop and Oracle databases, and is tightly integrated with other elements of the ATLAS computing infrastructure, including systems for data, metadata, and production management (AMI, Rucio and PANDA). The project is in operation since the start of LHC Run 2 in 2015, and is in permanent development in order to fit the analysis and production demands and follow technology evolutions. In this talk the EventIndex structure and components will be described together with the use cases.
</Content>
<field id="content">
The ATLAS experiment produced hundreds of petabytes of data and expects to have one order of magnitude more in the future. This data are spread among hundreds of computing Grid sites around the world. The EventIndex catalogues the basic elements of these data - real and simulated events. It provides the means to select and access event data in the ATLAS distributed storage system and is also used for completeness and consistency checks and data overlap studies. The EventIndex employs various data handling technologies like Hadoop and Oracle databases, and is tightly integrated with other elements of the ATLAS computing infrastructure, including systems for data, metadata, and production management (AMI, Rucio and PANDA). The project is in operation since the start of LHC Run 2 in 2015, and is in permanent development in order to fit the analysis and production demands and follow technology evolutions. In this talk the EventIndex structure and components will be described together with the use cases.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>UTFSM</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>UTFSM</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>199</Id>
<Title>
Evolution of the ATLAS EventIndex based on Apache Kudu storage
</Title>
<Content>
The ATLAS EventIndex has been in operation since the beginning of LHC Run 2 in 2015. Like all software projects, its components have been constantly evolving and improving in performance. The main data store in Hadoop, based on MapFiles and HBase, can work for the rest of Run 2 but new solutions are explored for the future. Kudu offers an interesting environment, with a mixture of BigData and relational database features, which looked promising at the design level and is now used to build a prototype to measure the scaling capabilities as a function of data input rates, total data volumes and data query and retrieval rates. An extension of the EventIndex functionalities to support the concept of Virtual Datasets produced additional requirements that are tested on the same Kudu prototype, in order to estimate the system performance and response times for different internal data organisations. This talk reports on the selected data schemas and on the current performance measurements with the Kudu prototypes.
</Content>
<field id="content">
The ATLAS EventIndex has been in operation since the beginning of LHC Run 2 in 2015. Like all software projects, its components have been constantly evolving and improving in performance. The main data store in Hadoop, based on MapFiles and HBase, can work for the rest of Run 2 but new solutions are explored for the future. Kudu offers an interesting environment, with a mixture of BigData and relational database features, which looked promising at the design level and is now used to build a prototype to measure the scaling capabilities as a function of data input rates, total data volumes and data query and retrieval rates. An extension of the EventIndex functionalities to support the concept of Virtual Datasets produced additional requirements that are tested on the same Kudu prototype, in order to estimate the system performance and response times for different internal data organisations. This talk reports on the selected data schemas and on the current performance measurements with the Kudu prototypes.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>200</Id>
<Title>
BigData tools for the monitoring of the ATLAS EventIndex
</Title>
<Content>
The ATLAS EventIndex collects event information from data both at CERN and Grid sites. It uses the Hadoop system to store the results, and web services to access them. Its successful operation depends on a number of different components, that have to be monitored constantly to ensure continuous operation of the system. Each component has completely different sets of parameters and states and requires a special approach. A scheduler runs monitoring tasks, which gather information by various methods: querying databases, web sites and storage systems, parsing logs and using CERN host monitoring services. Information is then fed to Grafana dashboards via InfluxDB. Using this platform allowed much faster performance and flexibility compared to the previously used Kibana system.
</Content>
<field id="content">
The ATLAS EventIndex collects event information from data both at CERN and Grid sites. It uses the Hadoop system to store the results, and web services to access them. Its successful operation depends on a number of different components, that have to be monitored constantly to ensure continuous operation of the system. Each component has completely different sets of parameters and states and requires a special approach. A scheduler runs monitoring tasks, which gather information by various methods: querying databases, web sites and storage systems, parsing logs and using CERN host monitoring services. Information is then fed to Grafana dashboards via InfluxDB. Using this platform allowed much faster performance and flexibility compared to the previously used Kibana system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgeny</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>aleksand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrei</FirstName>
<FamilyName>Kazymov</FamilyName>
<Email>kazymai@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>UTFSM</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Evgeny</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>aleksand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<Speaker>
<FirstName>Andrei</FirstName>
<FamilyName>Kazymov</FamilyName>
<Email>kazymai@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>201</Id>
<Title>
Trigger information data flow for the ATLAS EventIndex
</Title>
<Content>
The trigger information is an important part of the ATLAS event data. In the EventIndex trigger information is collected for various use cases, including event selection and overlap counting. Decoding the trigger information from the event records, stored as a bit mask, requires additional input from the conditions metadata database, as trigger configurations evolve with time. It depends on the run number for the real data and from the simulation settings for Monte-Carlo data. We describe trigger information handling in the EventIndex and the interfaces used to access it.
</Content>
<field id="content">
The trigger information is an important part of the ATLAS event data. In the EventIndex trigger information is collected for various use cases, including event selection and overlap counting. Decoding the trigger information from the event records, stored as a bit mask, requires additional input from the conditions metadata database, as trigger configurations evolve with time. It depends on the run number for the real data and from the simulation settings for Monte-Carlo data. We describe trigger information handling in the EventIndex and the interfaces used to access it.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Mineev</FamilyName>
<Email>mineev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>UTFSM</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Mineev</FamilyName>
<Email>mineev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>202</Id>
<Title>
Cloud-based Computing for LHAASO experiment at IHEP
</Title>
<Content>
Mass data processing and analysis contribute much to the development and discoveries of a new generation of High Energy Physics. The LHAASO(Large High Altitude Air Shower Observatory) experiment of IHEP located in Daocheng, Sichuan province (altitude 4410 m), is expected the most sensitive project to study the problems in Galactic cosmic ray physics, and requires massive storage and computing power, which is urgent to explore new solutions based on cloud computing models to integrate distributed heterogeneous resources. However, it faces with high operation and maintenance costs, system instability and other issues. To address these issues, we introduce cloud computing technology for LHAASO in order to make sure the system availability and stability, as well as simplify system deployment and significantly reduce the maintenance cost. Particularly, we discuss the cloud-based computing architecture to federate distributed resources across regions for LHAASO experiment, including distributed resource management, job scheduling, distributed monitoring and automated deployment. Also container orchestration is introduced to make use of the load balancing and fault tolerance to improve system availability. The prototype is based on Openstack and HTCondor to achieve a unified resource management and scheduling job across regions transparently, located in Beijing, Chengdu and Daocheng, some commercial cloud also included like Alibaba Cloud. We also report a dynamic resource provisioning approach to achieve the resource expansion on demand and the efficient job scheduling, so as to improve the overall resource utilization. Considering serving data access from remote site, we design a remote storage system named LEAF to provide the unified data view, data cache as well as high performance of data access over WLAN. This presentation also will discuss the cloud-based open platform for LHAASO. The feature of the platform makes it possible to do data analysis by the web browser as data, software and computing resources are available in cloud. Finally, a proposal of integrating the HPC facility into the current computing system to faster LHAASO reconstruction jobs will be discussed.
</Content>
<field id="content">
Mass data processing and analysis contribute much to the development and discoveries of a new generation of High Energy Physics. The LHAASO(Large High Altitude Air Shower Observatory) experiment of IHEP located in Daocheng, Sichuan province (altitude 4410 m), is expected the most sensitive project to study the problems in Galactic cosmic ray physics, and requires massive storage and computing power, which is urgent to explore new solutions based on cloud computing models to integrate distributed heterogeneous resources. However, it faces with high operation and maintenance costs, system instability and other issues. To address these issues, we introduce cloud computing technology for LHAASO in order to make sure the system availability and stability, as well as simplify system deployment and significantly reduce the maintenance cost. Particularly, we discuss the cloud-based computing architecture to federate distributed resources across regions for LHAASO experiment, including distributed resource management, job scheduling, distributed monitoring and automated deployment. Also container orchestration is introduced to make use of the load balancing and fault tolerance to improve system availability. The prototype is based on Openstack and HTCondor to achieve a unified resource management and scheduling job across regions transparently, located in Beijing, Chengdu and Daocheng, some commercial cloud also included like Alibaba Cloud. We also report a dynamic resource provisioning approach to achieve the resource expansion on demand and the efficient job scheduling, so as to improve the overall resource utilization. Considering serving data access from remote site, we design a remote storage system named LEAF to provide the unified data view, data cache as well as high performance of data access over WLAN. This presentation also will discuss the cloud-based open platform for LHAASO. The feature of the platform makes it possible to do data analysis by the web browser as data, software and computing resources are available in cloud. Finally, a proposal of integrating the HPC facility into the current computing system to faster LHAASO reconstruction jobs will be discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Qiulan</FirstName>
<FamilyName>Huang</FamilyName>
<Email>huangql@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics,Chinese Academy of Sciences</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Weidong</FirstName>
<FamilyName>Li</FamilyName>
<Email>liwd@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yaodong</FirstName>
<FamilyName>Cheng</FamilyName>
<Email>chyd@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Haibo</FirstName>
<FamilyName>Li</FamilyName>
<Email>lihaibo@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jingyan</FirstName>
<FamilyName>Shi</FamilyName>
<Email>shijy@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tao</FirstName>
<FamilyName>Cui</FamilyName>
<Email>cuit@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Xiaowei</FirstName>
<FamilyName>Jiang</FamilyName>
<Email>liyk@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese of Academy of Sciences</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Qiulan</FirstName>
<FamilyName>Huang</FamilyName>
<Email>huangql@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics,Chinese Academy of Sciences</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>203</Id>
<Title>About some block chain problems</Title>
<Content>
Cloud Computing stands out from any other distributed computing paradigm by offering services on-demand basis which are not limited to any geographical restrictions. Consequently this has revolutionized the computing by providing services to wide scope of customers starting from casual users to highly business oriented Industries. Despite of its capabilities, Cloud Computing still faces challenges in handling a wide array of faults, which could causes loss of credibility to Cloud Computing. Among those faults Byzantine faults offers serious challenge to fault tolerance mechanism, because it often goes undetected at the initial stages and it could easily propagate to other VMs before detection is made. Consequently some of the mission critical applications such as air traffic control, online baking etc still avoid using the use of cloud computing for such reasons. Moreover if a Byzantine faults is not detected and tolerated at initial stage then applications such as big data analytics can go completely wrong in spite of hours of computations performed by the entire cloud. Therefore in the previous work a fool-proof Byzantine fault detection has been proposed, as a continuation this work designs a scheduling algorithm (WSSS) and checkpoint optimization algorithm (TCC) to tolerate and eliminate the Byzantine faults before it makes any impact. The WSSS algorithm keeps track of server performance which is part of Virtual Clusters to help allocate best performing server to mission critical application. WSSS therefore ranks the servers based on a counter which monitors every Virtual Nodes (VN) for time and performance failures. The TCC algorithm works to generalize the possible Byzantine error prone region through monitoring delay variation to start new VNs with previous checkpointing. Moreover it can stretch the state interval for performing and error free VNs in an effect to minimize the space, time and cost overheads caused by checkpointing. The analysis is performed with plotting state transition and CloudSim based simulation. The result shows TCC reduces fault tolerance overhead exponentially and the WSSS allots virtual resources effectively.
</Content>
<field id="content">
Cloud Computing stands out from any other distributed computing paradigm by offering services on-demand basis which are not limited to any geographical restrictions. Consequently this has revolutionized the computing by providing services to wide scope of customers starting from casual users to highly business oriented Industries. Despite of its capabilities, Cloud Computing still faces challenges in handling a wide array of faults, which could causes loss of credibility to Cloud Computing. Among those faults Byzantine faults offers serious challenge to fault tolerance mechanism, because it often goes undetected at the initial stages and it could easily propagate to other VMs before detection is made. Consequently some of the mission critical applications such as air traffic control, online baking etc still avoid using the use of cloud computing for such reasons. Moreover if a Byzantine faults is not detected and tolerated at initial stage then applications such as big data analytics can go completely wrong in spite of hours of computations performed by the entire cloud. Therefore in the previous work a fool-proof Byzantine fault detection has been proposed, as a continuation this work designs a scheduling algorithm (WSSS) and checkpoint optimization algorithm (TCC) to tolerate and eliminate the Byzantine faults before it makes any impact. The WSSS algorithm keeps track of server performance which is part of Virtual Clusters to help allocate best performing server to mission critical application. WSSS therefore ranks the servers based on a counter which monitors every Virtual Nodes (VN) for time and performance failures. The TCC algorithm works to generalize the possible Byzantine error prone region through monitoring delay variation to start new VNs with previous checkpointing. Moreover it can stretch the state interval for performing and error free VNs in an effect to minimize the space, time and cost overheads caused by checkpointing. The analysis is performed with plotting state transition and CloudSim based simulation. The result shows TCC reduces fault tolerance overhead exponentially and the WSSS allots virtual resources effectively.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>MAGDALYNE</FirstName>
<FamilyName>KAMANDE</FamilyName>
<Email>magdalynde@gmail.com</Email>
<Affiliation>St Peterburg State Electrotechnical University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>204</Id>
<Title>
Essential aspects of IT training technology for processing, storage and intellectual analysis of the big data using the virtual computer lab
</Title>
<Content>
This paper discusses issues surrounding the training of specialists in the field of storage, processing and intellectual analysis of big data using virtual computer lab and its main architectural components.
</Content>
<field id="content">
This paper discusses issues surrounding the training of specialists in the field of storage, processing and intellectual analysis of big data using virtual computer lab and its main architectural components.
</field>
<field id="summary">
I. INTRODUCTION Many new professions in the field of the virtual computer lab entered the labor market, corresponding the needs of the Russian economy. These professions are: a designer of artificial intelligence systems, the big data analyst, specialist on the information promotion, specialist on the machine learning, analyst of the robotized process, digital information manager, developer of the informational system based on blockchain, and even such rare professions as a planner of the interaction with the artificial intelligence and cognitive copywriter. A new training methodology of the skilled IT specialists should be created as the new professions have entered the labor market. The professionals should have enough knowledge and skills to meet the current realities and needs of the business. They should get a job and the social position in the leading sector companies, market leaders of the high-teach goods and the services of new generation. These people will create and leap the economy of the future forward. Nowadays we face the primary task of supplying the software tools and technologies to provide the effective work in the classroom and at home, cause a true amazement and sincere admiration for the progress in math and computer science, making the students more self-confident and giving the reasons for the future actions. We should create sustainable development not only at the national level, but also in the globalization and international partnership. The import substitution policy is a short-term strategy, and for a long-term one it is necessary to consider that the education and the development of IT is out of politics as the innovation requires a deep understanding of all the existing achievements of humankind to date. Accepting the modern challenges, we began creating not only the technical environment but also the space for the knowledge sharing. We draw an analogy of the physical laws and thermodynamics with the laws of functioning of sociotechnical systems. We create the special conditions for the cooperation between the students and teachers by using of our flagship project "Virtual Computer Lab" (VCL), which has been constantly improved for more than 12 years. It gives an opportunity for the students to create and expand own created multi-component corporate information systems both by themselves and in a team, form the teaching aids in a team, making both the freshmen and graduates take part in the process. It significantly increases the cognitive perception of teaching material. II. BACKGROUND The training of the «consumers» should be cut off in the process of the IT specialists’ education, and we should spare no effort to training of the «creative doers». For this purpose, it is important not only to study the ways of creating the information systems from the scratch, paying attention to the configuring and adjustment of the equipment, connection and integration of all the necessary parts of the system without any help, and only after that to accomplish issue-oriented tasks. The practical study of the modern peer-to-peer protocols is very important nowadays. The students get acquainted with the approaches to the improvement of the existing systems, not changing end-user presentation level. They are taught how to setup modern systems with the horizontal scaling for the data storage, distributed map-reduce analytics, OLAP analysis based on the materialized views, which accelerate the data output in the business intelligence system without decreasing of the reliability and increased cost as compared to in-memory solutions. In addition to the training of programmers in the development of the mobile solutions and cloud services, it is necessary to focus attention on the training of the programmers in the neural network solutions using open source frameworks such as TensorFlow, Keras, OpenAI. The training should involve the use of the modern processor designs including AVX, FMA, SSE4x instructions and technologies of the distributed computing MPI, CUDA, OpenCL for the effective solving the tasks of the cognitive informatics and machine learning. The expert of the future is an expert, which has not only the fundamental scientific knowledge, but he is a promising engineer with an outstanding potential and is able to compose and make the capable computing solutions suitable for the project. Only the skilled professionals of this level can create the right conditions for the science development and its practical applications at an increasing rate. All above-mentioned problems can be solved in the virtual computer lab, which has become not only the innovative tool for the training of the high skilled IT specialists, but also a demanded space for the technical cooperation between a final-year student and a potential employer. It gives an opportunity to show your qualification in real time, and to present the employer's problem in the virtual format and try to solve it together, attracting the young minds and sometimes people with different ways of thinking, for example, the history of the neural network expansion and the idea of calculation of the back propagation errors, using the gradient descent method and so on. That is why the priority of the university is to create the most favorable conditions for the forming of the professional competence in IT, which will help the graduates to solve a wide range of the tasks, happening during all the stages of the corporate information systems development, including the design itself. It is evident that to form the professional competence the students should do the following in order master a lot of literature, do many practical tasks and make research works on the modern information systems, their deployment, maintenance and effective appliance for solving the problem-oriented tasks and so on. The following problems had to be solved for the effective target training of IT specialists: a lack of class hours for solving the necessary and sufficient practical tasks of the complex information systems studying; it’s impossible to get the work experience in the complex information systems by use of the personal computer with an average capacity as such systems demand different requirements of the hardware in comparison with the home, office and portable computers; one sometimes has problems during the setup and maintenance of the information systems, these tasks cannot be solved without the work experience in such systems; the price of some products licenses is extremely high for a user, in most cases, one needs a license only for the educational process. The main way to solve these problems has been to create a virtual computer lab that is able to solve the problem of insufficient computing and software resources and to provide an adequate level of technological and methodological support; to teach how to use modern technologies to work with distributed information systems; to organize group work with educational materials by involving users in the process of improving these materials and allowing them to communicate freely with each other on the basis of self-organizational principles. III. BRIEF CONCEPT OF USING VIRTUAL COMPUTER LAB The Virtual Computing Lab provides a set of software and hardware-based virtualization, containerization and management tools that enable the flexible and on-demand provision and use of computing resources, knowledge management system, theoretical materials and practical cookbooks in the form of cloud services for carrying out research projects, scientific computational calculations and tasks related to the development of complex corporate and other distributed information systems. The service also provides dedicated virtual servers for innovative projects that are carried out by students and staff at the Institute of System Analysis and Control of State Dubna University. One main distinguishing trait of the Virtual Computer Lab is its self-organizing principles, which make it possible to transition students from a rigid system of group security policies to a new system where each student can develop a sense of personal responsibility, respect for colleagues, and tolerance, which should provide a solid foundation for strengthening and developing basic civilizational values in the education environment. Thus, today the need has arisen to incorporate technologies into the educational process that will contribute to global integration in the foreseeable future. It is not arbitrary that education that is conducted through high-availability distributed information systems is a priority, because these types of software solutions have become an integral part of modern business. That’s why the task of designing and deploying failover clusters forms the topic of several special courses, which are designed to satisfy the demand for these skills by modern companies. When designing corporate information systems and ensuring the availability of critical applications that are independent of a hardware and software environment, it is critically important to ensure the successful implementation of many key business processes. Downtime, including for scheduled maintenance, leads to additional costs and the loss of customers, and the long outages are simply unacceptable for modern high-tech enterprises. IV. FUNDAMENTAL COMPONENTS OF VIRTUAL COMPUTER LAB Modern blade servers are the hardware components that support virtual computer labs. They are high-performance, high-capacity, but compact and allow the space in the server room to be used more efficiently. The software platform of the Virtual Computing Lab is implemented based on the VMware vSphere Software, which consists of vSphere ESXi hypervisors with some hand-made enhancements and optimizations for some specific hardware that handle all the computing work of the virtual machines as well as vCenter Server central management servers. The vCenter Server consists of the following key components: vCenter Single Sign-On. This component is critical to the whole environment, since it provides secure authentication services for many vSphere components. Single Sign-On creates an internal secure domain in which the various components and solutions that are included in the vSphere ecosystem are registered during the installation or upgrade process, and subsequently they will be assigned basic infrastructural resources. Within the VCL architecture this component is responsible not only for internal authentication services, but it is also used to authenticate users from the university's internal domain who have Microsoft Active Directory accounts at the university. vCenter Server. The vCenter Server component is a central component that is used to manage the vSphere environment. This module provides management and monitoring interfaces for several vSphere nodes, and it also enables the use of such technologies as VMware vSphere vMotion and VMware vSphere High Availability. vCenter Inventory Service. Approximately ninety percent of vSphere Web Client requests to the server are just requests to read the current configuration of the system and its state. The Inventory Service is a component that caches most of the information about the current state of the environment to respond to vSphere Web Client requests to reduce the load on vCenter basic processes. vSphere Server for Web Client (vSphere Web Client). vSphere Web Client is the main interface that is used to centrally manage the environment. It can be divided into two parts: the first server part, which serves requests from the second part, which is the end user's Adobe Flex compatible browser with support for NPAPI-plugins. It is worth noting that the VCL may also be managed using the vCenter Server Desktop Client that is installed on the end user's computer. vCenter Server Database. The database is one of the key modules in the vCenter Server stack architecture. Almost every request sent to the vCenter Server entails communicating with the database. This database is the main storage location for vCenter Server parameters, and it is also a repository of statistical data. Saved statistical data make it possible to optimize system performance during subsequent analysis. The NVidia Tesla, Volta, Pascal, Maxwell GPUs could be used for 3D virtualization and VMware Horizon Suite is used for remote VDI connections as well as for creating images of virtual servers and workstations that are separated into layers using VMware ThinApp and for managing these images. This solution is very important for machine learning due to significant increasing of neural networks training speed. A centralized management portal as well as a knowledge management system were created to improve productivity of work in the Virtual Computer Laboratory. The need to create such a system was conditioned by the fact that students are able to improve productivity of remote learning by themselves, so it is important to create a social network between all participants as well as to create an environment that allows pupils the opportunity to independently engage in such processes as the identification, acquisition, presentation, and use (distribution) of knowledge without the direct involvement of the instructor. Methods of use (propagation) are directly related to storage methods and, consequently, the technological tools that may be used for the transmission of formal knowledge include knowledge bases with various search functionality; blogs, wikis, and social networks; "Wiki Textbooks" that allow all participants to collaboratively create and update educational content and exchange practical problems (including from real companies); as well as user blogs, forums, and group chat systems. The new practice with containers is different compared to VMware case and effectively complements it for a wide range of practical tasks. For the underlying operating system kernel can be used for all containers. On the one hand, it introduces restrictions on the use of other operating systems while, on the other hand, it improves payload on the north of a similar configuration. This can be achieved due to the specifics of the containerization architecture, which we will examine on the example of Docker. Docker uses a client-server architecture in which the Docker-client interacts with the Docker daemon, enabling the operations of creating and launching containers on the server and providing them to students. In general, a containerization system can be represented in the form of three key components: images, registries, and containers. Images represent read-only templates that contain an operating system based on the same kernel version as the host system with necessary pre-configured and adapted software. These images are created, modified if necessary, and then used for deployment of individual solitary containers. The images are stored in the registry, which is a tool for their storage and distribution. The registry content corresponds the curriculum and laboratory plans prepared by the teaching staff. The containers per se are, in fact, like catalogues (directories) of an operating system, where all the changes made by the user and the system software while work are stored. Each container installed from an image provides the capacity for fast creation, start, stop, move, and delete. It also works as a safe sandbox for running applications, allowing the student to carry out any experiments without compromising the base operating system, while maintaining the highest level of performance. Current evolution of VCL lead to development of design templates for both corporate IT deployment and students learning project. V. CONCLUSION It should also be emphasized that the virtual computer lab has helped us provide an optimal and sustainable technological, educational-organizational, scientific-methodological, and regulatory-administrative environment for supporting innovative approaches to computer education. It promotes the integration of the scientific and educational potential of Dubna State University and the formation of industry and academic research partnerships with leading companies that are potential employers of graduates of the Institute of System Analysis and Control. The results that the Institute of System Analysis and Control has achieved in improving the educational process represent strategic foundations for overcoming perhaps one of the most acute problems in modern education: the fact that it tends to respond to changes in the external environment weakly and slowly.
</field>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@uni-dubna.ru</Email>
<Affiliation>Dubna State Univeristy</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>Dubna International University of Nature, Society and Man. Institute of system analysis and management</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kua@uni-dubna.ru</Email>
<Affiliation>Alekseevich</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>Dubna Univeristy</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@uni-dubna.ru</Email>
<Affiliation>Dubna State Univeristy</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>5. Distributed computing in education</Track>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>205</Id>
<Title>Virtual testbed for naval hydrodynamic problems</Title>
<Content>
Complex modeling of the behavior of marine objects under the influence of real external excitation is the most important problem. At present, the accuracy of direct simulation of phenomena with known physics is comparable to the accuracy of the results obtained during the model experiment in towing tanks. Particularly relevant is the creation of such marine virtual testbed for full-featured simulators and when testing the knowledge base of onboard intelligent systems. Such integrated environment is a complex information object that combines the features of both the enterprise system and the high-performance modeling tool. Integrated environment based on these basic principles is designed to solve in real time the following problems: 1. Collection and analysis of information on the current state of dynamic object (DO) and the environment, remote monitoring of the state of objects. 2. Evaluation and coordination of joint actions of DOs, proceeding from current conditions, with the aim of optimally common problem solving. 3. Centralized support for decision-making by operators of DO control in non-standard situations, organization of information support for the interaction of decision-makers in the conduct of ongoing operations. 4. Computer modeling of possible scenarios of situation development with the aim of selecting the optimal control strategy. 5. Centralized control of technical means. 6. Cataloging and accumulation of information in dynamic databases. Modern architecture of computer systems (especially GP GPU) allows direct full-featured simulation of a marine object in real time. Efficient mapping to a hybrid architecture allows even the ability to render ahead of time under various scenarios. The report discusses general concept of high-performance virtual testbed development and the experience of creating on their basis full-featured simulators.
</Content>
<field id="content">
Complex modeling of the behavior of marine objects under the influence of real external excitation is the most important problem. At present, the accuracy of direct simulation of phenomena with known physics is comparable to the accuracy of the results obtained during the model experiment in towing tanks. Particularly relevant is the creation of such marine virtual testbed for full-featured simulators and when testing the knowledge base of onboard intelligent systems. Such integrated environment is a complex information object that combines the features of both the enterprise system and the high-performance modeling tool. Integrated environment based on these basic principles is designed to solve in real time the following problems: 1. Collection and analysis of information on the current state of dynamic object (DO) and the environment, remote monitoring of the state of objects. 2. Evaluation and coordination of joint actions of DOs, proceeding from current conditions, with the aim of optimally common problem solving. 3. Centralized support for decision-making by operators of DO control in non-standard situations, organization of information support for the interaction of decision-makers in the conduct of ongoing operations. 4. Computer modeling of possible scenarios of situation development with the aim of selecting the optimal control strategy. 5. Centralized control of technical means. 6. Cataloging and accumulation of information in dynamic databases. Modern architecture of computer systems (especially GP GPU) allows direct full-featured simulation of a marine object in real time. Efficient mapping to a hybrid architecture allows even the ability to render ahead of time under various scenarios. The report discusses general concept of high-performance virtual testbed development and the experience of creating on their basis full-featured simulators.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Pylnev</FamilyName>
<Email>pylnev@neotech-marine.ru</Email>
<Affiliation>Engineering company «NEOTECH MARINE»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anatoly</FirstName>
<FamilyName>Eibozhenko</FamilyName>
<Email>eibozhenko@yandex.ru</Email>
<Affiliation>Engineering company «NEOTECH MARINE»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Gankevich</FamilyName>
<Email>igankevich@ya.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>206</Id>
<Title>
On porting of applications to new heterogeneous systems
</Title>
<Content>
This work is devoted to the development of guidelines for the porting of existing applications to GPGPU. The paper provides an overview of the current state of the parallel computation area with respect to GPGPU. Various approaches to the organization of parallel computations are considered, and their effectiveness is evaluated in relation to the application under study. Special attention is given to delicate relation between vectorization (done on the level of most internal loops of code) and parallelization (done on the external computational tasks). The proper combination of this makes it possible to get optimal speed-up. But in reality it can be too ideal point of view because of two principle limitations – memory of the GPGPU and the link between CPU and GPGPU. We argue that due to those limitations it is impossible to work out general strategy of porting applications to any GPGPU. Anyway for particular codes and special GPU’s the proposed approach makes it possible to get speed-up’s up to hundreds. This becomes even more effective when combined with virtualization of GPGPU to provide the balance between the size of computing core and rate of data transfer to it. We illustrate our approach on the examples of OpenFoam and DSMC porting to P100 GPU. It is clear, that only combination of all proposed measures makes it possible to get necessary speed-up. As a result, a strategy has been developed for migrating the application to a heterogeneous system. The results of the work can be applied when transferring similar applications to GPGPU or modified to transfer other types of applications.
</Content>
<field id="content">
This work is devoted to the development of guidelines for the porting of existing applications to GPGPU. The paper provides an overview of the current state of the parallel computation area with respect to GPGPU. Various approaches to the organization of parallel computations are considered, and their effectiveness is evaluated in relation to the application under study. Special attention is given to delicate relation between vectorization (done on the level of most internal loops of code) and parallelization (done on the external computational tasks). The proper combination of this makes it possible to get optimal speed-up. But in reality it can be too ideal point of view because of two principle limitations – memory of the GPGPU and the link between CPU and GPGPU. We argue that due to those limitations it is impossible to work out general strategy of porting applications to any GPGPU. Anyway for particular codes and special GPU’s the proposed approach makes it possible to get speed-up’s up to hundreds. This becomes even more effective when combined with virtualization of GPGPU to provide the balance between the size of computing core and rate of data transfer to it. We illustrate our approach on the examples of OpenFoam and DSMC porting to P100 GPU. It is clear, that only combination of all proposed measures makes it possible to get necessary speed-up. As a result, a strategy has been developed for migrating the application to a heterogeneous system. The results of the work can be applied when transferring similar applications to GPGPU or modified to transfer other types of applications.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Mareev</FamilyName>
<Email>map@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Storublevtzev</FamilyName>
<Email>100.rub@mail.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Denis</FirstName>
<FamilyName>Manyashin</FamilyName>
<Email>w3prog@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>207</Id>
<Title>
Применение эволюционных и роевых алгоритмов оптимизации для решения модельной задачи предсказания структуры белка
</Title>
<Content>
Работа посвящена проблемам прогнозирования пространственной структуры белковых молекул, полипептидов и их комплексов. Предлагаемый нами метод основан на решении задачи оптимизации, в которой целевой функцией является потенциальная энергия молекулы, а параметрами оптимизации – длины связей между атомами и углы вращения. Главными особенностями таких задач является большая размерность и высокая вычислительная сложность. Проведенные предварительные исследования показали, что множество существующих алгоритмов оптимизации решают такую задачу неудовлетворительно. Типичное время вычисления задачи занимает от нескольких часов до нескольких дней. Кроме того, для корректного вычисления целевой функции требуется серьезное программное обеспечение. Эти факторы существенно усложняют процесс разработки эффективных алгоритмов для решения задачи предсказания структуры белков и их комплексов. В настоящей работе предлагается упрощенная, модельная задача укладки графа на плоскости, которая позволяет проводить расчеты быстрее и без использования специального программного обеспечения, разрабатывать новые и улучшать существующие алгоритмы оптимизации. В работе приводятся результаты численного исследования ряда алгоритмов роевой и эволюционной оптимизации при решении поставленной модельной задачи.
</Content>
<field id="content">
Работа посвящена проблемам прогнозирования пространственной структуры белковых молекул, полипептидов и их комплексов. Предлагаемый нами метод основан на решении задачи оптимизации, в которой целевой функцией является потенциальная энергия молекулы, а параметрами оптимизации – длины связей между атомами и углы вращения. Главными особенностями таких задач является большая размерность и высокая вычислительная сложность. Проведенные предварительные исследования показали, что множество существующих алгоритмов оптимизации решают такую задачу неудовлетворительно. Типичное время вычисления задачи занимает от нескольких часов до нескольких дней. Кроме того, для корректного вычисления целевой функции требуется серьезное программное обеспечение. Эти факторы существенно усложняют процесс разработки эффективных алгоритмов для решения задачи предсказания структуры белков и их комплексов. В настоящей работе предлагается упрощенная, модельная задача укладки графа на плоскости, которая позволяет проводить расчеты быстрее и без использования специального программного обеспечения, разрабатывать новые и улучшать существующие алгоритмы оптимизации. В работе приводятся результаты численного исследования ряда алгоритмов роевой и эволюционной оптимизации при решении поставленной модельной задачи.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maxim</FirstName>
<FamilyName>Bystrov</FamilyName>
<Email>far334_oxid@mail.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Ershov</FamilyName>
<Email>ershovnm@gmail.com</Email>
<Affiliation>Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Maxim</FirstName>
<FamilyName>Bystrov</FamilyName>
<Email>far334_oxid@mail.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>12. Bioinformatics</Track>
</abstract>
<abstract>
<Id>208</Id>
<Title>
Algorithms for the calculation of nonlinear processes on hybrid architecture clusters
</Title>
<Content>
The problem of porting programs from one hardware platform to another has not ceased to be less relevant and simpler with time. The need to transfer programs to platforms with different architectures can have different roots. One of them is to increase the efficiency of executing programs for mass calculations on multiprocessor systems and clusters. The purpose of our work is to identify the key features of algorithms in porting codes for calculating of essentially nonlinear processes to a modern cluster of hybrid architecture that includes both CPUs (Intel Xeon) and GPU (NVIDIA TESLA) processors. In order to increase cluster productivity by the well-known Amdahl law, it is necessary to achieve heterogeneoty of computational nodes. As a test problem for studying the process of porting a code to a cluster of hybrid architecture, the KPI equation of Kadomtsev-Petviashvili was chosen, written in integro-differential form [1]. As a result of the work, the procedure for porting a simulation code for a two-dimensional nonstationary model problem to a hybrid system is proposed. The features of such a transition are revealed. References [1] A.V. Bogdanov, V.V. Mareev. Numerical Simulation KPI Equation. Proceedings of the 15th International Ship Stability Workshop, 13-15 June 2016, Stockholm, Sweden. pp. 115-117.
</Content>
<field id="content">
The problem of porting programs from one hardware platform to another has not ceased to be less relevant and simpler with time. The need to transfer programs to platforms with different architectures can have different roots. One of them is to increase the efficiency of executing programs for mass calculations on multiprocessor systems and clusters. The purpose of our work is to identify the key features of algorithms in porting codes for calculating of essentially nonlinear processes to a modern cluster of hybrid architecture that includes both CPUs (Intel Xeon) and GPU (NVIDIA TESLA) processors. In order to increase cluster productivity by the well-known Amdahl law, it is necessary to achieve heterogeneoty of computational nodes. As a test problem for studying the process of porting a code to a cluster of hybrid architecture, the KPI equation of Kadomtsev-Petviashvili was chosen, written in integro-differential form [1]. As a result of the work, the procedure for porting a simulation code for a two-dimensional nonstationary model problem to a hybrid system is proposed. The features of such a transition are revealed. References [1] A.V. Bogdanov, V.V. Mareev. Numerical Simulation KPI Equation. Proceedings of the 15th International Ship Stability Workshop, 13-15 June 2016, Stockholm, Sweden. pp. 115-117.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Mareev</FamilyName>
<Email>map@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Storublevtzev</FamilyName>
<Email>100rub@mail.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>209</Id>
<Title>
Convolutional neural networks for self-driving cars on GPU
</Title>
<Content>
The challenge is to teach how to drive a vehicle without human with the help of deep learning power using visual data from the cameras installed on the machine. The problem is to process the amount of data in the real time. Convolutional neural networks (CNNs) are used for training data. And the idea of how to use CNNs on graphical processing units is described.
</Content>
<field id="content">
The challenge is to teach how to drive a vehicle without human with the help of deep learning power using visual data from the cameras installed on the machine. The problem is to process the amount of data in the real time. Convolutional neural networks (CNNs) are used for training data. And the idea of how to use CNNs on graphical processing units is described.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Boris</FirstName>
<FamilyName>Tyulkin</FamilyName>
<Email>pdmrn@mail.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nataliia</FirstName>
<FamilyName>Kulabukhova</FamilyName>
<Email>kulabukhova.nv@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nataliia</FirstName>
<FamilyName>Kulabukhova</FamilyName>
<Email>kulabukhova.nv@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>210</Id>
<Title>
MULTIPARTICLE PRODUCTION, NEGATIVE BINOMIAL DISTRIBUTION, SUPERSYMMETRIC DYNAMICS, STATISTICAL SUMS AND ZETA-FUNCTIONS
</Title>
<Content>
Introduction in multi-particle-production-processes and negative-binomial-distribution; Boson, fermion, and super oscillators and (statistical) mechanism of cosmological constant; finite approximation of the zeta-function and fermion factorization of the bosonic statistical sum considered.
</Content>
<field id="content">
Introduction in multi-particle-production-processes and negative-binomial-distribution; Boson, fermion, and super oscillators and (statistical) mechanism of cosmological constant; finite approximation of the zeta-function and fermion factorization of the bosonic statistical sum considered.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nugzar</FirstName>
<FamilyName>Makhaldiani</FamilyName>
<Email>mnv@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nugzar</FirstName>
<FamilyName>Makhaldiani</FamilyName>
<Email>mnv@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>211</Id>
<Title>Big data as the future of information technology</Title>
<Content>
Currently, the problem of "Big Data" is one of the most, if not the most urgent in computer science. Its solution implies the possibility of processing uncorrelated and heterogeneous data of large volume, the implementation of their integration from distributed sources by consolidation or federalization methods and ensuring the security of access and storage of these data. Only the creation of technology that provides processing and storage of dissimilar, uncorrelated data of large volume can be considered a breakthrough result corresponding to the world level. To effectively address these issues, a new definition of this concept is proposed, namely, "Big Data" is characterized by the situation when the conditions for implementing the CAP theorem are relevant. The CAP theorem is a heuristic statement that in any realization of distributed computations, it is impossible to provide the following three properties: Consistency, Availability and Partition Tolerance. Thus, depending on which of the properties cannot be implemented, we are dealing with different types of “Big data”. And this, in turn, means that a standard approach based on the MapReduce concept has a limited scope of applicability. Various possibilities for implementing data processing in different cases are discussed, and a conclusion is made about the need to create an ecosystem of “Big data”. The work will review the world market of Big Data technologies, and also describe the state of work on this problem in various countries. At the end of the article, we will talk about the opportunities that the solution of the problem opens for various fields of science and business.
</Content>
<field id="content">
Currently, the problem of "Big Data" is one of the most, if not the most urgent in computer science. Its solution implies the possibility of processing uncorrelated and heterogeneous data of large volume, the implementation of their integration from distributed sources by consolidation or federalization methods and ensuring the security of access and storage of these data. Only the creation of technology that provides processing and storage of dissimilar, uncorrelated data of large volume can be considered a breakthrough result corresponding to the world level. To effectively address these issues, a new definition of this concept is proposed, namely, "Big Data" is characterized by the situation when the conditions for implementing the CAP theorem are relevant. The CAP theorem is a heuristic statement that in any realization of distributed computations, it is impossible to provide the following three properties: Consistency, Availability and Partition Tolerance. Thus, depending on which of the properties cannot be implemented, we are dealing with different types of “Big data”. And this, in turn, means that a standard approach based on the MapReduce concept has a limited scope of applicability. Various possibilities for implementing data processing in different cases are discussed, and a conclusion is made about the need to create an ecosystem of “Big data”. The work will review the world market of Big Data technologies, and also describe the state of work on this problem in various countries. At the end of the article, we will talk about the opportunities that the solution of the problem opens for various fields of science and business.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Thurein</FirstName>
<FamilyName>Kyaw</FamilyName>
<Email>trkl.mm@mail.ru</Email>
<Affiliation>Lwin</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>212</Id>
<Title>
Deep Learning Methodology for Prediction of Long-Term Dynamics of Financial Derivatives
</Title>
<Content>
Algorithms for predicting the dynamics of stock options and other assets derivatives for both small times (where one plays on market fluctuations), and medium ones (where trade is stressed at the beginning and closing moments) are well developed, and trading robots are actively used for these purposes. Analysis of the dynamics of assets for very long time-frames (of several months order) is still beyond the scope of analysts as it is expensively prohibited, although this issue is extremely important for hedging the investments portfolios. In the paper the dynamic processes in the stock market in long-term periods are considered. Pricing of portfolio investments dynamics is made on the basis of neural networks using the deep learning and soft computing methodology. It does not require heavy computational resources, and their relatively low accuracy is not a disadvantage in tasks where only trends are subject to consideration. Operation with two and three layers neural networks produced until recently still unfitting results. However, emergence of the suggested approaches with specialized processors and software for learning the multi-layer networks has changed the situation. The most important factor is a high quality trained artificial neural network and its ability to predict for a long time-frame without retraining. The number of layers in experiments reached 250. For network input data the real ??? price series was taken dated from 1950 till 2017 with several one-day steps. Model predictions vs true ???; price performance has demonstrated practically acceptable compliance.
</Content>
<field id="content">
Algorithms for predicting the dynamics of stock options and other assets derivatives for both small times (where one plays on market fluctuations), and medium ones (where trade is stressed at the beginning and closing moments) are well developed, and trading robots are actively used for these purposes. Analysis of the dynamics of assets for very long time-frames (of several months order) is still beyond the scope of analysts as it is expensively prohibited, although this issue is extremely important for hedging the investments portfolios. In the paper the dynamic processes in the stock market in long-term periods are considered. Pricing of portfolio investments dynamics is made on the basis of neural networks using the deep learning and soft computing methodology. It does not require heavy computational resources, and their relatively low accuracy is not a disadvantage in tasks where only trends are subject to consideration. Operation with two and three layers neural networks produced until recently still unfitting results. However, emergence of the suggested approaches with specialized processors and software for learning the multi-layer networks has changed the situation. The most important factor is a high quality trained artificial neural network and its ability to predict for a long time-frame without retraining. The number of layers in experiments reached 250. For network input data the real ???; price series was taken dated from 1950 till 2017 with several one-day steps. Model predictions vs true ???; price performance has demonstrated practically acceptable compliance.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Rukovchuk</FamilyName>
<Email>vrukovchuk@gmail.com</Email>
<Affiliation>CSA</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Kirill</FirstName>
<FamilyName>Lysov</FamilyName>
<Email>thereis9000@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>213</Id>
<Title>
Comparison of different convolution neural network architectures for the solution of the problem of emotion recognition by facial expression
</Title>
<Content>
In this paper the usage of convolution neural networks considers for solving the problem of emotion recognition by face expression images. Emotion recognition is a complex task and the result of recognition is highly dependent on the choice of the neural network architecture. In this paper various architectures of convolutional neural networks were reviewed and there were selected the most prospective architectures. The training experiments were conducted on selected neural networks. The proposed neural network architectures were trained on the AffectNet dataset, widely used for emotion recognition experiments. A comparison of the proposed neural network architectures was made using the following metrics: accuracy, precision, recall and training speed. At the end of this paper the comparative analysis was made and obtained results were overviewed.
</Content>
<field id="content">
In this paper the usage of convolution neural networks considers for solving the problem of emotion recognition by face expression images. Emotion recognition is a complex task and the result of recognition is highly dependent on the choice of the neural network architecture. In this paper various architectures of convolutional neural networks were reviewed and there were selected the most prospective architectures. The training experiments were conducted on selected neural networks. The proposed neural network architectures were trained on the AffectNet dataset, widely used for emotion recognition experiments. A comparison of the proposed neural network architectures was made using the following metrics: accuracy, precision, recall and training speed. At the end of this paper the comparative analysis was made and obtained results were overviewed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Anton</FirstName>
<FamilyName>Vorontsov</FamilyName>
<Email>dealwithbotalfred@gmail.com</Email>
<Affiliation>-</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Averkin</FamilyName>
<Email>averkin2003@inbox.ru</Email>
<Affiliation>Informatics and Control Research Institute of Russian Academy of Sciences</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anton</FirstName>
<FamilyName>Vorontsov</FamilyName>
<Email>dealwithbotalfred@gmail.com</Email>
<Affiliation>-</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>214</Id>
<Title>
Parallel calculations of ground states of 6,7,9,11Li nuclei by Feynman’s continual integrals method
</Title>
<Content>
The structure of lithium isotopes and nuclear reactions with their participation are extensively studied both experimentally and theoretically. In this work, the wave functions of the ground states of few-body nuclei 6,7,9,11Li are calculated by Feynman’s continual integrals method in Euclidean time. The algorithm of parallel calculations was implemented in C++ programming language using NVIDIA CUDA technology. Calculations were performed on the NVIDIA Tesla K40 accelerator installed within the heterogeneous cluster of the Laboratory of Information Technologies, Joint Institute for Nuclear Research, Dubna. The studied isotopes are considered as cluster nuclei with the following configurations: 6Li (α + n + p), 7Li (α + n + n + p) 9Li (7Li + n + n), and 11Li (9Li + n + n). The results of calculations for the studied nuclei are in good agreement with the experimental energies of separation into clusters and nucleons. The obtained probability densities may be used for the correct definition of the initial conditions in the time-dependent calculations of reactions with the considered nuclei. This work was supported by the Russian Science Foundation (RSF), research project 17-12-01170.
</Content>
<field id="content">
The structure of lithium isotopes and nuclear reactions with their participation are extensively studied both experimentally and theoretically. In this work, the wave functions of the ground states of few-body nuclei 6,7,9,11Li are calculated by Feynman’s continual integrals method in Euclidean time. The algorithm of parallel calculations was implemented in C++ programming language using NVIDIA CUDA technology. Calculations were performed on the NVIDIA Tesla K40 accelerator installed within the heterogeneous cluster of the Laboratory of Information Technologies, Joint Institute for Nuclear Research, Dubna. The studied isotopes are considered as cluster nuclei with the following configurations: 6Li (α + n + p), 7Li (α + n + n + p) 9Li (7Li + n + n), and 11Li (9Li + n + n). The results of calculations for the studied nuclei are in good agreement with the experimental energies of separation into clusters and nucleons. The obtained probability densities may be used for the correct definition of the initial conditions in the time-dependent calculations of reactions with the considered nuclei. This work was supported by the Russian Science Foundation (RSF), research project 17-12-01170.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Naumenko</FamilyName>
<Email>anaumenko@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Viacheslav</FirstName>
<FamilyName>Samarin</FamilyName>
<Email>samarin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research, Flerov Laboratory of Nuclear Reactions
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Naumenko</FamilyName>
<Email>anaumenko@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>215</Id>
<Title>
The Usage of HPC Systems for Simulation of Dynamic Earthquake Process
</Title>
<Content>
Nowadays the HPC systems are very widespread in the world. Due to their computational power it is possible to simulate with a high precision a lot of phenomena: drugs development, seismic survey process, hydraulic fracturing and multi-component fluid flow, human-human interaction, high-speed collisions in open space, tsunami and earthquake initiation. That is why the development of modern applied research software for multi-processors systems are important. In the current work seismic waves generated during the earthquake process are considered. To describe precisely the dynamic behavior of the heterogeneous geological medium the 2D/3D full-wave system of elastic equations was used. Unfortunately, the analytical solution is available only for simple source and geometry of the area of interest. The grid-characteristic numerical method on curvilinear structured meshes was successfully applied. To achieve enough computational speed on large grids the research software designed by Khokhlov N.I. at MIPT was used. It is parallelized with OpenMP and MPI technologies with a good scalability up to thousands of CPU cores. A low-parameteric numerical model of hypocenter was introduced. As a verification a set of calculations for simple geological models in 3D were carried out. In 2D/3D cases the process of earthquake initiation at shelf was simulated. The contact between water (acoustic approximation) and geological bottom of the sea (full-wave elastic approximation) was explicitly taken into account. The magnitude at hypocenter was estimated with the Richter scale. The obtained time-spatial distribution of elastic stresses may be subsequently used in problems of strength of structures. The reported study was funded by RFBR according to the research project № 18-37-00127.
</Content>
<field id="content">
Nowadays the HPC systems are very widespread in the world. Due to their computational power it is possible to simulate with a high precision a lot of phenomena: drugs development, seismic survey process, hydraulic fracturing and multi-component fluid flow, human-human interaction, high-speed collisions in open space, tsunami and earthquake initiation. That is why the development of modern applied research software for multi-processors systems are important. In the current work seismic waves generated during the earthquake process are considered. To describe precisely the dynamic behavior of the heterogeneous geological medium the 2D/3D full-wave system of elastic equations was used. Unfortunately, the analytical solution is available only for simple source and geometry of the area of interest. The grid-characteristic numerical method on curvilinear structured meshes was successfully applied. To achieve enough computational speed on large grids the research software designed by Khokhlov N.I. at MIPT was used. It is parallelized with OpenMP and MPI technologies with a good scalability up to thousands of CPU cores. A low-parameteric numerical model of hypocenter was introduced. As a verification a set of calculations for simple geological models in 3D were carried out. In 2D/3D cases the process of earthquake initiation at shelf was simulated. The contact between water (acoustic approximation) and geological bottom of the sea (full-wave elastic approximation) was explicitly taken into account. The magnitude at hypocenter was estimated with the Richter scale. The obtained time-spatial distribution of elastic stresses may be subsequently used in problems of strength of structures. The reported study was funded by RFBR according to the research project № 18-37-00127.
</field>
<field id="summary">
The new approach for taking into account the heterogeneity of geological massif during the earthquake process simulation was investigated. To carry out numerical experiments for reasonable time the research software with multi-core/multi-processors parallelization was used.
</field>
<PrimaryAuthor>
<FirstName>Yulia</FirstName>
<FamilyName>Golubeva</FamilyName>
<Email>uma-mipt@mail.ru</Email>
<Affiliation>MIPT</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vasily</FirstName>
<FamilyName>Golubev</FamilyName>
<Email>w.golubev@mail.ru</Email>
<Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yulia</FirstName>
<FamilyName>Golubeva</FamilyName>
<Email>uma-mipt@mail.ru</Email>
<Affiliation>MIPT</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>216</Id>
<Title>
Combining satellite imagery and machine learning to predict atmospheric heavy metal contamination
</Title>
<Content>
We present an approach to predict atmospheric heavy metals contamination by statistical models and machine learning algorithms. The source of the field contamination data is ICP Vegetation Data Management System (DMS). DMS is a cloud platform developed at the Joint Institute of Nuclear Research (JINR) to manage ICP Vegetation data. The aim of the UNECE International Cooperative Program (ICP) Vegetation in the framework of the United Nations Convention on Long-Range Transboundary Air Pollution (CLRTAP) is to identify the main polluted areas of Europe and Asia. Currently there are 6000 sampling sites from 40 regions of different countries presented at the DMS now. The source of the satellite imagery is Google Earth Engine platform (GEE). There are more than 100 satellite programs and modeled datasets at GEE. We are taking data from GEE together with sampling data from DMS to train our deep neural models, but then on the next inference stage we apply the trained neural net to only data from GEE to predict atmospheric contamination by some of heavy metals. Correlation between the satellite imagery data and the heavy metals contamination, considered statistical models and modeling results are presented.
</Content>
<field id="content">
We present an approach to predict atmospheric heavy metals contamination by statistical models and machine learning algorithms. The source of the field contamination data is ICP Vegetation Data Management System (DMS). DMS is a cloud platform developed at the Joint Institute of Nuclear Research (JINR) to manage ICP Vegetation data. The aim of the UNECE International Cooperative Program (ICP) Vegetation in the framework of the United Nations Convention on Long-Range Transboundary Air Pollution (CLRTAP) is to identify the main polluted areas of Europe and Asia. Currently there are 6000 sampling sites from 40 regions of different countries presented at the DMS now. The source of the satellite imagery is Google Earth Engine platform (GEE). There are more than 100 satellite programs and modeled datasets at GEE. We are taking data from GEE together with sampling data from DMS to train our deep neural models, but then on the next inference stage we apply the trained neural net to only data from GEE to predict atmospheric contamination by some of heavy metals. Correlation between the satellite imagery data and the heavy metals contamination, considered statistical models and modeling results are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Uzhinskiy</FamilyName>
<Email>zalexandr@list.ru</Email>
<Affiliation>Dr.</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Goncharov</FamilyName>
<Email>kaliostrogoblin3@gmail.com</Email>
<Affiliation>Sukhoi State Technical University of Gomel, Gomel, Belarus</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Marina</FirstName>
<FamilyName>Frontsyeva</FamilyName>
<Email>marina@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Uzhinskiy</FamilyName>
<Email>zalexandr@list.ru</Email>
<Affiliation>Dr.</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>217</Id>
<Title>
Architecture and basic principles of the multifunctional platform for plant disease detection
</Title>
<Content>
The aim of our research is to facilitate the detection and preventing diseases of agricultural plants by combining deep learning and programming services. The idea is to develop multifunctional platform for plant disease detection (PDD) that will use modern organization and deep learning technologies to provide new level of service to farmer’s community. Web-platform for PDD consists of a set of interconnected services and tools developed, deployed and hosted in the JINR cloud infrastructure. We are going to use the only free software and to provide open access to our image database. The platform will include web-interface and a mobile application allowing users to send photos of sick plants with some accompanying text and then obtain a reply explaining a possible cause of the illness. PDD is intended for the data management of various crop data bases needed to train and test corresponding deep neural model. The platform will also provide effective, reliable, secure and convenient tools for storing, transferring and mining of the farmer's text and photo materials. We considered several models to identify the most appropriate type and architecture of deep neural network. The PDD basic principles together with results of comparative study of various deep neural models and their architecture are presented. Up to now we reached promising accuracy result on the level over 90% in the detection of three concrete diseases on the dataset of images of grape leaves.
</Content>
<field id="content">
The aim of our research is to facilitate the detection and preventing diseases of agricultural plants by combining deep learning and programming services. The idea is to develop multifunctional platform for plant disease detection (PDD) that will use modern organization and deep learning technologies to provide new level of service to farmer’s community. Web-platform for PDD consists of a set of interconnected services and tools developed, deployed and hosted in the JINR cloud infrastructure. We are going to use the only free software and to provide open access to our image database. The platform will include web-interface and a mobile application allowing users to send photos of sick plants with some accompanying text and then obtain a reply explaining a possible cause of the illness. PDD is intended for the data management of various crop data bases needed to train and test corresponding deep neural model. The platform will also provide effective, reliable, secure and convenient tools for storing, transferring and mining of the farmer's text and photo materials. We considered several models to identify the most appropriate type and architecture of deep neural network. The PDD basic principles together with results of comparative study of various deep neural models and their architecture are presented. Up to now we reached promising accuracy result on the level over 90% in the detection of three concrete diseases on the dataset of images of grape leaves.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Uzhinskiy</FamilyName>
<Email>zalexandr@list.ru</Email>
<Affiliation>Dr.</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Goncharov</FamilyName>
<Email>kaliostrogoblin3@gmail.com</Email>
<Affiliation>Sukhoi State Technical University of Gomel, Gomel, Belarus</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Uzhinskiy</FamilyName>
<Email>zalexandr@list.ru</Email>
<Affiliation>Dr.</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>218</Id>
<Title>
Хаотическая динамика мгновенного сердечного ритма и его фазовое пространство.
</Title>
<Content>
В данном докладе на основе данных суточного холтеровского мониторирования построены фазовые пространства мгновенного сердечного ритма четырех пациентов Тверского областного клинического кардиологического диспансера. Эти пространства наиболее адекватно отражают такие важные свойства кардиоритмов, как хаотичность и самоподобие (фрактальность). Приведены методы вычисления фрактальной размерности D и D-мерного объема фазового пространства мгновенного сердечного ритма в наиболее удобном для практического применения виде. На основе созданного и реализованного авторами комплекса программ проведено вычисление таких параметров состояния мгновенного сердечного ритма, как значение фрактальной размерности D, фрактального фазового объема Γ, определяемого по покрытию фазовой траектории сеткой единичного размера. Показана близость фазовых пространств мгновенного сердечного ритма исследуемых пациентов к фракталам с точностью 4.53∙10-2 в C-метрике. При временах холтеровского мониторирования превышающих 6 часов эти параметры стремятся к постоянным значениям и могут быть использованы как маркеры состояния сердечно-сосудистой системы в кардиодиагностике.
</Content>
<field id="content">
В данном докладе на основе данных суточного холтеровского мониторирования построены фазовые пространства мгновенного сердечного ритма четырех пациентов Тверского областного клинического кардиологического диспансера. Эти пространства наиболее адекватно отражают такие важные свойства кардиоритмов, как хаотичность и самоподобие (фрактальность). Приведены методы вычисления фрактальной размерности D и D-мерного объема фазового пространства мгновенного сердечного ритма в наиболее удобном для практического применения виде. На основе созданного и реализованного авторами комплекса программ проведено вычисление таких параметров состояния мгновенного сердечного ритма, как значение фрактальной размерности D, фрактального фазового объема Γ, определяемого по покрытию фазовой траектории сеткой единичного размера. Показана близость фазовых пространств мгновенного сердечного ритма исследуемых пациентов к фракталам с точностью 4.53∙10-2 в C-метрике. При временах холтеровского мониторирования превышающих 6 часов эти параметры стремятся к постоянным значениям и могут быть использованы как маркеры состояния сердечно-сосудистой системы в кардиодиагностике.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>tsvetkov.vp@tversu.ru</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Kudinov</FamilyName>
<Email>ucnam@yandex.ru</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ivanovcardio@yandex.ru</Email>
<Affiliation>Tver cardiology health center</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>ilya.v.tsvetkov@gmail.com</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Mikheev</FamilyName>
<Email>sergjan800@rambler.ru</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>tsvetkov.vp@tversu.ru</Email>
<Affiliation>Tver State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>12. Bioinformatics</Track>
</abstract>
<abstract>
<Id>219</Id>
<Title>
Визуализация квантового фазового пространства мгновенного сердечного ритма
</Title>
<Content>
Данные суточного холтеровского мониторирования (ХМ) по кардиоинтервалам для анализа представляются в форме, сочетающей простоту и информативность. Показано, что это можно сделать, используя визуализацию массива данных по кардиоритмам на основе квантового фазового пространства мгновенного сердечного ритма. Под визуализацией квантового фазового пространства мгновенного сердечного ритма мы будем понимать способ представления цифровой информации о мгновенном сердечном ритме в виде, удобном для наблюдения и анализа. Сформулированное в докладе квантование фазового пространства мгновенного сердечного ритма приводит его к делению на ячейки конечной величины h и объема ΔΓ=h^2. Информация о структуре фазового пространстве мгновенного сердечного ритма при этом будет определяться числами заполнения этих ячеек. Важнейшей задачей нашего подхода является визуализация точек кавантового фазового пространства мгновенного сердечного ритма. Для этого его точкам приписали определённые значения цвета в зависимости от значений чисел заполнения состояний.
</Content>
<field id="content">
Данные суточного холтеровского мониторирования (ХМ) по кардиоинтервалам для анализа представляются в форме, сочетающей простоту и информативность. Показано, что это можно сделать, используя визуализацию массива данных по кардиоритмам на основе квантового фазового пространства мгновенного сердечного ритма. Под визуализацией квантового фазового пространства мгновенного сердечного ритма мы будем понимать способ представления цифровой информации о мгновенном сердечном ритме в виде, удобном для наблюдения и анализа. Сформулированное в докладе квантование фазового пространства мгновенного сердечного ритма приводит его к делению на ячейки конечной величины h и объема ΔΓ=h^2. Информация о структуре фазового пространстве мгновенного сердечного ритма при этом будет определяться числами заполнения этих ячеек. Важнейшей задачей нашего подхода является визуализация точек кавантового фазового пространства мгновенного сердечного ритма. Для этого его точкам приписали определённые значения цвета в зависимости от значений чисел заполнения состояний.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>ilya.v.tsvetkov@gmail.com</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>tsvetkov.vp@tversu.ru</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Mikheev</FamilyName>
<Email>sergjan800@rambler.ru</Email>
<Affiliation>Tver State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Tsvetkov</FamilyName>
<Email>tsvetkov.vp@tversu.ru</Email>
<Affiliation>Tver State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>12. Bioinformatics</Track>
</abstract>
<abstract>
<Id>220</Id>
<Title>Text segmentation on photorealistic images</Title>
<Content>
The paper proposes an algorithm for segmentation of text, applied or presented in photorealistic images, characterized by a complex background. Because of its application, the exact location of image regions containing text is determined. The algorithm implements the method for semantic segmentation of images, while the text symbols serve as detectable objects. The original images are pre-processed and fed to the input of the pre-trained convolutional neural network. The paper proposes a network architecture for text segmentation, describes the procedure for the formation of the training set, and considers the algorithm for pre-processing images, reducing the amount of processed data and simplifying the segmentation of the object "background". The network architecture is a modification of well-known ResNet network and takes into account the specifics of text character images. The convolutional neural network is implemented using CUDA parallel computing technology at the GPU. The experimental results for evaluating quality of the text segmentation IoU (Intersection over Union) criterion have proved effectiveness of the proposed method.
</Content>
<field id="content">
The paper proposes an algorithm for segmentation of text, applied or presented in photorealistic images, characterized by a complex background. Because of its application, the exact location of image regions containing text is determined. The algorithm implements the method for semantic segmentation of images, while the text symbols serve as detectable objects. The original images are pre-processed and fed to the input of the pre-trained convolutional neural network. The paper proposes a network architecture for text segmentation, describes the procedure for the formation of the training set, and considers the algorithm for pre-processing images, reducing the amount of processed data and simplifying the segmentation of the object "background". The network architecture is a modification of well-known ResNet network and takes into account the specifics of text character images. The convolutional neural network is implemented using CUDA parallel computing technology at the GPU. The experimental results for evaluating quality of the text segmentation IoU (Intersection over Union) criterion have proved effectiveness of the proposed method.
</field>
<field id="summary">
We propose the algorithm for the segmentation of text regions in photorealistic images. It consists of a preprocessing step, a recognition step, and a localization step. The second step uses the modified convolutional ResNet network for recognition. Unlike the original network, the modified neural network saves the geometric structure of text characters into the feature maps. The third step determines the exact localization of recognized text characters and finds areas of the image containing text. Experimental results show effectiveness of the proposed algorithm. Quality of segmentation is evaluated using the IoU metric and reaches 78%, which is sufficient for further processing of the image text using OCR systems. Use of parallel processing technologies significantly reduces processing time of large series of images.
</field>
<PrimaryAuthor>
<FirstName>Valery</FirstName>
<FamilyName>Grishkin</FamilyName>
<Email>valery-grishkin@yandex.ru</Email>
<Affiliation>SPbGU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Aleksaner</FirstName>
<FamilyName>Ebral</FamilyName>
<Email>aleksandr.ebr@gmail.com</Email>
<Affiliation>SPbGU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolai</FirstName>
<FamilyName>Stepenko</FamilyName>
<Email>n.stepenko@spbu.ru</Email>
<Affiliation>SPbGu</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jean</FirstName>
<FamilyName>Sene</FamilyName>
<Email>senejeanvalery@yahoo.fr</Email>
<Affiliation>SPbGU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Valery</FirstName>
<FamilyName>Grishkin</FamilyName>
<Email>valery-grishkin@yandex.ru</Email>
<Affiliation>SPbGU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>221</Id>
<Title>
Integrating LEAF to data management workflow in LHAASO
</Title>
<Content>
Nowadays, data storage and management in cloud computing environment has been very important in high energy physics filed. The LHAASO(Large High Altitude Air Shower Observatory) experiment of IHEP will generate 2 PB per year in the future. These massive data processing faces many challenges in the distributed computing environment. For example, some sites may have no local HEP storage which made the distributed computing unavailable. Our goal is to make the data available for LHAASO in any remote sites. In our architecture, we use EOS as our local storage system, and use LEAF as the data federation system. LEAF is a data cache and access system across remote sites proposed by IHEP. LEAF can present one same file system view at local and the remote sites, supporting directly data access on demand. In this paper, we will present the whole data management architecture, data workflow and performance evaluation of LEAF in LHAASO.
</Content>
<field id="content">
Nowadays, data storage and management in cloud computing environment has been very important in high energy physics filed. The LHAASO(Large High Altitude Air Shower Observatory) experiment of IHEP will generate 2 PB per year in the future. These massive data processing faces many challenges in the distributed computing environment. For example, some sites may have no local HEP storage which made the distributed computing unavailable. Our goal is to make the data available for LHAASO in any remote sites. In our architecture, we use EOS as our local storage system, and use LEAF as the data federation system. LEAF is a data cache and access system across remote sites proposed by IHEP. LEAF can present one same file system view at local and the remote sites, supporting directly data access on demand. In this paper, we will present the whole data management architecture, data workflow and performance evaluation of LEAF in LHAASO.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Haibo</FirstName>
<FamilyName>Li</FamilyName>
<Email>lihaibo@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics,Chinese Academy of Sciences</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Yaodong</FirstName>
<FamilyName>Cheng</FamilyName>
<Email>chyd@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qi</FirstName>
<FamilyName>XU</FamilyName>
<Email>xuq@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics, Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qiulan</FirstName>
<FamilyName>Huang</FamilyName>
<Email>huangql@ihep.ac.cn</Email>
<Affiliation>
Institute of High Energy of Physics(IHEP), Chinese Academy of Science
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Haibo</FirstName>
<FamilyName>Li</FamilyName>
<Email>lihaibo@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics,Chinese Academy of Sciences</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>222</Id>
<Title>
Botnet in PyPy to speed up the work of the Earley parser
</Title>
<Content>
Extraction of information from texts is a crucial task in the area of Natural Language Processing. It includes such tasks as named-entity recognition, relationship extraction, coreference resolution, etc. These problems are being resolved using two approaches. The first one is the rules-based approach and the second one is machine learning. Solutions based on machine learning are currently very popular but work well only with frequently used entities such as person, company, or organization. They require the presence of a large tagged dataset. With attributes from narrow subject areas and facts, machine learning works much worse. It is better to use this approach in writing rules for context-free grammars. The problem here is that the more grammars there are, the slower the analyzer works. Often the speed of the algorithm is less than 1 KB of text per second. For my project concerned with collecting information and statistics in the subject area of oil and gas geology, I created a system that includes a botnet using the Selenium library, in which one computer generates search queries from a list of objects and collects the resulting links. Then, the resulting links send the tasks to other computers via the REST service based on the asynchronous queue implemented in the Flask framework. To avoid link duplication, hashing was used with Redis. Next, the task is occupied by a botnet consisting of various computers. They take the task and depending on the content of the link carry out the following: if this is an ordinary site, then its contents are parsed; if this is a doc/docx/pdf document, then it is downloaded and then text is extracted from it using the textract library. After saving the text, algorithms are used to extract entities and thematic attributes using the GRL parser on context-free grammars. The first step to accelerate is the use of distributed computing as described above. The second step in the acceleration that was undertaken in this study is the use of the PyPy interpreter for Python, which compiles the code in the C language. It accelerated the work of the algorithm 4 times on average, but the consumption of RAM increased by ~ 25%. The calculations involved 8 computers with 4 threads each. Thus, the use of distributed computations together with the replacement of the standard Python interpreter with PyPy allowed to increase the speed of the extraction of facts increased ~ 128 times.
</Content>
<field id="content">
Extraction of information from texts is a crucial task in the area of Natural Language Processing. It includes such tasks as named-entity recognition, relationship extraction, coreference resolution, etc. These problems are being resolved using two approaches. The first one is the rules-based approach and the second one is machine learning. Solutions based on machine learning are currently very popular but work well only with frequently used entities such as person, company, or organization. They require the presence of a large tagged dataset. With attributes from narrow subject areas and facts, machine learning works much worse. It is better to use this approach in writing rules for context-free grammars. The problem here is that the more grammars there are, the slower the analyzer works. Often the speed of the algorithm is less than 1 KB of text per second. For my project concerned with collecting information and statistics in the subject area of oil and gas geology, I created a system that includes a botnet using the Selenium library, in which one computer generates search queries from a list of objects and collects the resulting links. Then, the resulting links send the tasks to other computers via the REST service based on the asynchronous queue implemented in the Flask framework. To avoid link duplication, hashing was used with Redis. Next, the task is occupied by a botnet consisting of various computers. They take the task and depending on the content of the link carry out the following: if this is an ordinary site, then its contents are parsed; if this is a doc/docx/pdf document, then it is downloaded and then text is extracted from it using the textract library. After saving the text, algorithms are used to extract entities and thematic attributes using the GRL parser on context-free grammars. The first step to accelerate is the use of distributed computing as described above. The second step in the acceleration that was undertaken in this study is the use of the PyPy interpreter for Python, which compiles the code in the C language. It accelerated the work of the algorithm 4 times on average, but the consumption of RAM increased by ~ 25%. The calculations involved 8 computers with 4 threads each. Thus, the use of distributed computations together with the replacement of the standard Python interpreter with PyPy allowed to increase the speed of the extraction of facts increased ~ 128 times.
</field>
<field id="summary">
The use of distributed computations together with the replacement of the standard Python interpreter with PyPy allowed to increase the speed of the extraction of facts increased ~ 128 times using GRL parser on context-free grammars.
</field>
<PrimaryAuthor>
<FirstName>Vladislav</FirstName>
<FamilyName>Radishevskiy</FamilyName>
<Email>vladrad95@mail.ru</Email>
<Affiliation>Leonidovich</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Aleksey</FirstName>
<FamilyName>Kulnevich</FamilyName>
<Email>kulnevich94@mail.ru</Email>
<Affiliation>Dmitrievich</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladislav</FirstName>
<FamilyName>Radishevskiy</FamilyName>
<Email>vladrad95@mail.ru</Email>
<Affiliation>Leonidovich</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>223</Id>
<Title>
Machine learning for natural language processing tasks
</Title>
<Content>
There are two popular algorithms for text vector extraction: bag of words and skip-gram. The intuition behind it is that a word can be predicted by context and context can be predicted from a word. The vector size of a word is the number of neurons in the hidden layer. The task of named entity recognition can be solved by using LSTM neural networks. The features for every word can be word-embeddings (skip-gram or bag of words model), char-embeddings features, and additional features, for example, morphological. To solve this task, we used a tagged dataset (where a human choose which words are entities like a Person, Organization, Location or Product type). We used the softmax function in a neural network for classification. Also, is possible to use other approaches like CRF. There are many neural architectures for the problem of named entity recognition. After that, it is possible to teach our model to predict the entities of predefined types. There are many approaches for text classification, and for vectorization it is possible to use document-embeddings (doc2vec model) or TF-IDF. After this, it is possible to use classification algorithms like an SVM or Random Forest model. To verify the classification task, it is possible to use the most important words in class (for example 20-30 most important words can include the terms which characterize the class).
</Content>
<field id="content">
There are two popular algorithms for text vector extraction: bag of words and skip-gram. The intuition behind it is that a word can be predicted by context and context can be predicted from a word. The vector size of a word is the number of neurons in the hidden layer. The task of named entity recognition can be solved by using LSTM neural networks. The features for every word can be word-embeddings (skip-gram or bag of words model), char-embeddings features, and additional features, for example, morphological. To solve this task, we used a tagged dataset (where a human choose which words are entities like a Person, Organization, Location or Product type). We used the softmax function in a neural network for classification. Also, is possible to use other approaches like CRF. There are many neural architectures for the problem of named entity recognition. After that, it is possible to teach our model to predict the entities of predefined types. There are many approaches for text classification, and for vectorization it is possible to use document-embeddings (doc2vec model) or TF-IDF. After this, it is possible to use classification algorithms like an SVM or Random Forest model. To verify the classification task, it is possible to use the most important words in class (for example 20-30 most important words can include the terms which characterize the class).
</field>
<field id="summary">
This paper explains the basics of using machine learning in natural language processing and describes a neural network architecture for named entity recognition and text classification by topic.
</field>
<PrimaryAuthor>
<FirstName>Aleksey</FirstName>
<FamilyName>Kulnevich</FamilyName>
<Email>kulnevich94@mail.ru</Email>
<Affiliation>Dmitrievich</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladislav</FirstName>
<FamilyName>Radishevskiy</FamilyName>
<Email>vladrad95@mail.ru</Email>
<Affiliation>Leonidovich</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Aleksey</FirstName>
<FamilyName>Kulnevich</FamilyName>
<Email>kulnevich94@mail.ru</Email>
<Affiliation>Dmitrievich</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>224</Id>
<Title>
Cache-friendly memory traversal to improve performance of grid-characteristic method
</Title>
<Content>
We consider well known cache optimization techniques to find the most efficient one when applied to grid-characteristic method. Grid-characteristic method is used to solve elastic wave equation. Elastic wave equation is hyperbolic system of equations inferred from model of linear elastic material, which describes propagation of elastic waves in deformable rigid bodies. The solution to this problem is important in seismic tomography and exploration geophysics. In this work only 2D scenario is studied, but it has an extension to 3D. Grid-characteristic method consists of a bunch of iterations over the array representing nodes of computational grid. The increase of performance due to change of array traversal order is connected with the spatial and time locality of memory accesses. The following 3 techniques are evaluated: bypassing of memory in rectangular blocks (block tiling), in blocks of diamond shape (diamond tiling), and in recurrently nested tiles of smaller size (hierarchical tiling). In the case of block tiling we achieved highest performance gain (about 15%). In contrast, performance with diamond tiling is declined by 6% and with hierarchical tiling is dropped by 13%. We assume that last two methods degrade performance because the amount of memory for single grid node is too large. Therefore all the necessary local nodes for block and hierarchical tiling can’t fit simultaneously in L1 cache. We have concluded that block tiling is the most appropriate technique for grid-characteristic method optimization. The reported study was funded by RFBR according to the research project № 18-07-00914 A.
</Content>
<field id="content">
We consider well known cache optimization techniques to find the most efficient one when applied to grid-characteristic method. Grid-characteristic method is used to solve elastic wave equation. Elastic wave equation is hyperbolic system of equations inferred from model of linear elastic material, which describes propagation of elastic waves in deformable rigid bodies. The solution to this problem is important in seismic tomography and exploration geophysics. In this work only 2D scenario is studied, but it has an extension to 3D. Grid-characteristic method consists of a bunch of iterations over the array representing nodes of computational grid. The increase of performance due to change of array traversal order is connected with the spatial and time locality of memory accesses. The following 3 techniques are evaluated: bypassing of memory in rectangular blocks (block tiling), in blocks of diamond shape (diamond tiling), and in recurrently nested tiles of smaller size (hierarchical tiling). In the case of block tiling we achieved highest performance gain (about 15%). In contrast, performance with diamond tiling is declined by 6% and with hierarchical tiling is dropped by 13%. We assume that last two methods degrade performance because the amount of memory for single grid node is too large. Therefore all the necessary local nodes for block and hierarchical tiling can’t fit simultaneously in L1 cache. We have concluded that block tiling is the most appropriate technique for grid-characteristic method optimization. The reported study was funded by RFBR according to the research project № 18-07-00914 A.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ip-e@mail.ru</Email>
<Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Khokhlov</FamilyName>
<Email>k_h@inbox.ru</Email>
<Affiliation>MIPT</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ip-e@mail.ru</Email>
<Affiliation>Moscow Institute of Physics and Technology</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>225</Id>
<Title>
Scalable semantic virtual machine framework for language-agnostic static analysis
</Title>
<Content>
The more static program analysis spreads in the industry, the clearer it becomes that its real-world usage requires something more than just optimized algorithms to make execution fast. Distribution of computations to cluster nodes is one of the viable ways for speeding up the process. The research demonstrates one of the approaches for organizing distributed static analysis - "Semantic Virtual Machines", which is based upon symbolic execution of language-agnostic intermediate codes of programs. These codes belong to the developed language for program representation with semantic objects behind. As objects are fully serializable, they can be saved to and later retrieved from a distributed database. That opens up a possibility for making an analyzer spread to many cluster nodes. Semantic Hypervisor is the core of the system which acts as a master node for managing runnable virtual machines and also a relay between them and a semantics storage. Main efforts are put into developing of intercommunication mechanisms which can be effective in most of the scenarios, as well as into integration of the approach in existing static analyzer. The study shows the architecture, describes its advantages and disadvantages, suggests solutions for the biggest problems observed during research. The empirical study shows improved performance compared to single node run, and the result is almost linearly scalable due to the high locality of data. The main result is the created scalable and unified language-agnostic architecture for static analysis with semantics data in a distributed storage behind of it. The significance of the research is in high achieved performance level alongside with a clean architecture.
</Content>
<field id="content">
The more static program analysis spreads in the industry, the clearer it becomes that its real-world usage requires something more than just optimized algorithms to make execution fast. Distribution of computations to cluster nodes is one of the viable ways for speeding up the process. The research demonstrates one of the approaches for organizing distributed static analysis - "Semantic Virtual Machines", which is based upon symbolic execution of language-agnostic intermediate codes of programs. These codes belong to the developed language for program representation with semantic objects behind. As objects are fully serializable, they can be saved to and later retrieved from a distributed database. That opens up a possibility for making an analyzer spread to many cluster nodes. Semantic Hypervisor is the core of the system which acts as a master node for managing runnable virtual machines and also a relay between them and a semantics storage. Main efforts are put into developing of intercommunication mechanisms which can be effective in most of the scenarios, as well as into integration of the approach in existing static analyzer. The study shows the architecture, describes its advantages and disadvantages, suggests solutions for the biggest problems observed during research. The empirical study shows improved performance compared to single node run, and the result is almost linearly scalable due to the high locality of data. The main result is the created scalable and unified language-agnostic architecture for static analysis with semantics data in a distributed storage behind of it. The significance of the research is in high achieved performance level alongside with a clean architecture.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maxim</FirstName>
<FamilyName>Menshchikov</FamilyName>
<Email>maximmenshchikov@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Maxim</FirstName>
<FamilyName>Menshchikov</FamilyName>
<Email>maximmenshchikov@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>226</Id>
<Title>.NET Core technology in scientific tasks</Title>
<Content>
Today we have an established stack of tools to develop applications, systems and services for scientific purposes. However, not so long ago a new technology, called .NET Core appeared. It’s supervised by Microsoft, but its open-source and developed by a wide community of programmers and engineers. That technology has a lot of advantages like high performance, simple and productive parallel programming abilities, support of high-level programming languages (C# 7.0, F#) and so on. But the main advantage in comparison with its predecessors is cross-platform abilities. Once written code can be natively compiled on a large number of platforms and hardware systems. It can be used on Windows, Linux, Mac and all Unix-based operation systems. It supports different hardware components and processors, for example, it can be used on ARM processors. The technology supports containerization and can be used with Docker or Kubernetes. It gives an ability to develop applications with micro-service architecture form the box and provides convenient deployment tools. Now, in a set of tasks .NET Core surpasses currently used tools in scientific sphere tools like Python, Go, Ruby, Java, Node.JS and others. Also, it can be used to develop native desktop applications with HTML-based GUI for administration and monitoring purposes.
</Content>
<field id="content">
Today we have an established stack of tools to develop applications, systems and services for scientific purposes. However, not so long ago a new technology, called .NET Core appeared. It’s supervised by Microsoft, but its open-source and developed by a wide community of programmers and engineers. That technology has a lot of advantages like high performance, simple and productive parallel programming abilities, support of high-level programming languages (C# 7.0, F#) and so on. But the main advantage in comparison with its predecessors is cross-platform abilities. Once written code can be natively compiled on a large number of platforms and hardware systems. It can be used on Windows, Linux, Mac and all Unix-based operation systems. It supports different hardware components and processors, for example, it can be used on ARM processors. The technology supports containerization and can be used with Docker or Kubernetes. It gives an ability to develop applications with micro-service architecture form the box and provides convenient deployment tools. Now, in a set of tasks .NET Core surpasses currently used tools in scientific sphere tools like Python, Go, Ruby, Java, Node.JS and others. Also, it can be used to develop native desktop applications with HTML-based GUI for administration and monitoring purposes.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Dorokhin</FamilyName>
<Email>victor.doroh@gmail.com</Email>
<Affiliation>Dubna University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Dorokhin</FamilyName>
<Email>victor.doroh@gmail.com</Email>
<Affiliation>Dubna University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>227</Id>
<Title>Improving Networking Performance of a Linux Node</Title>
<Content>
Linux networking performance is excellent. Linux networking stack is perfect, it is very effective. There are many options that could be configured for different cases. This article is devoted to questions related to networking stack implementation and configuration. Questions related to effective configuration will be discussed. Possible problems and solutions will be shown. Different options for configuration will be discussed. Questions will be discussed on the example of a computational cluster as well as on the example of a generic node. Effective usage of existing solutions is very important in both cases. Different aspects of configuration of the networking stack in general, configuration of TCP/IP stack, configuration of networking interfaces, real as well as virtual, are discussed in this article. In the end several recommendations are given. Keywords: computational clusters, Linux, networking, networking protocols, kernel, sockets, NAPI, GSO, GRO.
</Content>
<field id="content">
Linux networking performance is excellent. Linux networking stack is perfect, it is very effective. There are many options that could be configured for different cases. This article is devoted to questions related to networking stack implementation and configuration. Questions related to effective configuration will be discussed. Possible problems and solutions will be shown. Different options for configuration will be discussed. Questions will be discussed on the example of a computational cluster as well as on the example of a generic node. Effective usage of existing solutions is very important in both cases. Different aspects of configuration of the networking stack in general, configuration of TCP/IP stack, configuration of networking interfaces, real as well as virtual, are discussed in this article. In the end several recommendations are given. Keywords: computational clusters, Linux, networking, networking protocols, kernel, sockets, NAPI, GSO, GRO.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Gaiduchok</FamilyName>
<Email>gvladimiru@gmail.com</Email>
<Affiliation>
Saint Petersburg Electrotechnical University "LETI", Russia
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nabil</FirstName>
<FamilyName>Ahmed</FamilyName>
<Email>aboroan1987@yahoo.com</Email>
<Affiliation>Saint Petersburg Electrotechnical University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Gaiduchok</FamilyName>
<Email>gvladimiru@gmail.com</Email>
<Affiliation>
Saint Petersburg Electrotechnical University "LETI", Russia
</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>228</Id>
<Title>
Когнитивно интеллектуальная система диагностики, адаптации и обучения детей-аутистов. Модуль обработки данных
</Title>
<Content>
Когнитивно-интеллектуальная система адаптации и обучения детей-аутистов предназначена для извлечения, обработки и формирования программы обучения на основе когнитивных процессов, в частности ЭЭГ сигналов, адаптации детей- аутистов к социуму и обучения базовым бытовым навыкам. Основной частью КИСДАОДА является модуль обработки данных. Модуль обработки данных представляет собой структуру взаимодействия ребёнка и программы обучения посредством применения нечеткой логики. Модуль обработки данных предназначен для извлечения ЭЭГ посредством когнитивного шлема, обработки и фильтрации полученного сигнала, формирования программы обучения на основе когнитивных процессов, диагностики проблем работы ребёнка с системой и оценки реакции оператора на задания, сформированные модулем обучения. Модуль обработки данных состоит из: 1. Обработчика ЭЭГ, который выводит необработанный сигнал ЭЭГ с датчиков когнитивного шлема, фильтрует и обрабатывает. 2. Модуля распознавания эмоций. 3. Шины, формирующей пакет данных для передачи их на вход оптимизатора баз знаний. 4. Интерпретатора полученного коэффициента от оптимизатора баз знаний. В результате работы модуля обработки данных в блок обучения передаётся сформированный параметр оценки задания. Таким образом происходит корректировка программы обучения и индивидуализация системы под конкретного ребёнка.
</Content>
<field id="content">
Когнитивно-интеллектуальная система адаптации и обучения детей-аутистов предназначена для извлечения, обработки и формирования программы обучения на основе когнитивных процессов, в частности ЭЭГ сигналов, адаптации детей- аутистов к социуму и обучения базовым бытовым навыкам. Основной частью КИСДАОДА является модуль обработки данных. Модуль обработки данных представляет собой структуру взаимодействия ребёнка и программы обучения посредством применения нечеткой логики. Модуль обработки данных предназначен для извлечения ЭЭГ посредством когнитивного шлема, обработки и фильтрации полученного сигнала, формирования программы обучения на основе когнитивных процессов, диагностики проблем работы ребёнка с системой и оценки реакции оператора на задания, сформированные модулем обучения. Модуль обработки данных состоит из: 1. Обработчика ЭЭГ, который выводит необработанный сигнал ЭЭГ с датчиков когнитивного шлема, фильтрует и обрабатывает. 2. Модуля распознавания эмоций. 3. Шины, формирующей пакет данных для передачи их на вход оптимизатора баз знаний. 4. Интерпретатора полученного коэффициента от оптимизатора баз знаний. В результате работы модуля обработки данных в блок обучения передаётся сформированный параметр оценки задания. Таким образом происходит корректировка программы обучения и индивидуализация системы под конкретного ребёнка.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Shevchenko</FamilyName>
<Email>s13m@yandex.ru</Email>
<Affiliation>Vladimirovich</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alla</FirstName>
<FamilyName>Mamaeva</FamilyName>
<Email>allamamaeva.d@gmail.com</Email>
<Affiliation>Alexandrovna</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Shevchenko</FamilyName>
<Email>s13m@yandex.ru</Email>
<Affiliation>Vladimirovich</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>229</Id>
<Title>
Когнитивно-интеллектуальная система адаптации и обучения детей-аутистов
</Title>
<Content>
Когнитивно-интеллектуальная система адаптации и обучения детей-аутистов предназначена для извлечения, обработки и формирования программы обучения на основе когнитивных процессов, в частности ЭЭГ сигналов, адаптации детей- аутистов к социуму и обучения базовым бытовым навыкам. Рассмотрена и проанализирована возможность программной реализации определения уровня эмоционального возбуждения. Проведенная работа демонстрирует оптимальную обучаемость системы, возможность создания БЗ на основе регистрируемого сигнала ЭЭГ и использования полученных результатов для распознавания эмоций. Использование интеллектуальной надстройки в виде ОБЗ, основанной на нечеткой логике, в распознавании эмоций является наиболее оптимальным решением по сравнению с использованием нейронных сетей, которые представляют из себя исключительно абстрактный математический аппарат.
</Content>
<field id="content">
Когнитивно-интеллектуальная система адаптации и обучения детей-аутистов предназначена для извлечения, обработки и формирования программы обучения на основе когнитивных процессов, в частности ЭЭГ сигналов, адаптации детей- аутистов к социуму и обучения базовым бытовым навыкам. Рассмотрена и проанализирована возможность программной реализации определения уровня эмоционального возбуждения. Проведенная работа демонстрирует оптимальную обучаемость системы, возможность создания БЗ на основе регистрируемого сигнала ЭЭГ и использования полученных результатов для распознавания эмоций. Использование интеллектуальной надстройки в виде ОБЗ, основанной на нечеткой логике, в распознавании эмоций является наиболее оптимальным решением по сравнению с использованием нейронных сетей, которые представляют из себя исключительно абстрактный математический аппарат.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alla</FirstName>
<FamilyName>Mamaeva</FamilyName>
<Email>allamamaeva.d@gmail.com</Email>
<Affiliation>Alexandrovna</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Shevchenko</FamilyName>
<Email>s13m@yandex.ru</Email>
<Affiliation>Vladimirochiv</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alla</FirstName>
<FamilyName>Mamaeva</FamilyName>
<Email>allamamaeva.d@gmail.com</Email>
<Affiliation>Alexandrovna</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>230</Id>
<Title>Distrubited virtual cluster management system</Title>
<Content>
An effective cluster management system is the key to solving many problems that arise in the field of distributed computing. The wide spread of computing clusters led to the active development of task management systems, however, many issues, in particular, migration issues, have not yet been resolved. In this paper, we consider the organization of a virtual cluster created with virtualization at the OS level, as well as issues related to the dynamic migration of individual processes and containers. The complexity of this task within the cluster is determined by stringent requirements, a large set of process and container state parameters, and the possibility of using specialized equipment. The ability to restore the state of a container to another node is a complex task that requires the development and implementation of multiple subsystems. Migration of containers and processes is much more difficult than migration of virtual machines because of close integration into the OS and the ability to work with individual components of equipment directly: you need to restore the state of individual subsystems, while in the case of a traditional virtual machine, the VM works with virtual equipment, provided by the hypervisor, and the state of the guest OS is inside the VM itself. Migration of processes and containers is an actively developing direction at present. We will present and discuss a technique for managing distributed heterogeneous virtual clusters using virtualization at the OS level; a technique of ensuring reliability, fault tolerance and load balancing of computing clusters due to the dynamic migration of tasks within a virtual cluster; an architecture of a virtual computer network for a computing cluster, minimizing the overhead associated with data exchange, for a specific task.
</Content>
<field id="content">
An effective cluster management system is the key to solving many problems that arise in the field of distributed computing. The wide spread of computing clusters led to the active development of task management systems, however, many issues, in particular, migration issues, have not yet been resolved. In this paper, we consider the organization of a virtual cluster created with virtualization at the OS level, as well as issues related to the dynamic migration of individual processes and containers. The complexity of this task within the cluster is determined by stringent requirements, a large set of process and container state parameters, and the possibility of using specialized equipment. The ability to restore the state of a container to another node is a complex task that requires the development and implementation of multiple subsystems. Migration of containers and processes is much more difficult than migration of virtual machines because of close integration into the OS and the ability to work with individual components of equipment directly: you need to restore the state of individual subsystems, while in the case of a traditional virtual machine, the VM works with virtual equipment, provided by the hypervisor, and the state of the guest OS is inside the VM itself. Migration of processes and containers is an actively developing direction at present. We will present and discuss a technique for managing distributed heterogeneous virtual clusters using virtualization at the OS level; a technique of ensuring reliability, fault tolerance and load balancing of computing clusters due to the dynamic migration of tasks within a virtual cluster; an architecture of a virtual computer network for a computing cluster, minimizing the overhead associated with data exchange, for a specific task.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Amissi</FirstName>
<FamilyName>Cubahiro</FamilyName>
<Email>amcubahiro@gmail.com</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI", Russia</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Gaiduchok</FamilyName>
<Email>gvladimiru@gmail.com</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI", Russia</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>MAGDALYNE</FirstName>
<FamilyName>KAMANDE</FamilyName>
<Email>magdalynde@gmail.com</Email>
<Affiliation>St Peterburg State Electrotechnical University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Gaiduchok</FamilyName>
<Email>gvladimiru@gmail.com</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI", Russia</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>231</Id>
<Title>
Data consolidation and analysis system for brain research
</Title>
<Content>
Comprehensive human studies, in particular studies in the field of brain pathology, require strong information support for the consolidation of clinical and biological data from different sources in order to allow processing and analysis of data. The heterogeneity of data sources, the variety of presentation formats and the resource-intensive nature of preprocessing make it difficult to conduct comprehensive interdisciplinary research. Combining data for each individual case is a time-consuming process that requires not only time, but profound knowledge in the field of information technology. To solve the problem of sharing heterogeneous sources of clinical and biological species in brain research, an information system with unified access to heterogeneous data is required. Effective implementation of such a system requires creating a model for combining disparate data into a single information environment and adapting preprocessing methods applied individually to each individual data type. The introduction of a model that solves the fundamental problem of consolidating medical and biological data in the form of a cloud service will solve the problem of organizing researchers' access to consolidation results, and equalizing the geographical distribution of research groups and equipment. We analyze the possibilities and methods of consolidation of clinical and biological data, build a model for the consolidation and interaction of heterogeneous data sources for brain research, programmatically implement the model as a cloud service, and provide an interface for supporting queries in a format encapsulating a complex consolidation architecture from the user. We present the design and implementation of an information system for the collection, consolidation and analysis of patient data; we show and discuss the results of the application of cluster analysis methods for the automatic processing of voxel based magnetic resonance imaging data to facilitate the early diagnosis of Alzheimer's disease. Our results show that a detailed study of the properties of cluster analysis data can significantly help neurophysiologists in the study of Alzheimer's disease, especially with the help of automated data processing provided by the proposed information system.
</Content>
<field id="content">
Comprehensive human studies, in particular studies in the field of brain pathology, require strong information support for the consolidation of clinical and biological data from different sources in order to allow processing and analysis of data. The heterogeneity of data sources, the variety of presentation formats and the resource-intensive nature of preprocessing make it difficult to conduct comprehensive interdisciplinary research. Combining data for each individual case is a time-consuming process that requires not only time, but profound knowledge in the field of information technology. To solve the problem of sharing heterogeneous sources of clinical and biological species in brain research, an information system with unified access to heterogeneous data is required. Effective implementation of such a system requires creating a model for combining disparate data into a single information environment and adapting preprocessing methods applied individually to each individual data type. The introduction of a model that solves the fundamental problem of consolidating medical and biological data in the form of a cloud service will solve the problem of organizing researchers' access to consolidation results, and equalizing the geographical distribution of research groups and equipment. We analyze the possibilities and methods of consolidation of clinical and biological data, build a model for the consolidation and interaction of heterogeneous data sources for brain research, programmatically implement the model as a cloud service, and provide an interface for supporting queries in a format encapsulating a complex consolidation architecture from the user. We present the design and implementation of an information system for the collection, consolidation and analysis of patient data; we show and discuss the results of the application of cluster analysis methods for the automatic processing of voxel based magnetic resonance imaging data to facilitate the early diagnosis of Alzheimer's disease. Our results show that a detailed study of the properties of cluster analysis data can significantly help neurophysiologists in the study of Alzheimer's disease, especially with the help of automated data processing provided by the proposed information system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladislav</FirstName>
<FamilyName>Volosnikov</FamilyName>
<Email>volosnikov.apmath@gmail.com</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Vorontsov</FamilyName>
<Email>andreivorontsov96@yandex.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Natalia</FirstName>
<FamilyName>Zalutskaya</FamilyName>
<Email>nzalutskaya@yandex.ru</Email>
<Affiliation>V.M.Bekhterev National Research Medical Center for Psychiatry and Neurology</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Kirill</FirstName>
<FamilyName>Gribkov</FamilyName>
<Email>gribkov.kirill.v@gmail.com</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Neznanov</FamilyName>
<Email>sci.bekhterev@gmail.com</Email>
<Affiliation>V.M.Bekhterev National Research Medical Center for Psychiatry and Neurology</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Natalia</FirstName>
<FamilyName>Ananyeva</FamilyName>
<Email>ananieva_n@mail.ru</Email>
<Affiliation>V.M.Bekhterev National Research Medical Center for Psychiatry and Neurology</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>232</Id>
<Title>
THE USE OF SQUARE INTERPOLATION TO ACCELERATE THE CONVERGENCE OF THE CONTINUOUS ANALOGUE OF THE NEWTON METHOD
</Title>
<Content>
In this paper we present a method for solving nonlinear equations based on the control of the step size parameter in the continuous analogue of Newton's method on the domain of convergence and convergence rate. An approach proposed to optimize the convergence process of the continuous analog of the Newton method (NAMS) is based on the use of a quadratic interpolation polynomial. On the basis of this approach, a mechanism was developed to control the rate of convergence of continuous analog of Newton's method using a difference scheme for the numerical solution of the differential equation of NAMN as the controlling parameter of the step variation coefficient of the difference scheme.
</Content>
<field id="content">
In this paper we present a method for solving nonlinear equations based on the control of the step size parameter in the continuous analogue of Newton's method on the domain of convergence and convergence rate. An approach proposed to optimize the convergence process of the continuous analog of the Newton method (NAMS) is based on the use of a quadratic interpolation polynomial. On the basis of this approach, a mechanism was developed to control the rate of convergence of continuous analog of Newton's method using a difference scheme for the numerical solution of the differential equation of NAMN as the controlling parameter of the step variation coefficient of the difference scheme.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitrii</FirstName>
<FamilyName>Kazakov</FamilyName>
<Email>mitya_kazakov@inbox.ru</Email>
<Affiliation>student</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Eduard</FirstName>
<FamilyName>Nikonov</FamilyName>
<Email>e.nikonov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitrii</FirstName>
<FamilyName>Kazakov</FamilyName>
<Email>mitya_kazakov@inbox.ru</Email>
<Affiliation>student</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>233</Id>
<Title>
BigPanDA Experience on Titan for the ATLAS Experiment at the LHC
</Title>
<Content>
The PanDA software is used for workload management on distributed grid resources by the ATLAS experiment at the LHC. An effort was launched to extend PanDA, called BigPanDA, to access HPC resources, funded by the US Department of Energy (DOE-ASCR). Through this successful effort, ATLAS today uses over 25 million hours monthly on the Titan supercomputer at Oak Ridge National Laboratory. Many challenges were met and overcome in using HPCs for ATLAS simulations. ATLAS uses two different operational modes at Titan. The traditional mode uses allocations - which require software innovations to fit the low latency requirements of experimental science. New techniques were implemented to shape large jobs using allocations on a leadership class machine. In the second mode, high priority work is constantly sent to Titan to backfill high priority leadership class jobs. This has resulted in impressive gains in overall utilization of Titan, while benefiting the physics objectives of ATLAS. For both modes, BigPanDA has integrated traditional grid computing with HPC architecture. This talk will summarize the innovations to successfully use Titan for LHC physics goals
</Content>
<field id="content">
The PanDA software is used for workload management on distributed grid resources by the ATLAS experiment at the LHC. An effort was launched to extend PanDA, called BigPanDA, to access HPC resources, funded by the US Department of Energy (DOE-ASCR). Through this successful effort, ATLAS today uses over 25 million hours monthly on the Titan supercomputer at Oak Ridge National Laboratory. Many challenges were met and overcome in using HPCs for ATLAS simulations. ATLAS uses two different operational modes at Titan. The traditional mode uses allocations - which require software innovations to fit the low latency requirements of experimental science. New techniques were implemented to shape large jobs using allocations on a leadership class machine. In the second mode, high priority work is constantly sent to Titan to backfill high priority leadership class jobs. This has resulted in impressive gains in overall utilization of Titan, while benefiting the physics objectives of ATLAS. For both modes, BigPanDA has integrated traditional grid computing with HPC architecture. This talk will summarize the innovations to successfully use Titan for LHC physics goals
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Kaushik</FirstName>
<FamilyName>De</FamilyName>
<Email>kaushik@uta.edu</Email>
<Affiliation>University of Texas at Arlington</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Danila</FirstName>
<FamilyName>Oleynik</FamilyName>
<Email>danila@jinr.ru</Email>
<Affiliation>JINR LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ruslan</FirstName>
<FamilyName>Mashinistov</FamilyName>
<Email>rmashinistov@gmail.com</Email>
<Affiliation>NRC KI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jack</FirstName>
<FamilyName>Wells</FamilyName>
<Email>wellsjc@ornl.gov</Email>
<Affiliation>Oak Ridge National Laboratory</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>234</Id>
<Title>
Разработка перспективной системы сбора данных на основе TRB-3
</Title>
<Content>
Всвязи с увеличением объема информации получаемой в ходе эксперимента ALICE на Большом Адронном Коллайдере, повышаются требования к системам сбора данных с детекторов, например увеличение пропускной способности. Одним из возможных методов решения данной проблемы является использование TRB-3 платформы. Решение представляет собой глубокую модернизацию существующей модели сбора данных.
</Content>
<field id="content">
Всвязи с увеличением объема информации получаемой в ходе эксперимента ALICE на Большом Адронном Коллайдере, повышаются требования к системам сбора данных с детекторов, например увеличение пропускной способности. Одним из возможных методов решения данной проблемы является использование TRB-3 платформы. Решение представляет собой глубокую модернизацию существующей модели сбора данных.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kondratyev</FamilyName>
<Email>kondratyev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Kurepin</FamilyName>
<Email>alexander.kurepin@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kurepin</FamilyName>
<Email>nikolay.kurepin@cern.ch</Email>
<Affiliation>INR RAS</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Skazytkin</FamilyName>
<Email>konstantin.skazytkin@cern.ch</Email>
<Affiliation>INR RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kondratyev</FamilyName>
<Email>kondratyev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>235</Id>
<Title>
Enabling Biology, Chemistry and Other Sciences on Titan through BigPanDA
</Title>
<Content>
The Oak Ridge Leadership Computing Facility (OLCF) is one of the most powerful HPC centers available to researchers from different scientific fields to solve some of the world’s most challenging scientific problems. Small scientific groups often need to develop expertise to optimize their applications for running on Titan, and to fit the usage policies of such big machines. We have installed the BigPanDA workload management system at OLCF to simplify the submission of user tasks to Titan. In this talk we will present results of an RandD project to execute workloads from different scientific groups at OLCF. We will describe all steps: starting from deployment of PanDA server as service on demand at OLCF in OpenShift containers, to the adaptation of PanDA client tools for new users. Examples from some of the different scientific fields using this service will include biology/genomics, molecular dynamics, LQCD, solid-state and neutrino physics, and different data science experiments: nEDM, LSST, and IceQube. In more details we will address a “proof of concept” project with BlueBrain. It was conducted jointly by the BigPanDA team and the Blue Brain Project (BBP) of the Ecole Polytechnique Federal de Lausanne (EPFL). This proof of concept project showed the efficient application of the BigPanDA system to support the complex scientific workflow of the BBP using a mix of desktop, cluster and supercomputers to reconstruct and simulate accurate models of brain tissue.
</Content>
<field id="content">
The Oak Ridge Leadership Computing Facility (OLCF) is one of the most powerful HPC centers available to researchers from different scientific fields to solve some of the world’s most challenging scientific problems. Small scientific groups often need to develop expertise to optimize their applications for running on Titan, and to fit the usage policies of such big machines. We have installed the BigPanDA workload management system at OLCF to simplify the submission of user tasks to Titan. In this talk we will present results of an RandD project to execute workloads from different scientific groups at OLCF. We will describe all steps: starting from deployment of PanDA server as service on demand at OLCF in OpenShift containers, to the adaptation of PanDA client tools for new users. Examples from some of the different scientific fields using this service will include biology/genomics, molecular dynamics, LQCD, solid-state and neutrino physics, and different data science experiments: nEDM, LSST, and IceQube. In more details we will address a “proof of concept” project with BlueBrain. It was conducted jointly by the BigPanDA team and the Blue Brain Project (BBP) of the Ecole Polytechnique Federal de Lausanne (EPFL). This proof of concept project showed the efficient application of the BigPanDA system to support the complex scientific workflow of the BBP using a mix of desktop, cluster and supercomputers to reconstruct and simulate accurate models of brain tissue.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Kaushik</FirstName>
<FamilyName>De</FamilyName>
<Email>kaushik@uta.edu</Email>
<Affiliation>University of Texas at Arlington</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ruslan</FirstName>
<FamilyName>Mashinistov</FamilyName>
<Email>rmashinistov@gmail.com</Email>
<Affiliation>NRC KI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Danila</FirstName>
<FamilyName>Oleynik</FamilyName>
<Email>danila@jinr.ru</Email>
<Affiliation>JINR LIT</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Pavlo</FirstName>
<FamilyName>Svirin</FamilyName>
<Email>pavlo.svirin@cern.ch</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jack</FirstName>
<FamilyName>Wells</FamilyName>
<Email>wellsjc@ornl.gov</Email>
<Affiliation>Oak Ridge National Laboratory</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Panitkin</FamilyName>
<Email>spanitkin@bnl.gov</Email>
<Affiliation>BNL</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>236</Id>
<Title>
Multicomponent cluster management system for the computing center at IHEP
</Title>
<Content>
Cluster management system is a core part of any computing infrustructure. Such system includes components for allocating and controling over resources for different computing tasks, components for configuration management and software distribution on the computing hardware, components for monitoring and management software for the whole distributed infrustructure. The main goals of such system are to create autonomic computing system with functional areas such as self-configuration, self-healing, self-optimization and self-protection or to help to reduce the overall cost and complexity of IT management by simplifying the tasks of installing, configuring, operating, and maintaining clusters. In the presented work current implementation of the multicomponent cluster management system for the IHEP computing center will be shown. For the moment this system consists of event-driven management system, configuration management system, monitoring and accounting system and a chat-ops technology which is used for the administration tasks.
</Content>
<field id="content">
Cluster management system is a core part of any computing infrustructure. Such system includes components for allocating and controling over resources for different computing tasks, components for configuration management and software distribution on the computing hardware, components for monitoring and management software for the whole distributed infrustructure. The main goals of such system are to create autonomic computing system with functional areas such as self-configuration, self-healing, self-optimization and self-protection or to help to reduce the overall cost and complexity of IT management by simplifying the tasks of installing, configuring, operating, and maintaining clusters. In the presented work current implementation of the multicomponent cluster management system for the IHEP computing center will be shown. For the moment this system consists of event-driven management system, configuration management system, monitoring and accounting system and a chat-ops technology which is used for the administration tasks.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anna</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>anna.kotliar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victoria</FirstName>
<FamilyName>Ezhova</FamilyName>
<Email>victoria@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ekaterina</FirstName>
<FamilyName>Popova</FamilyName>
<Email>ekaterina.popova@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>237</Id>
<Title>
A way of anomaly detection in engineering equipment characteristics of Symmetra at IHEP IT center
</Title>
<Content>
The information flow should be monitored on anomaly detection. It is important, because it allows you to see a possible problem in advance and prevent it from turning into a real one. A huge flow of diverse data within the modern computing center flows from everywhere. As a rule, these are time series - numerical characteristics that are consistently measured after some time intervals. At this work there was developed the way of analysis for engineering equipment characteristics in centralized system of uninterrupted power supply (Symmetra) at IHEP IT center. When tracking time series, extracted from the data processing and storage system, anomalies are detected using the Twitter AnomalyDetection package. The information on problem is provided to the engineering and operational staff.
</Content>
<field id="content">
The information flow should be monitored on anomaly detection. It is important, because it allows you to see a possible problem in advance and prevent it from turning into a real one. A huge flow of diverse data within the modern computing center flows from everywhere. As a rule, these are time series - numerical characteristics that are consistently measured after some time intervals. At this work there was developed the way of analysis for engineering equipment characteristics in centralized system of uninterrupted power supply (Symmetra) at IHEP IT center. When tracking time series, extracted from the data processing and storage system, anomalies are detected using the Twitter AnomalyDetection package. The information on problem is provided to the engineering and operational staff.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ekaterina</FirstName>
<FamilyName>Popova</FamilyName>
<Email>popova@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>238</Id>
<Title>
Event-Driven Automation and chat-ops on IHEP computing cluster
</Title>
<Content>
Dealing with cluster-systems you have multiple ordinary situations which can be solved using automation tools. Stackstorm is a quite good event-driven system which helps to manage typical problems and to communicate with cluster via chat-ops extension. Just write a rule for such eventand it will be triggered and solved. In the presented work will be shown an example of a real event- driven system on IHEP computing cluster which use Nagios, CheckMK, Stackstorm, Mattermost for routine work automation as a part of multicomponent cluster managment system.
</Content>
<field id="content">
Dealing with cluster-systems you have multiple ordinary situations which can be solved using automation tools. Stackstorm is a quite good event-driven system which helps to manage typical problems and to communicate with cluster via chat-ops extension. Just write a rule for such eventand it will be triggered and solved. In the presented work will be shown an example of a real event- driven system on IHEP computing cluster which use Nagios, CheckMK, Stackstorm, Mattermost for routine work automation as a part of multicomponent cluster managment system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Anna</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>anna.kotliar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>239</Id>
<Title>
Efficiency measurement system for the computing cluster at IHEP
</Title>
<Content>
Every day IHEP central computing cluster produce thousands of calculations related to research activities, both IHEP and GRID experiments. A lot of machine resources are expended on this work. So, we can estimate the size of the spent resources used for all types of tasks, make decisions for changing cluster configuration and to do the forecast for the work of the computer center in general. In this work you can see the calculations of the efficiency index and the graphical representation of work of a cluster on the basis of account information. It is one of the main tasks within work on creation of system of uniform monitoring of computer center of IHEP.
</Content>
<field id="content">
Every day IHEP central computing cluster produce thousands of calculations related to research activities, both IHEP and GRID experiments. A lot of machine resources are expended on this work. So, we can estimate the size of the spent resources used for all types of tasks, make decisions for changing cluster configuration and to do the forecast for the work of the computer center in general. In this work you can see the calculations of the efficiency index and the graphical representation of work of a cluster on the basis of account information. It is one of the main tasks within work on creation of system of uniform monitoring of computer center of IHEP.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victoria</FirstName>
<FamilyName>Ezhova</FamilyName>
<Email>victoria@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>240</Id>
<Title>
Pseudo-random number generator based on neural network
</Title>
<Content>
Pseudorandom uniform distributed number generators are used in many fields of science and technology [1]. It is very important to test the quality of pseudo-random sequence produced by an algorithm. An overview of a large number of criteria for testing the quality of the sequence produced by pseudo-random generators can be found in the third chapter of [2], as well as in the article [3]. One of the most robust software packages that implements such tests is the DieHarder utility [4]. Among the algorithms that show good results when passing the entire set of DieHarder tests, the following three groups of algorithms should be mentioned. • The Mersenne Twister (MT — Mersenne Twister) [5] gives a very qualitative sequence, but is relatively complex. It is currently used as default for many modern programming languages. • Algorithms xorshift, xorshift+ and xorshift* [6] pass all sorts of tests from the DieHarder package on a level with Mersenne Twister, but algorithmically they are more simple than MT, although they have a slightly shorter period. • The family of KISS algorithms [5] (Keep It Simple Stupid), whose name indicates the extreme simplicity, are almost as good as Mersenne Twister and even simpler then xorshift. In this paper, we test our pseudo-random number generator based on the neural network, comparing it with the above algorithms. This comparison imposes strict requirements for the efficiency of the neural network, as each of these three algorithms has the potential for parallelization, requires for initialization from one to three initial seeds, uses only arithmetic and bitwise operations, and does not fail any DieHarder test, showing weak results in only 4 of them (of more than 100). References 1. M.N. Gevorkyan, M. Hnatich, I.M. Gostev, A.V. Demidova, A.V. Korolkova, D.S. Kulyabov, L.A. Sevastianov, The Stochastic Processes Generation in OpenModelica, in: V.M. Vishnevskiy, K.E. Samouylov, D.V. Kozyrev (Eds.), DCCN 2016, Moscow, Russia, November 21-25, 2016, Revised Selected Papers, Springer International Publishing, 2016: pp. 538–552. 2. Knuth Donald E. The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms. — Boston, MA, USA : Addison-Wesley Longman Publishing Co., Inc., 1997.—Vol. 2.—ISBN: 0-201-89684-2 3. L’Ecuyer Pierre, Simard Richard. TestU01: A C library for empirical testing of random number generators // ACM Transactions on Mathematical Software (TOMS). — 2007. — Vol. 33, no. 4. — P. 22 4. Brown Robert G., Eddelbuettel Dirk, Bauer David. Dieharder: A Random Number Test Suite. — 2013. — Access mode: http://www.phy.duke.edu/~rgb/General/rand_rate.php . 5. Rose Greg. KISS: A Bit Too Simple. — 2011. — Access mode: https://eprint.iacr.org/2011/007.pdf 6. Marsaglia George. Xorshift RNGs // Journal of Statistical Software. — 2003. — Vol. 8, no. 1. — P. 1–6.
</Content>
<field id="content">
Pseudorandom uniform distributed number generators are used in many fields of science and technology [1]. It is very important to test the quality of pseudo-random sequence produced by an algorithm. An overview of a large number of criteria for testing the quality of the sequence produced by pseudo-random generators can be found in the third chapter of [2], as well as in the article [3]. One of the most robust software packages that implements such tests is the DieHarder utility [4]. Among the algorithms that show good results when passing the entire set of DieHarder tests, the following three groups of algorithms should be mentioned. • The Mersenne Twister (MT — Mersenne Twister) [5] gives a very qualitative sequence, but is relatively complex. It is currently used as default for many modern programming languages. • Algorithms xorshift, xorshift+ and xorshift* [6] pass all sorts of tests from the DieHarder package on a level with Mersenne Twister, but algorithmically they are more simple than MT, although they have a slightly shorter period. • The family of KISS algorithms [5] (Keep It Simple Stupid), whose name indicates the extreme simplicity, are almost as good as Mersenne Twister and even simpler then xorshift. In this paper, we test our pseudo-random number generator based on the neural network, comparing it with the above algorithms. This comparison imposes strict requirements for the efficiency of the neural network, as each of these three algorithms has the potential for parallelization, requires for initialization from one to three initial seeds, uses only arithmetic and bitwise operations, and does not fail any DieHarder test, showing weak results in only 4 of them (of more than 100). References 1. M.N. Gevorkyan, M. Hnatich, I.M. Gostev, A.V. Demidova, A.V. Korolkova, D.S. Kulyabov, L.A. Sevastianov, The Stochastic Processes Generation in OpenModelica, in: V.M. Vishnevskiy, K.E. Samouylov, D.V. Kozyrev (Eds.), DCCN 2016, Moscow, Russia, November 21-25, 2016, Revised Selected Papers, Springer International Publishing, 2016: pp. 538–552. 2. Knuth Donald E. The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms. — Boston, MA, USA : Addison-Wesley Longman Publishing Co., Inc., 1997.—Vol. 2.—ISBN: 0-201-89684-2 3. L’Ecuyer Pierre, Simard Richard. TestU01: A C library for empirical testing of random number generators // ACM Transactions on Mathematical Software (TOMS). — 2007. — Vol. 33, no. 4. — P. 22 4. Brown Robert G., Eddelbuettel Dirk, Bauer David. Dieharder: A Random Number Test Suite. — 2013. — Access mode: http://www.phy.duke.edu/~rgb/General/rand_rate.php . 5. Rose Greg. KISS: A Bit Too Simple. — 2011. — Access mode: https://eprint.iacr.org/2011/007.pdf 6. Marsaglia George. Xorshift RNGs // Journal of Statistical Software. — 2003. — Vol. 8, no. 1. — P. 1–6.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Kulyabov</FamilyName>
<Email>yamadharma@gmail.com</Email>
<Affiliation>PFUR and JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anna</FirstName>
<FamilyName>Korolkova</FamilyName>
<Email>avkorolkova@gmail.com</Email>
<Affiliation>Peoples' Friendship University of Russia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Migran</FirstName>
<FamilyName>Gevorkyan</FamilyName>
<Email>mngevorkyan@sci.pfu.edu.ru</Email>
<Affiliation>PFU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anastasiya</FirstName>
<FamilyName>Demidova</FamilyName>
<Email>demidova_av@rudn.university</Email>
<Affiliation>RUDN University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Migran</FirstName>
<FamilyName>Gevorkyan</FamilyName>
<Email>mngevorkyan@sci.pfu.edu.ru</Email>
<Affiliation>PFU</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>241</Id>
<Title>
Новый вероятностно-статистический подход расчета информационных потерь в распределенных системах хранения и обработки данных физических экспериментов
</Title>
<Content>
В рамках работ по созданию компьютерной системы хранения и обработки данных установок B@MN и MPD, входящих в проект коллайдера NICA, возникает проблема выбора оптимальной конфигурации необходимого компьютерного и сетевого оборудования. Для решения этой проблемы требовалось разработать и исследовать модель перемещения данных внутри системы. Предыдущий опыт моделирования авторов настоящей статьи [1], показал, что описанные в литературе подходы моделирования процессов обработки потока заданий в распределенных и облачных системах [2,3], не подходят для анализа потоков данных, поскольку в библиотеках указанных моделирующих программ выполняется детализация потока данных до уровня пакета или файла, что приводит к сложной организации программ и большим вычислительным затратам. Поэтому нами предложен и реализован подход, рассматривающий процесс перемещения данных, как поток байтов, имеющий статистическую природу, без анализа отдельных частей этого потока. Для оценки различных конфигураций оборудования использовался вероятностно-статистический подход, при котором определяются вероятности потерь информации, поступающей с детекторов для каждой из этих конфигураций. В качестве причины потерь рассматривается переполнение буферов на одной из стадий накопления и передачи данных. Оптимальной конфигурацией считается та, что имеет минимальную стоимость при заданном допустимом уровне потерь. Для реализации этой схемы моделирования компьютерной системы хранения и обработки данных потребовалось, прежде всего, описать эту систему с помощью набора параметров, которые могут быть дефолтными или задаются пользователем. К параметрам относятся размеры дисковых буферов, количество потоков данных, пропускные способности каналов передачи и т.п. Для каждого параметра должны быть определены его граничные значения и шаг его изменения. На основе подготовленного набора параметров формируется поток независимых заданий расчёта потерь при передачах данных за время физического сеанса работы ускорителя. Таким образом, для осуществления процесса моделирования перемещения данных в системе и подсчета происходящих потерь потребовалось разработать два программных модуля. Первый записывает в базу данных сформированный набор параметров и автоматически строит и записывает в базу данных набор независимых заданий. Собственно расчёт выполняет второй модуль. Он выбирает из базы задание, строит конфигурацию оборудования, в соответствии с заданными параметрами, разыгрывает интенсивность потока данных на каждом шаге и рассчитывает процесс миграции данных в системе. Предложенная схема допускает параллельный расчёт вариантов, что позволяет анализировать значительное количество вариантов (десятки тысяч). Как перспектива развития этого подхода рассматривается создание классов, которые позволяют гибко менять топологию системы хранения данных. В существующем варианте она ориентирована только на анализ потерь при работе установок B@MN + MPD. Полученные к настоящему времени результаты моделирования позволили вести с проектировщиками систем DAQ и триггеров содержательные и аргументированные дискуссии по поводу параметров потоков данных, способствующие принятию мотивированных решений.
</Content>
<field id="content">
В рамках работ по созданию компьютерной системы хранения и обработки данных установок B@MN и MPD, входящих в проект коллайдера NICA, возникает проблема выбора оптимальной конфигурации необходимого компьютерного и сетевого оборудования. Для решения этой проблемы требовалось разработать и исследовать модель перемещения данных внутри системы. Предыдущий опыт моделирования авторов настоящей статьи [1], показал, что описанные в литературе подходы моделирования процессов обработки потока заданий в распределенных и облачных системах [2,3], не подходят для анализа потоков данных, поскольку в библиотеках указанных моделирующих программ выполняется детализация потока данных до уровня пакета или файла, что приводит к сложной организации программ и большим вычислительным затратам. Поэтому нами предложен и реализован подход, рассматривающий процесс перемещения данных, как поток байтов, имеющий статистическую природу, без анализа отдельных частей этого потока. Для оценки различных конфигураций оборудования использовался вероятностно-статистический подход, при котором определяются вероятности потерь информации, поступающей с детекторов для каждой из этих конфигураций. В качестве причины потерь рассматривается переполнение буферов на одной из стадий накопления и передачи данных. Оптимальной конфигурацией считается та, что имеет минимальную стоимость при заданном допустимом уровне потерь. Для реализации этой схемы моделирования компьютерной системы хранения и обработки данных потребовалось, прежде всего, описать эту систему с помощью набора параметров, которые могут быть дефолтными или задаются пользователем. К параметрам относятся размеры дисковых буферов, количество потоков данных, пропускные способности каналов передачи и т.п. Для каждого параметра должны быть определены его граничные значения и шаг его изменения. На основе подготовленного набора параметров формируется поток независимых заданий расчёта потерь при передачах данных за время физического сеанса работы ускорителя. Таким образом, для осуществления процесса моделирования перемещения данных в системе и подсчета происходящих потерь потребовалось разработать два программных модуля. Первый записывает в базу данных сформированный набор параметров и автоматически строит и записывает в базу данных набор независимых заданий. Собственно расчёт выполняет второй модуль. Он выбирает из базы задание, строит конфигурацию оборудования, в соответствии с заданными параметрами, разыгрывает интенсивность потока данных на каждом шаге и рассчитывает процесс миграции данных в системе. Предложенная схема допускает параллельный расчёт вариантов, что позволяет анализировать значительное количество вариантов (десятки тысяч). Как перспектива развития этого подхода рассматривается создание классов, которые позволяют гибко менять топологию системы хранения данных. В существующем варианте она ориентирована только на анализ потерь при работе установок B@MN + MPD. Полученные к настоящему времени результаты моделирования позволили вести с проектировщиками систем DAQ и триггеров содержательные и аргументированные дискуссии по поводу параметров потоков данных, способствующие принятию мотивированных решений.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Daria</FirstName>
<FamilyName>Priakhina</FamilyName>
<Email>pryahinad@jinr.ru</Email>
<Affiliation>ЛИТ</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>VLADIMIR</FirstName>
<FamilyName>TROFIMOV</FamilyName>
<Email>tvv@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Daria</FirstName>
<FamilyName>Priakhina</FamilyName>
<Email>pryahinad@jinr.ru</Email>
<Affiliation>ЛИТ</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>242</Id>
<Title>An Image Verification Framework Development</Title>
<Content>
An efficient representation and implementation of image are necessary, as a digital image is an approximation of some real situation, and carries some uncertainty. In order to deal with this uncertainty we need appropriate image model, which also enable image processing without losing the information regarding the uncertainty. Interval arithmetic techniques appear as a good option for handling the uncertainty. In this work we will discuss the extended of the classical notion of digital image, in the which each pixel has as degree of intensity an exact value to the interval digital image one, where each pixel possesses an interval intensity that include lower and upper bound of every element of the image. The time consuming process of image data processing can be address using parallel computing techniques that provide an efficient and convenient way to address this issue. The paper concludes that considering the interval arithmetic in designing solutions for some applications may impact the performance of algorithms and the image processing tasks may benefit from an efficient image verification model.
</Content>
<field id="content">
An efficient representation and implementation of image are necessary, as a digital image is an approximation of some real situation, and carries some uncertainty. In order to deal with this uncertainty we need appropriate image model, which also enable image processing without losing the information regarding the uncertainty. Interval arithmetic techniques appear as a good option for handling the uncertainty. In this work we will discuss the extended of the classical notion of digital image, in the which each pixel has as degree of intensity an exact value to the interval digital image one, where each pixel possesses an interval intensity that include lower and upper bound of every element of the image. The time consuming process of image data processing can be address using parallel computing techniques that provide an efficient and convenient way to address this issue. The paper concludes that considering the interval arithmetic in designing solutions for some applications may impact the performance of algorithms and the image processing tasks may benefit from an efficient image verification model.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ahmed</FirstName>
<FamilyName>Elaraby</FamilyName>
<Email>a.elaraby87@gmail.com</Email>
<Affiliation>Faculty of Science, South Valley University, Qena, Egypt.</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>12. Bioinformatics</Track>
</abstract>
<abstract>
<Id>243</Id>
<Title>THE BIGPANDA MONITORING SYSTEM ARCHITECTURE</Title>
<Content>
Currently-running large-scale scientific projects involve unprecedented amounts of data and computing power. For example, the ATLAS experiment at the Large Hadron Collider (LHC) has collected 140 PB of data over the course of Run 1 and this value increases at rate of ~800 MB/s during the ongoing Run 2 and recently has reached 350 PB. Processing and analysis of such amounts of data demands development of complex operational workflow and payload systems along with building top edge computing facilities. In the ATLAS experiment a key element of the workflow management is the Production and Distributed Analysis system (PanDA). It consists of several core components and one of them is the monitoring. The latter is responsible for providing a comprehensive and coherent view of the tasks and jobs executed by the system, from high level summaries to detailed drill-down job diagnostics. The BigPanDA monitoring has been in production since the middle of 2014 and it continuously evolves to satisfy increasing demands in functionality and growing payload scales. Today it effectively keeps track of more than 2 million jobs per day distributed over 170 computing centers worldwide in the largest instance of the BigPanDA monitoring: the ATLAS experiment. In this paper we describe the monitoring architecture and its principal features.
</Content>
<field id="content">
Currently-running large-scale scientific projects involve unprecedented amounts of data and computing power. For example, the ATLAS experiment at the Large Hadron Collider (LHC) has collected 140 PB of data over the course of Run 1 and this value increases at rate of ~800 MB/s during the ongoing Run 2 and recently has reached 350 PB. Processing and analysis of such amounts of data demands development of complex operational workflow and payload systems along with building top edge computing facilities. In the ATLAS experiment a key element of the workflow management is the Production and Distributed Analysis system (PanDA). It consists of several core components and one of them is the monitoring. The latter is responsible for providing a comprehensive and coherent view of the tasks and jobs executed by the system, from high level summaries to detailed drill-down job diagnostics. The BigPanDA monitoring has been in production since the middle of 2014 and it continuously evolves to satisfy increasing demands in functionality and growing payload scales. Today it effectively keeps track of more than 2 million jobs per day distributed over 170 computing centers worldwide in the largest instance of the BigPanDA monitoring: the ATLAS experiment. In this paper we describe the monitoring architecture and its principal features.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Tatiana</FirstName>
<FamilyName>Korchuganova</FamilyName>
<Email>tatianakorchuganova@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Siarhei</FirstName>
<FamilyName>Padolski</FamilyName>
<Email>siarhei.padolski@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Torre</FirstName>
<FamilyName>Wenaus</FamilyName>
<Email>wenaus@gmail.com</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Tatiana</FirstName>
<FamilyName>Korchuganova</FamilyName>
<Email>tatianakorchuganova@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>244</Id>
<Title>
Modernization of web service for the data center simulation program
</Title>
<Content>
The data storage and processing systems simulation program "SyMSim" was developed at the Laboratory of Information Technologies of the Joint Institute for Nuclear Research. The input parameters and the simulation results are stored in a database. A web service was developed to interact with the program, but it has some disadvantages. - there are no examples of program operation and guest mode; - user's personal cabinet isn’t finished; - there is no visual output of the results; - there is no form for creating a model; - design of the web service isn’t finished; The web service user interface modernization results are presented: - user manual, examples of using the program and guest mode are developed; - user's personal account is modified and has user-friendly interface; - the process of creating the simulated infrastructure and setting the equipment parameters is modified; - results output has been visualized.
</Content>
<field id="content">
The data storage and processing systems simulation program "SyMSim" was developed at the Laboratory of Information Technologies of the Joint Institute for Nuclear Research. The input parameters and the simulation results are stored in a database. A web service was developed to interact with the program, but it has some disadvantages. - there are no examples of program operation and guest mode; - user's personal cabinet isn’t finished; - there is no visual output of the results; - there is no form for creating a model; - design of the web service isn’t finished; The web service user interface modernization results are presented: - user manual, examples of using the program and guest mode are developed; - user's personal account is modified and has user-friendly interface; - the process of creating the simulated infrastructure and setting the equipment parameters is modified; - results output has been visualized.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Дмитрий</FirstName>
<FamilyName>Маров</FamilyName>
<Email>dmitriy.marov97@yandex.ru</Email>
<Affiliation>Университет "Дубна"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Дарья</FirstName>
<FamilyName>Пряхина</FamilyName>
<Email>pry-darya@yandex.ru</Email>
<Affiliation>ЛИТ, ОИЯИ</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Дмитрий</FirstName>
<FamilyName>Маров</FamilyName>
<Email>dmitriy.marov97@yandex.ru</Email>
<Affiliation>Университет "Дубна"</Affiliation>
</Speaker>
<Speaker>
<FirstName>Дарья</FirstName>
<FamilyName>Пряхина</FamilyName>
<Email>pry-darya@yandex.ru</Email>
<Affiliation>ЛИТ, ОИЯИ</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>245</Id>
<Title>
Cluster analysis of scientific payload to execute it efficiently in distributed computing environment
</Title>
<Content>
Every modern scientific experiment deals with the processing of large amounts of experimental data (up to exabytes) employing millions of computing processes and delivering corresponding scientific payloads. It raises the task of the efficient management of the batch processing of payloads, that should consider the well-defined grouping mechanism. Understanding naturally occurring groupings of payloads would increase the processing rate and improve the scheduling. The automated discovery of the payloads groups should consider not only the descriptive parameters of the payload itself but the characteristics of its interaction processes with computing resources and the computing environment. Our work is focused on applying machine learning methods to solve the stated problem, and particularly to evaluate and to use the approach of cluster analysis. Besides the ultimate goal, it will benefit the other related analytical services and processes aimed at analyzing particular payload parameters (e.g., prediction process) and/or set of parameters (e.g., correlations discovery).
</Content>
<field id="content">
Every modern scientific experiment deals with the processing of large amounts of experimental data (up to exabytes) employing millions of computing processes and delivering corresponding scientific payloads. It raises the task of the efficient management of the batch processing of payloads, that should consider the well-defined grouping mechanism. Understanding naturally occurring groupings of payloads would increase the processing rate and improve the scheduling. The automated discovery of the payloads groups should consider not only the descriptive parameters of the payload itself but the characteristics of its interaction processes with computing resources and the computing environment. Our work is focused on applying machine learning methods to solve the stated problem, and particularly to evaluate and to use the approach of cluster analysis. Besides the ultimate goal, it will benefit the other related analytical services and processes aimed at analyzing particular payload parameters (e.g., prediction process) and/or set of parameters (e.g., correlations discovery).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maksim</FirstName>
<FamilyName>Gubin</FamilyName>
<Email>gubin.m.u@gmail.com</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Maria</FirstName>
<FamilyName>Grigoryeva</FamilyName>
<Email>magsend@gmail.com</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Maksim</FirstName>
<FamilyName>Gubin</FamilyName>
<Email>gubin.m.u@gmail.com</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>246</Id>
<Title>
Реализация вычислений с динамическими зависимостями задач в среде десктоп грид с использованием Everest и Templet Web
</Title>
<Content>
Целью исследования была экспериментальная проверка технологии автоматизированной разработки приложений с динамически формируемым графом зависимостей между задачами для вычислений в грид-среде настольных компьютеров организации. Данный тип вычислений привлекателен с точки зрения минимума аппаратных затрат, но остается сложным как для программирования, так и для развертывания. Специфические требования, которые учитывались при разработке тестового приложения, включают: (1) использование простаивающих компьютеров; (2) исполнение на гетерогенном оборудовании; (3) простоту и оперативность развертывания компонентов приложения; (4) организацию длительных вычислений, устойчивых к отказу; (5) организацию вычислений с большим количеством задач и сложными зависимостями между ними. Нами было разработано приложение блочной сортировки большого массива экспериментальных данных в грид-среде настольных компьютеров, в котором были учтены заявленные требования. Протестировано использование ноутбуков, рабочих станций, виртуальных машин, имитирующих варианты доступных простаивающих вычислительных ресурсов организации. Компоненты приложения исполнялись под управлением операционной системы Linux (оркестратор вычислений), а также под управлением ОС Windows (сортировщики и мерджеры блоков данных). Простота развертывания обеспечивалась за счет использования платформ Everest [1], Templet Web [2] и программ-агентов с несложной процедурой установки. Отказоустойчивость вычислений задач обеспечивалась внутренними механизмами платформы Everest. Оркестратор в целях отказоустойчивости развертывался из сервиса Templet Web на виртуальную машину под управлением VMware. Код оркестратора сортировки написан на языке С++ c использованием специально разработанного акторо-подобного фреймворка, что позволило формировать задачи сортировки и объединения блоков динамически в зависимости от результатов исполнения ранее запущенных задач. В дальнейшем планируется расширить код фреймворка для прозрачного взаимодействия между платформой Everest и Templet Web, что позволит реализовывать код оркестратора непосредственно исследователем, без участия системного программиста. [1] Sukhoroslov O. A Web-Based Platform for Publication and Distributed Execution of Computing Applications [Text] / Sukhoroslov O., Volkov S., Afanasiev A. // IEEE Xplore. – 2015. – Vol. 14. – P. 175-184. [2] Vostokin S.V. Templet Web: the use of volunteer computing approach in PaaS-style cloud [Text] / S.V. Vostokin, Y.S. Artamonov, D.A. Tsaryov // Open Engineering. – 2018. – Vol. 8(1). – P. 50-56.
</Content>
<field id="content">
Целью исследования была экспериментальная проверка технологии автоматизированной разработки приложений с динамически формируемым графом зависимостей между задачами для вычислений в грид-среде настольных компьютеров организации. Данный тип вычислений привлекателен с точки зрения минимума аппаратных затрат, но остается сложным как для программирования, так и для развертывания. Специфические требования, которые учитывались при разработке тестового приложения, включают: (1) использование простаивающих компьютеров; (2) исполнение на гетерогенном оборудовании; (3) простоту и оперативность развертывания компонентов приложения; (4) организацию длительных вычислений, устойчивых к отказу; (5) организацию вычислений с большим количеством задач и сложными зависимостями между ними. Нами было разработано приложение блочной сортировки большого массива экспериментальных данных в грид-среде настольных компьютеров, в котором были учтены заявленные требования. Протестировано использование ноутбуков, рабочих станций, виртуальных машин, имитирующих варианты доступных простаивающих вычислительных ресурсов организации. Компоненты приложения исполнялись под управлением операционной системы Linux (оркестратор вычислений), а также под управлением ОС Windows (сортировщики и мерджеры блоков данных). Простота развертывания обеспечивалась за счет использования платформ Everest [1], Templet Web [2] и программ-агентов с несложной процедурой установки. Отказоустойчивость вычислений задач обеспечивалась внутренними механизмами платформы Everest. Оркестратор в целях отказоустойчивости развертывался из сервиса Templet Web на виртуальную машину под управлением VMware. Код оркестратора сортировки написан на языке С++ c использованием специально разработанного акторо-подобного фреймворка, что позволило формировать задачи сортировки и объединения блоков динамически в зависимости от результатов исполнения ранее запущенных задач. В дальнейшем планируется расширить код фреймворка для прозрачного взаимодействия между платформой Everest и Templet Web, что позволит реализовывать код оркестратора непосредственно исследователем, без участия системного программиста. [1] Sukhoroslov O. A Web-Based Platform for Publication and Distributed Execution of Computing Applications [Text] / Sukhoroslov O., Volkov S., Afanasiev A. // IEEE Xplore. – 2015. – Vol. 14. – P. 175-184. [2] Vostokin S.V. Templet Web: the use of volunteer computing approach in PaaS-style cloud [Text] / S.V. Vostokin, Y.S. Artamonov, D.A. Tsaryov // Open Engineering. – 2018. – Vol. 8(1). – P. 50-56.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Vostokin</FamilyName>
<Email>easts@mail.ru</Email>
<Affiliation>Samara National Research University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Sukhoroslov</FamilyName>
<Email>oleg.sukhoroslov@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Irina</FirstName>
<FamilyName>Bobyleva</FamilyName>
<Email>ikazakova90@gmail.com</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Stefan</FirstName>
<FamilyName>Popov</FamilyName>
<Email>stefanpopov@list.ru</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Vostokin</FamilyName>
<Email>easts@mail.ru</Email>
<Affiliation>Samara National Research University</Affiliation>
</Speaker>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Sukhoroslov</FamilyName>
<Email>oleg.sukhoroslov@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>247</Id>
<Title>
Modeling of task scheduling in desktop grid systems at the initial stage of development
</Title>
<Content>
The paper presents an overview of modern methods for task scheduling in desktop grid systems, estimates of the quality of methods, including: the time of execution of all tasks, the level of resource utilization. Heuristic approach to task scheduling is considered, which allows ensuring high performance and reliability of such systems at the early stages of development. A comparative analysis of the results of computational experiments performed with the help of the GridSim high performance computing simulation tool for various desktop grid system configurations is carried out.
</Content>
<field id="content">
The paper presents an overview of modern methods for task scheduling in desktop grid systems, estimates of the quality of methods, including: the time of execution of all tasks, the level of resource utilization. Heuristic approach to task scheduling is considered, which allows ensuring high performance and reliability of such systems at the early stages of development. A comparative analysis of the results of computational experiments performed with the help of the GridSim high performance computing simulation tool for various desktop grid system configurations is carried out.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Kurochkin</FamilyName>
<Email>qurochkin@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Kurochkin</FamilyName>
<Email>qurochkin@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>248</Id>
<Title>
Possible application areas of machine learning techniques at MPD/NICA experiment and their implementation prospects in distributed computing environment
</Title>
<Content>
At present, the accelerator complex NICA [1] is being built at JINR (Dubna). It is intended for performing experiments to study interactions of relativistic nuclei and polarized particles (protons and deuterons). One of the experimental facilities MPD (MultiPurpose Detector) [2] was designed to investigate nucleus-nucleus, proton-nucleus and proton-proton interactions. Preparation of the physics research program requires production of a large amount of simulated data, including high-multiplicity events of heavy-ion interactions with high energy. Realistic modelling of the detector response for such events can be significantly accelerated by making use of the generative models. Selection of rare physics processes traditionally utilizes machine learning based approaches. During the high luminosity accelerator operation for the proton-proton interaction research program it will be necessary to develop high-level trigger algorithms, based, among others, on machine learning methods. As the data taking proceeds, the tasks of the fast and efficient processing of experimental data and their storage in large volumes will become more and more important, requiring involvement of distributed computing resources. In this work these problems are considered in connection to the MPD/NICA experimental program preparation. [1] Nuclotron-based Ion Collider fAcility web-site: http://nica.jinr.ru [2] MultiPurpose Detector web-site: http://mpd.jinr.ru
</Content>
<field id="content">
At present, the accelerator complex NICA [1] is being built at JINR (Dubna). It is intended for performing experiments to study interactions of relativistic nuclei and polarized particles (protons and deuterons). One of the experimental facilities MPD (MultiPurpose Detector) [2] was designed to investigate nucleus-nucleus, proton-nucleus and proton-proton interactions. Preparation of the physics research program requires production of a large amount of simulated data, including high-multiplicity events of heavy-ion interactions with high energy. Realistic modelling of the detector response for such events can be significantly accelerated by making use of the generative models. Selection of rare physics processes traditionally utilizes machine learning based approaches. During the high luminosity accelerator operation for the proton-proton interaction research program it will be necessary to develop high-level trigger algorithms, based, among others, on machine learning methods. As the data taking proceeds, the tasks of the fast and efficient processing of experimental data and their storage in large volumes will become more and more important, requiring involvement of distributed computing resources. In this work these problems are considered in connection to the MPD/NICA experimental program preparation. [1] Nuclotron-based Ion Collider fAcility web-site: http://nica.jinr.ru [2] MultiPurpose Detector web-site: http://mpd.jinr.ru
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Zinchenko</FamilyName>
<Email>zinchenk1994@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Zinchenko</FamilyName>
<Email>alexander.zinchenko@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Eduard</FirstName>
<FamilyName>Nikonov</FamilyName>
<Email>e.nikonov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Zinchenko</FamilyName>
<Email>zinchenk1994@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>249</Id>
<Title>
Using binary file format description languages for verifying raw data in astroparticle physics experiments
</Title>
<Content>
The exponential growth of the amount of astroparticle data is expected to happen in near future. This trend gives rise to a number of emerging issues of big data management. One of the important issues is how to describe and verify raw binary data to support their availability and reuse in future. The present work demonstrates a possible solution for this issue in application to data of TAIGA observatory, which consists of a number of facilities featuring five different formats of raw data. The long-term preservation of raw binary data as originally generated is essential for rerunning analyses and reproducing research results in future. In this case, the raw data should be well documented and accompanied by the parsing and verifying tools. There are some declarative languages for describing binary file format description (e.g. DFDL, FLEXT, KAITAI STRUCT). The present work shows the progress of the application of KAITAI STRUCT to specify, parse and verify raw data of the TAIGA binary file formats. The format specifications implemented using this framework allow us to generate program code for parsing and verifying the raw binary data in the target languages (C++, Java, Python, etc.). The libraries were tested on real data, have shown good performance and indicated the parts with corrupted data. This study can be interested in other experiments which raw binary data formats remain weakly documented. This work was financially supported by the Russian Scientific Foundation (grant 18-41-06003).
</Content>
<field id="content">
The exponential growth of the amount of astroparticle data is expected to happen in near future. This trend gives rise to a number of emerging issues of big data management. One of the important issues is how to describe and verify raw binary data to support their availability and reuse in future. The present work demonstrates a possible solution for this issue in application to data of TAIGA observatory, which consists of a number of facilities featuring five different formats of raw data. The long-term preservation of raw binary data as originally generated is essential for rerunning analyses and reproducing research results in future. In this case, the raw data should be well documented and accompanied by the parsing and verifying tools. There are some declarative languages for describing binary file format description (e.g. DFDL, FLEXT, KAITAI STRUCT). The present work shows the progress of the application of KAITAI STRUCT to specify, parse and verify raw data of the TAIGA binary file formats. The format specifications implemented using this framework allow us to generate program code for parsing and verifying the raw binary data in the target languages (C++, Java, Python, etc.). The libraries were tested on real data, have shown good performance and indicated the parts with corrupted data. This study can be interested in other experiments which raw binary data formats remain weakly documented. This work was financially supported by the Russian Scientific Foundation (grant 18-41-06003).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Korosteleva</FamilyName>
<Email>elkrs@yandex.ru</Email>
<Affiliation>Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Kostunin</FamilyName>
<Email>dmitriy.kostunin@kit.edu</Email>
<Affiliation>Karlsruhe Institute of Technology (KIT)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>bychkov@icc.ru</Email>
<Affiliation>Matrosov Institute for System Dynamics and Control Theory SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Khmelnov</FamilyName>
<Email>hmelnov@icc.ru</Email>
<Affiliation>Matrosov Institute for System Dynamics and Control Theory SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Mikhailov</FamilyName>
<Email>mikhailov@icc.ru</Email>
<Affiliation>Matrosov Institute for System Dynamics and Control Theory SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Fedorov</FamilyName>
<Email>offedoroff@yandex.ru</Email>
<Affiliation>Applied Physics Institute Irkutsk State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Shigarov</FamilyName>
<Email>shigarov@gmail.com</Email>
<Affiliation>ISDCT SB RAS</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Korosteleva</FamilyName>
<Email>elkrs@yandex.ru</Email>
<Affiliation>Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>250</Id>
<Title>
Application of Hubzero platform for the educational process in astroparticle physics
</Title>
<Content>
In the frame of the Karlsruhe-Russian Astroparticle Data Life Cycle Initiative it was proposed to deploy an educational resource astroparticle.online for the training of students and graduate students in the field of astroparticle physics. This resource is based on HUBzero, which is an open-source software platform for building powerful websites, which supports scientific discovery, learning, and collaboration. HUBzero have been deployed on the servers of Matrosov Institute for System Dynamics and Control Theory. The educational resource astroparticle.online is being filled with the information covering cosmic messengers, astroparticle physics experi-ments and educational courses and schools on astroparticle physics. Furthermore, the educational resource astroparticle.online can be used for online collaboration. We present the current status of this project and our first experience of application of this service as a collaboration frame-work. This work was financially supported by Russian Science Foundation and Helmholtz Society, Grant No. 18-41-06003. The devel-oped educational resources were freely deployed on the cloud infrastructure of the Shared Equipment Center of Integrated Infor-mation and Computing Network for Irkutsk Research and Educational Complex (http://net.icc.ru).
</Content>
<field id="content">
In the frame of the Karlsruhe-Russian Astroparticle Data Life Cycle Initiative it was proposed to deploy an educational resource astroparticle.online for the training of students and graduate students in the field of astroparticle physics. This resource is based on HUBzero, which is an open-source software platform for building powerful websites, which supports scientific discovery, learning, and collaboration. HUBzero have been deployed on the servers of Matrosov Institute for System Dynamics and Control Theory. The educational resource astroparticle.online is being filled with the information covering cosmic messengers, astroparticle physics experi-ments and educational courses and schools on astroparticle physics. Furthermore, the educational resource astroparticle.online can be used for online collaboration. We present the current status of this project and our first experience of application of this service as a collaboration frame-work. This work was financially supported by Russian Science Foundation and Helmholtz Society, Grant No. 18-41-06003. The devel-oped educational resources were freely deployed on the cloud infrastructure of the Shared Equipment Center of Integrated Infor-mation and Computing Network for Irkutsk Research and Educational Complex (http://net.icc.ru).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yuliya</FirstName>
<FamilyName>Kazarina</FamilyName>
<Email>lutien777@mail.ru</Email>
<Affiliation>API ISU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Fedorov</FamilyName>
<Email>offedoroff@yandex.ru</Email>
<Affiliation>API ISU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Mikhailov</FamilyName>
<Email>mikhailov@icc.ru</Email>
<Affiliation>ISDCT SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>bychkov@icc.ru</Email>
<Affiliation>ISDCT SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andreas</FirstName>
<FamilyName>Haungs</FamilyName>
<Email>andreas.haungs@kit.edu</Email>
<Affiliation>Karlsruhe Institute of Technology (KIT)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Kostunin</FamilyName>
<Email>dmitriy.kostunin@kit.edu</Email>
<Affiliation>Karlsruhe Institute of Technology (KIT)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Shigarov</FamilyName>
<Email>shigarov@gmail.com</Email>
<Affiliation>ISDCT SB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Shipilov</FamilyName>
<Email>justforprince@gmail.com</Email>
<Affiliation>API ISU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Daria</FirstName>
<FamilyName>Chernykh</FamilyName>
<Email>chernykh.dash@gmail.com</Email>
<Affiliation>API ISU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yuliya</FirstName>
<FamilyName>Kazarina</FamilyName>
<Email>lutien777@mail.ru</Email>
<Affiliation>API ISU</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>5. Distributed computing in education</Track>
</abstract>
<abstract>
<Id>251</Id>
<Title>
Usage of the distributed computing system in the recovery of the spectral density of sea waves
</Title>
<Content>
This article presents a task of the recovery of the spectral density of sea waves in the linear case. Creation of the onboard ship system giving the current information about sea state and weather forecast in the navigation area is one of the most urgent problem. Weather forecast can be based on the analysis of the sea waves spectral density change. Evaluation of the sea wave spectral density is solved on the basis of indirect dynamic measurements of vibrational motion of the marine dynamic object in a seaway. The first researcher to raise the wave parameter identification problem on the basis of object behavior was Y. Nechayev [Nechaev Y. I., The collection of reports on the scientific and technical conference on experimental fluid mechanics (1990)], [Nechaev Y. I., Navigation and Hydrography 3 (1996)]. Over the past fifteen years, this problem has become rather popular and the works of Nielsen [Nielsen U. D., Stredulinsky D. C., Proceedings of the 12th International Ship StabilityWorkshop, pp.61-67 (2011)], Simons [Simons A. N., Tannuri E. A., Sparano J. V., Matos V. L. F., Applied Ocean Research v.32, i.2, pp.191-208 (2010)], Pascoal [Pascoal R., C. Guedes Soares., Ocean Engineering v.36, i.6-7, pp.477-488 (2009)] and others are of the most significance. Nevertheless, despite of researches large number it is still impossible to speak of an acceptable effective solution to this problem. The recovery of the sea waves on the basis of the behavior of the marine dynamic object requires the analysis and processing of large amounts of information. To improve the accuracy of identification requires using different algorithm of recovery and a large number of test calculations. The calculations should be made in real time. The system should also store processed data and provide access at any time. The software should have the fault-tolerance property, i.e. the software should continue to work in the case of failure of one of the parts. All these requirements and features make us to use distributed computing system for developing software of the solution of the problem.
</Content>
<field id="content">
This article presents a task of the recovery of the spectral density of sea waves in the linear case. Creation of the onboard ship system giving the current information about sea state and weather forecast in the navigation area is one of the most urgent problem. Weather forecast can be based on the analysis of the sea waves spectral density change. Evaluation of the sea wave spectral density is solved on the basis of indirect dynamic measurements of vibrational motion of the marine dynamic object in a seaway. The first researcher to raise the wave parameter identification problem on the basis of object behavior was Y. Nechayev [Nechaev Y. I., The collection of reports on the scientific and technical conference on experimental fluid mechanics (1990)], [Nechaev Y. I., Navigation and Hydrography 3 (1996)]. Over the past fifteen years, this problem has become rather popular and the works of Nielsen [Nielsen U. D., Stredulinsky D. C., Proceedings of the 12th International Ship StabilityWorkshop, pp.61-67 (2011)], Simons [Simons A. N., Tannuri E. A., Sparano J. V., Matos V. L. F., Applied Ocean Research v.32, i.2, pp.191-208 (2010)], Pascoal [Pascoal R., C. Guedes Soares., Ocean Engineering v.36, i.6-7, pp.477-488 (2009)] and others are of the most significance. Nevertheless, despite of researches large number it is still impossible to speak of an acceptable effective solution to this problem. The recovery of the sea waves on the basis of the behavior of the marine dynamic object requires the analysis and processing of large amounts of information. To improve the accuracy of identification requires using different algorithm of recovery and a large number of test calculations. The calculations should be made in real time. The system should also store processed data and provide access at any time. The software should have the fault-tolerance property, i.e. the software should continue to work in the case of failure of one of the parts. All these requirements and features make us to use distributed computing system for developing software of the solution of the problem.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Busko</FamilyName>
<Email>trassae95st@mail.ru</Email>
<Affiliation>SPSU Faculty of Applied Mathematics and Control Processes</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Busko</FamilyName>
<Email>trassae95st@mail.ru</Email>
<Affiliation>SPSU Faculty of Applied Mathematics and Control Processes</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>252</Id>
<Title>
DEVELOPMENT OF SOFTWARE FOR FACE RETRIEVAL SYSTEMS M ODELING
</Title>
<Content>
The development of software for face retrieval systems modeling is studied. An overview of the state of the problem is provided. Computer modeling is shown to be required to select the most appropriate system structure, set of modules and their parameters. The basic requirements for modern face retrieval systems are determined. It is found that they provided the concept of building a software complex for FaRetSys modeling, which formed the basis for a new Simulink library developed by the authors. Examples of solving practical problems of facial biometrics, structure, composition and parameters of blocks of used systems are shown. Compact models of computer experiments are presented.
</Content>
<field id="content">
The development of software for face retrieval systems modeling is studied. An overview of the state of the problem is provided. Computer modeling is shown to be required to select the most appropriate system structure, set of modules and their parameters. The basic requirements for modern face retrieval systems are determined. It is found that they provided the concept of building a software complex for FaRetSys modeling, which formed the basis for a new Simulink library developed by the authors. Examples of solving practical problems of facial biometrics, structure, composition and parameters of blocks of used systems are shown. Compact models of computer experiments are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Shchegoleva</FamilyName>
<Email>stil_hope@mail.ru</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Varvara</FirstName>
<FamilyName>Petrova</FamilyName>
<Email>petrova.varvara.0381@mail.ru</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI"</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Varvara</FirstName>
<FamilyName>Petrova</FamilyName>
<Email>petrova.varvara.0381@mail.ru</Email>
<Affiliation>Saint Petersburg Electrotechnical University "LETI"</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>12. Bioinformatics</Track>
</abstract>
<abstract>
<Id>253</Id>
<Title>Application of unified monitoring system in LHAASO</Title>
<Content>
LHAASO The on line machinecomputer room of LHAASO experiment located at has high altitude and poor natural environment. As t, and there is no permanent resident maintenance manpowerpersonnel, so it needs to deploy an automatic operation and maintenance system for the remote management. According to the characteristics of the LHAASO cluster management, we have designed a distributed monitoring framework, to support the site monitoring and management. In this framework, the monitoring data is collected in real-time at remote site, then the data is compressed and transferred the data back to IHEP. The servers at IHEP and used to analyze the data and display the running status via web page.. This monitoring system monitors can monitoring and displays the machine performance of both physical machine and virtual machine, cluster's service status, job status and equipment energy consumption information in real-time. The system; detects abnormal equipment and givesfor real-time alarm; It creates and destroys the virtual machines based on physical machine states to maintain adequate computing capacity; provide accurate cause of failure for temporary maintenance staff at LHAASO personnel.
</Content>
<field id="content">
LHAASO The on line machinecomputer room of LHAASO experiment located at has high altitude and poor natural environment. As t, and there is no permanent resident maintenance manpowerpersonnel, so it needs to deploy an automatic operation and maintenance system for the remote management. According to the characteristics of the LHAASO cluster management, we have designed a distributed monitoring framework, to support the site monitoring and management. In this framework, the monitoring data is collected in real-time at remote site, then the data is compressed and transferred the data back to IHEP. The servers at IHEP and used to analyze the data and display the running status via web page.. This monitoring system monitors can monitoring and displays the machine performance of both physical machine and virtual machine, cluster's service status, job status and equipment energy consumption information in real-time. The system; detects abnormal equipment and givesfor real-time alarm; It creates and destroys the virtual machines based on physical machine states to maintain adequate computing capacity; provide accurate cause of failure for temporary maintenance staff at LHAASO personnel.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Qingbao</FirstName>
<FamilyName>Hu</FamilyName>
<Email>huqb@ihep.ac.cn</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>wei</FirstName>
<FamilyName>zheng</FamilyName>
<Email>zhengw@ihep.ac.cn</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Qiulan</FirstName>
<FamilyName>Huang</FamilyName>
<Email>huangql@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy of Physics(IHEP), Chinese Academy of Science</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Haibo</FirstName>
<FamilyName>Li</FamilyName>
<Email>lihaibo@ihep.ac.cn</Email>
<Affiliation>Institute of High Energy Physics,Chinese Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>xiaowei</FirstName>
<FamilyName>jiang</FamilyName>
<Email>jiangxw@ihep.ac.cn</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Qingbao</FirstName>
<FamilyName>Hu</FamilyName>
<Email>huqb@ihep.ac.cn</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>254</Id>
<Title>
Ways to improve the productivity of fire simulation tools on modern equipment
</Title>
<Content>
One of the problems for all countries of the world are fires, in particular,fires in the premises. For the creation and effective use of firefighting means, it is necessary to calculate possible scenarios for the development of fires in specific conditions. At the present time, there are various tools for computer modeling of fires, but they have disadvantages - they have either a large error or a low performance. In this paper, mathematical models of fires and possible ways to improve fire modeling tools are considered, in particular, parallelization on GPU and distribution to multiple computers.
</Content>
<field id="content">
One of the problems for all countries of the world are fires, in particular,fires in the premises. For the creation and effective use of firefighting means, it is necessary to calculate possible scenarios for the development of fires in specific conditions. At the present time, there are various tools for computer modeling of fires, but they have disadvantages - they have either a large error or a low performance. In this paper, mathematical models of fires and possible ways to improve fire modeling tools are considered, in particular, parallelization on GPU and distribution to multiple computers.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Smirnov</FamilyName>
<Email>ariox41@gmail.com</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Smirnov</FamilyName>
<Email>ariox41@gmail.com</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>255</Id>
<Title>
Using multivariate quantile function for solving bioinformatics problems
</Title>
<Content>
In this work, we study evolutionary optimization algorithms for solving the problems in structural bioinformatics: prediction of three-dimensional peptide structure from amino acid sequence and peptide-protein docking. We provide a way of using evolutionary optimization algorithms based on using quantile functions. The used schemes for building and using of the quantile functions were described. The GPU-accelerated implementation of the presented schemes was carried out. We present the results of various numerical experiments.
</Content>
<field id="content">
In this work, we study evolutionary optimization algorithms for solving the problems in structural bioinformatics: prediction of three-dimensional peptide structure from amino acid sequence and peptide-protein docking. We provide a way of using evolutionary optimization algorithms based on using quantile functions. The used schemes for building and using of the quantile functions were described. The GPU-accelerated implementation of the presented schemes was carried out. We present the results of various numerical experiments.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Poluyan</FamilyName>
<Email>svpoluyan@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikolay</FirstName>
<FamilyName>Ershov</FamilyName>
<Email>ershovnm@gmail.com</Email>
<Affiliation>Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Poluyan</FamilyName>
<Email>svpoluyan@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>12. Bioinformatics</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>256</Id>
<Title>
Selection of rational composition of IT-services of information system with the purpose of increase of efficiency of transport logistics companies functioning
</Title>
<Content>
When the automation of transport logistics companies there is a problem with the gap between the existing business processes and means of automation. This circumstance makes it necessary to search for options for creating a software infrastructure (SI) for transport logistics enterprises. One of the solutions to this problem can be the transition of the enterprises to Service-Oriented Architecture (SOA). In essence, this means not considering the applied information system as a whole, but considering its individual functional components - IT services. The proposed SOA architecture is recommended to be formed from a set of business-oriented IT-services that collectively satisfy the tasks and business processes of the enterprise in transport logistics. When investigating the SI of the transport logistics system (TLS), it was established that the same function is often represented by different services as well as by different suppliers with different costs for providing access to services, speed of service provision, service availability, etc. Then the process of determining the necessary selection of services should be considered as a solution of the multicriteria problem of service composition for selected indicators. It is known that methods for solving multicriteria problems are divided into two groups. In this case, these groups are reduced to different strategies. The first strategy is based on the principle of the worst reaction of the external environment. The second strategy is based on the principle of equilibrium (the Nash principle). In addition, in most practical applications, the tasks of forming a rational composition of services have to be solved in conditions of significant uncertainty under the influence of the following factors: 1. The lack of a uniform and universally accepted methodology for the development and implementation of IT strategies at transport logistics enterprises. 2. Availability in the IT market of a big number of alternative IT solutions that implement similar functionality to automate the business processes of the enterprise. 3. The need to take into account the total costs associated with their acquisition and operation of IT services, etc. These factors form elements of uncertainty, which causes the need to use the mathematical apparatus of fuzzy sets. And then, the process of formation of a rational composition of IT services in the TLS infrastructure can be represented as a solution of multicriterial problem under fuzzy sets.
</Content>
<field id="content">
When the automation of transport logistics companies there is a problem with the gap between the existing business processes and means of automation. This circumstance makes it necessary to search for options for creating a software infrastructure (SI) for transport logistics enterprises. One of the solutions to this problem can be the transition of the enterprises to Service-Oriented Architecture (SOA). In essence, this means not considering the applied information system as a whole, but considering its individual functional components - IT services. The proposed SOA architecture is recommended to be formed from a set of business-oriented IT-services that collectively satisfy the tasks and business processes of the enterprise in transport logistics. When investigating the SI of the transport logistics system (TLS), it was established that the same function is often represented by different services as well as by different suppliers with different costs for providing access to services, speed of service provision, service availability, etc. Then the process of determining the necessary selection of services should be considered as a solution of the multicriteria problem of service composition for selected indicators. It is known that methods for solving multicriteria problems are divided into two groups. In this case, these groups are reduced to different strategies. The first strategy is based on the principle of the worst reaction of the external environment. The second strategy is based on the principle of equilibrium (the Nash principle). In addition, in most practical applications, the tasks of forming a rational composition of services have to be solved in conditions of significant uncertainty under the influence of the following factors: 1. The lack of a uniform and universally accepted methodology for the development and implementation of IT strategies at transport logistics enterprises. 2. Availability in the IT market of a big number of alternative IT solutions that implement similar functionality to automate the business processes of the enterprise. 3. The need to take into account the total costs associated with their acquisition and operation of IT services, etc. These factors form elements of uncertainty, which causes the need to use the mathematical apparatus of fuzzy sets. And then, the process of formation of a rational composition of IT services in the TLS infrastructure can be represented as a solution of multicriterial problem under fuzzy sets.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Gennady</FirstName>
<FamilyName>Dik</FamilyName>
<Email>g.dick@prodick.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Dik</FamilyName>
<Email>alexdicks@mail.ru</Email>
<Affiliation>St. Petersburg state Electrotechnical University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Gennady</FirstName>
<FamilyName>Dik</FamilyName>
<Email>g.dick@prodick.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>257</Id>
<Title>PIK Computing Centre</Title>
<Content>
In the framework of the PIK nuclear reactor reconstruction project, a new PIK Computing Centre was commissioned in 2017, the main task of which is storage and processing of PIK experiments data. The Centre's capacity is also used by other scientific groups at PNPI for solving problems in different areas of science such as computational biology and condensed matter physics. It also becomes an integral part of computing capacities of NRC "Kurchatov Institute". The PIK Computing Centre has a heterogeneous structure and consists of several types of computing nodes suitable for a wide range of tasks and two independent data storage systems, all of which are interconnected with a fast InfiniBand network. The engineering infrastructure provides redundant main power and two independent UPS installations for computing equipment and for cooling system.
</Content>
<field id="content">
In the framework of the PIK nuclear reactor reconstruction project, a new PIK Computing Centre was commissioned in 2017, the main task of which is storage and processing of PIK experiments data. The Centre's capacity is also used by other scientific groups at PNPI for solving problems in different areas of science such as computational biology and condensed matter physics. It also becomes an integral part of computing capacities of NRC "Kurchatov Institute". The PIK Computing Centre has a heterogeneous structure and consists of several types of computing nodes suitable for a wide range of tasks and two independent data storage systems, all of which are interconnected with a fast InfiniBand network. The engineering infrastructure provides redundant main power and two independent UPS installations for computing equipment and for cooling system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>258</Id>
<Title>
OpenFOAM wave modelling optimization with heterogeneous systems application porting.
</Title>
<Content>
In the report application of porting optimization on heteregeneous systems in the field of wave propagation modelling is discussed. Also, reviews of organization of computing in frame of OpenFOAM package and estimations of effectiveness of application porting on heteregeneous systems are given for the wave propagation problem in fluid. Evaluations of the difficulty and time required for implementations of these approaches in relation to performance improvements are considered.
</Content>
<field id="content">
In the report application of porting optimization on heteregeneous systems in the field of wave propagation modelling is discussed. Also, reviews of organization of computing in frame of OpenFOAM package and estimations of effectiveness of application porting on heteregeneous systems are given for the wave propagation problem in fluid. Evaluations of the difficulty and time required for implementations of these approaches in relation to performance improvements are considered.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Nizovtsov</FamilyName>
<Email>vostatin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Nizovtsov</FamilyName>
<Email>vostatin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>259</Id>
<Title>
Using extended reality technologies in distributed computer systems
</Title>
<Content>
Over the past few decades, web technologies have proven to be a fast, convenient and easy-to-access tool for retrieving information and sharing a large amount of heterogeneous data. The technologies used there, and in particular the HTML technology - have played a major role in the development of the Internet as it is. This is all due to the standardization and creation of a single tool to create network content. The goal of this work is to create a standard and a language for developing extended reality applications and interfaces built into existing applications. With this language, developers who are familiar with web technologies can quickly and with minimal effort move on to new technology and fill it with content, the lack of which is now the main problem of all extended reality technologies. With usage of the developed system it will be possible to combine the formed community of web developers and perspective technology on the basis of a standardized set of tools that will positively affect both developers and the pace of technology development. In the future that technology can be used in different spheres, like education, business, advertisement, etc. Also it can be used in global distributed computer systems, what will give an ability to make a global network of virtual objects referred to real-life points.
</Content>
<field id="content">
Over the past few decades, web technologies have proven to be a fast, convenient and easy-to-access tool for retrieving information and sharing a large amount of heterogeneous data. The technologies used there, and in particular the HTML technology - have played a major role in the development of the Internet as it is. This is all due to the standardization and creation of a single tool to create network content. The goal of this work is to create a standard and a language for developing extended reality applications and interfaces built into existing applications. With this language, developers who are familiar with web technologies can quickly and with minimal effort move on to new technology and fill it with content, the lack of which is now the main problem of all extended reality technologies. With usage of the developed system it will be possible to combine the formed community of web developers and perspective technology on the basis of a standardized set of tools that will positively affect both developers and the pace of technology development. In the future that technology can be used in different spheres, like education, business, advertisement, etc. Also it can be used in global distributed computer systems, what will give an ability to make a global network of virtual objects referred to real-life points.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Vozdvizhenskaya</FamilyName>
<Email>n.vozdvizhenskay@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nadezhda</FirstName>
<FamilyName>Vozdvizhenskaya</FamilyName>
<Email>n.vozdvizhenskay@gmail.com</Email>
<Affiliation>Dubna State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>260</Id>
<Title>
Current status of data center for cosmic rays based on KCDC
</Title>
<Content>
We present a current status of data center based on KCDC (KASCADE Cosmic Ray Data Centre), which was originally designed for providing an open access to the events measured and analyzed by KASCADE-Grande, a cosmic-ray experiment located in KIT, Karlsruhe. In the frame of the Russian-German Astroparticle Data Life Cycle Initiative we extend KCDC in order to provide an access to different cosmic-ray experiments and make possible aggregation and joint querying of heterogeneous air-shower data. In the present talk we discuss the description of data and metadata structures, implementation of data querying and merging, and first results on including data of experiments located in Tunka, Russia, in this common data center.
</Content>
<field id="content">
We present a current status of data center based on KCDC (KASCADE Cosmic Ray Data Centre), which was originally designed for providing an open access to the events measured and analyzed by KASCADE-Grande, a cosmic-ray experiment located in KIT, Karlsruhe. In the frame of the Russian-German Astroparticle Data Life Cycle Initiative we extend KCDC in order to provide an access to different cosmic-ray experiments and make possible aggregation and joint querying of heterogeneous air-shower data. In the present talk we discuss the description of data and metadata structures, implementation of data querying and merging, and first results on including data of experiments located in Tunka, Russia, in this common data center.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitriy</FirstName>
<FamilyName>Kostunin</FamilyName>
<Email>dmitriy.kostunin@kit.edu</Email>
<Affiliation>Karlsruhe Institute of Technology (KIT)</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Victoria</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Victoria</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
9. Consolidation and integration of distributed resources
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>261</Id>
<Title>
Исследование скорости сбора информации в беспроводных сенсорных сетях.
</Title>
<Content>
В настоящее время происходит автоматизация многих сфер человеческой деятельности. Важной составляющей частью автоматизации является использование сенсоров, измеряющих различные физические величины. Зачастую такие сенсоры объединяются в сети, использующие беспроводные каналы связи. Эти сети получили название беспроводных сенсорных сетей(БСС). БСС состоят из узлов двух типов - конечных устройств и стоков. Стоки накапливают информацию с других сенсоров, а затем передают ее на базовую станцию. Необходимость выделения стоков вызвана тем, что узлы БСС работают от батареек, а следовательно, имеют жесткие ограничения на энергопотребление. Необходимость экономить энергию оказывает влияние и на протоколы маршрутизации. Весьма известным протоколом, используемым в БСС, является PEGASIS, предложенный в [1]. Он основан на принципе цепочки (chain-based) - каждый узел имеет двух соседей, принимает сообщение от одного из них, передает другому. Функционирование БСС разбито на раунды. По цепочке информация стекается к узлу-стоку, который передает данные на базовую станцию. После этого раунд заканчивается, случайным образом выбирается новый сток, начинается следующий раунд. В [1] показано, что энергоэффективность протокола PEGASIS по сравнению с прямой передачей от каждого узла на базовую станцию и кластерной организацией сети. В данной работе проведено исследование продолжительности раунда протокола PEGASIS в случае наличия у узлов сети фазы сна (используется для повышения оптимизации расхода энергии). Распределение продолжительности фаз сна и активности подчинено условиям модели Чиассерини-Гаретто [2]. Исследования проводились путем имитационного моделирования протокола PEGASIS в цепочке статичных сенсоров. Исследования проводились на суперкомпьютере BlueGene\P суперкомпьютерного комплекса МГУ им. М.В.Ломоносова. Одновременный розыгрыш симуляций модели с различными параметрами на узлах BlueGene позволил значительно уменьшить время сбора необходимый статистических данных. Литература 1. Chiasserini C.F., Garetto M. Modeling the Performance of Wireless Sensor Networks. In: Proceedings of INFOCOM 2004, pp. 220-231 2 Lindsey, S., Raghavendra, C.S.: PEGASIS: Power Efficient Gathering in Sensor Information Systems. In: Proceedings of IEEE ICC 2001, pp. 1125–1130
</Content>
<field id="content">
В настоящее время происходит автоматизация многих сфер человеческой деятельности. Важной составляющей частью автоматизации является использование сенсоров, измеряющих различные физические величины. Зачастую такие сенсоры объединяются в сети, использующие беспроводные каналы связи. Эти сети получили название беспроводных сенсорных сетей(БСС). БСС состоят из узлов двух типов - конечных устройств и стоков. Стоки накапливают информацию с других сенсоров, а затем передают ее на базовую станцию. Необходимость выделения стоков вызвана тем, что узлы БСС работают от батареек, а следовательно, имеют жесткие ограничения на энергопотребление. Необходимость экономить энергию оказывает влияние и на протоколы маршрутизации. Весьма известным протоколом, используемым в БСС, является PEGASIS, предложенный в [1]. Он основан на принципе цепочки (chain-based) - каждый узел имеет двух соседей, принимает сообщение от одного из них, передает другому. Функционирование БСС разбито на раунды. По цепочке информация стекается к узлу-стоку, который передает данные на базовую станцию. После этого раунд заканчивается, случайным образом выбирается новый сток, начинается следующий раунд. В [1] показано, что энергоэффективность протокола PEGASIS по сравнению с прямой передачей от каждого узла на базовую станцию и кластерной организацией сети. В данной работе проведено исследование продолжительности раунда протокола PEGASIS в случае наличия у узлов сети фазы сна (используется для повышения оптимизации расхода энергии). Распределение продолжительности фаз сна и активности подчинено условиям модели Чиассерини-Гаретто [2]. Исследования проводились путем имитационного моделирования протокола PEGASIS в цепочке статичных сенсоров. Исследования проводились на суперкомпьютере BlueGene\P суперкомпьютерного комплекса МГУ им. М.В.Ломоносова. Одновременный розыгрыш симуляций модели с различными параметрами на узлах BlueGene позволил значительно уменьшить время сбора необходимый статистических данных. Литература 1. Chiasserini C.F., Garetto M. Modeling the Performance of Wireless Sensor Networks. In: Proceedings of INFOCOM 2004, pp. 220-231 2 Lindsey, S., Raghavendra, C.S.: PEGASIS: Power Efficient Gathering in Sensor Information Systems. In: Proceedings of IEEE ICC 2001, pp. 1125–1130
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Илья</FirstName>
<FamilyName>Никольский</FamilyName>
<Email>haifly@rambler.ru</Email>
<Affiliation>МГУ им М.В. Ломоносова</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Илья</FirstName>
<FamilyName>Никольский</FamilyName>
<Email>haifly@rambler.ru</Email>
<Affiliation>МГУ им М.В. Ломоносова</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>262</Id>
<Title>
Accelerating real-time ship motion simulations using general purpose GPU computations
</Title>
<Content>
Software suites for ship simulations are typically used for statistical studies of ship dynamics, but also as a simulator for training ship crew in dangerous situations. One problem that arises during training is speeding-up a part of the session which does not involve actions from the crew. The aim of the study reported here is to accelerate solution of ship motions equations using general purpose computations on GPU. These equations describe dynamics of ship manoeuvring in wavy sea surface, and are central to the simulator programme. The equations are solved numerically via Runge—Kutta—Fehlberg method. Due to high number of floating point operations, computation on GPU achieves considerable speed-up over CPU. High performance solution allows to shorten training sessions and make them more efficient, but also beneficial for statistical studies as it reduces simulation time.
</Content>
<field id="content">
Software suites for ship simulations are typically used for statistical studies of ship dynamics, but also as a simulator for training ship crew in dangerous situations. One problem that arises during training is speeding-up a part of the session which does not involve actions from the crew. The aim of the study reported here is to accelerate solution of ship motions equations using general purpose computations on GPU. These equations describe dynamics of ship manoeuvring in wavy sea surface, and are central to the simulator programme. The equations are solved numerically via Runge—Kutta—Fehlberg method. Due to high number of floating point operations, computation on GPU achieves considerable speed-up over CPU. High performance solution allows to shorten training sessions and make them more efficient, but also beneficial for statistical studies as it reduces simulation time.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Petriakov</FamilyName>
<Email>st049350@student.spbu.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Gankevich</FamilyName>
<Email>i.gankevich@spbu.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Petriakov</FamilyName>
<Email>st049350@student.spbu.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>263</Id>
<Title>
Real-time visualization of ship and wavy surface motions based on GPGPU computations
</Title>
<Content>
One of the key stages in ship design process is the modeling of its behavior on the wavy sea surface, carried out with the expected operational characteristics taken into account. Similar modeling process could be done within real conditions at virtual testbed, which allows to monitor the influence of external disturbances on ship's running characteristics in real time with sensors installed onboard. Visualization of the results for such modeling process allows the researcher to correctly and holistically perceive occurring events, as well as to predict and timely respond to emerging dangerous situations. If we are using GPGPU technology for computation purposes, results of modeling will be already placed in GPU memory after process completion. This fact can be regarded as an opportunity to optimize the visualization process by converting the raw simulation data into graphic objects directly on the GPU, and interaction mechanisms between OpenGL and OpenCL could be used here. In this article we demonstrate the effectiveness of this technique on the example of ship behaviour visualization on a wavy sea surface, as well as forces acting on the ship's hull.
</Content>
<field id="content">
One of the key stages in ship design process is the modeling of its behavior on the wavy sea surface, carried out with the expected operational characteristics taken into account. Similar modeling process could be done within real conditions at virtual testbed, which allows to monitor the influence of external disturbances on ship's running characteristics in real time with sensors installed onboard. Visualization of the results for such modeling process allows the researcher to correctly and holistically perceive occurring events, as well as to predict and timely respond to emerging dangerous situations. If we are using GPGPU technology for computation purposes, results of modeling will be already placed in GPU memory after process completion. This fact can be regarded as an opportunity to optimize the visualization process by converting the raw simulation data into graphic objects directly on the GPU, and interaction mechanisms between OpenGL and OpenCL could be used here. In this article we demonstrate the effectiveness of this technique on the example of ship behaviour visualization on a wavy sea surface, as well as forces acting on the ship's hull.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Anton</FirstName>
<FamilyName>Gavrikov</FamilyName>
<Email>gavrikovantonkapi@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrei</FirstName>
<FamilyName>Ivashchenko</FamilyName>
<Email>aiivashchenko@cc.spbu.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Gankevich</FamilyName>
<Email>i.gankevich@spbu.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nataliia</FirstName>
<FamilyName>Kulabukhova</FamilyName>
<Email>kulabukhova.nv@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Rukovchuk</FamilyName>
<Email>v.rukovchuk@mail.ru</Email>
<Affiliation>Computer Science Alliance</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Anton</FirstName>
<FamilyName>Gavrikov</FamilyName>
<Email>gavrikovantonkapi@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>264</Id>
<Title>
Creating tools to assist in development of CMS software
</Title>
<Content>
Software packages, created for the modern physics experiments, present a sets intertwined code structures, written by different people and bundled together to perform range of different reconstruction and analysis tasks on the experimental data. Sometimes due to complicated nature of such frameworks a new set of tools is required to simplify their further development. In this work we investigate an example of such tool, created for the CMS experiment to analyse the structure of it's software components.
</Content>
<field id="content">
Software packages, created for the modern physics experiments, present a sets intertwined code structures, written by different people and bundled together to perform range of different reconstruction and analysis tasks on the experimental data. Sometimes due to complicated nature of such frameworks a new set of tools is required to simplify their further development. In this work we investigate an example of such tool, created for the CMS experiment to analyse the structure of it's software components.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>George</FirstName>
<FamilyName>Adamov</FamilyName>
<Email>adamov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>George</FirstName>
<FamilyName>Adamov</FamilyName>
<Email>adamov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>265</Id>
<Title>
A distributed data warehouse system for astroparticle physics
</Title>
<Content>
A distributed data warehouse system is one of the actual issues in the field of astroparticle physics. Famous experiments, such as Tunka, Taiga, produce tens of terabytes of data measured by their instruments. It is critical to have a smart data warehouse system on-site to store the collected data for further distribution effectively. It is also vital to provide scientists with a handy and user-friendly interface to access the collected data with proper permissions not only on-site but also online. The latter case is handy when scientists need to combine data from different experiments for analysis. In this work, we describe an approach to implementing a distributed data warehouse system that allows scientists to acquire just the necessary data from different experiments via the Internet on demand. The implementation is based on the CERN CVMFS with additional components developed by us to search through the whole available data sets and deliver their subsets to users' computers.
</Content>
<field id="content">
A distributed data warehouse system is one of the actual issues in the field of astroparticle physics. Famous experiments, such as Tunka, Taiga, produce tens of terabytes of data measured by their instruments. It is critical to have a smart data warehouse system on-site to store the collected data for further distribution effectively. It is also vital to provide scientists with a handy and user-friendly interface to access the collected data with proper permissions not only on-site but also online. The latter case is handy when scientists need to combine data from different experiments for analysis. In this work, we describe an approach to implementing a distributed data warehouse system that allows scientists to acquire just the necessary data from different experiments via the Internet on demand. The implementation is based on the CERN CVMFS with additional components developed by us to search through the whole available data sets and deliver their subsets to users' computers.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Minh Duc</FirstName>
<FamilyName>Nguyen</FamilyName>
<Email>nguyendmitri@gmail.com</Email>
<Affiliation>Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Minh Duc</FirstName>
<FamilyName>Nguyen</FamilyName>
<Email>nguyendmitri@gmail.com</Email>
<Affiliation>Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>266</Id>
<Title>
Optimization of Neural networks training with Vector-Free heuristic on Apache Spark
</Title>
<Content>
One of the most computation complicated tasks during Neural networks development is a training process. It could be considered as high dimensional numerical optimization tasks. In the modern MapReduce systems, like Apache Spark, it's hard to efficiently implement traditional for Neural networks training gradient-based algorithms or Quasi-newton L-BFGS method, because there are too many non-linear or memory bound operations, could dramatically decrease the perfomance of your cluster and scalability of your training task. It's known that for L-BFGS methods there are Vector-Free heuristics which allows to reduce task complexity in terms of an amount of Map and Reduce operations, in the application for large-scale logistic regression tasks. Also it's unclear in which types of neural networks these approaches are applicable. In this research, we applied the heuristics which reduces the amount of the memory-bound and nonlinear operations, Vector-Free heuristic, to the modern numerical optimization algorithms, like L-BFGS, Adam, AdaGrad on Spark cluster. We tested modified versions of algorithms on the different types of NNs architectures: MLP, VGG-16, LSTM, which covers popular neural network types and particular tasks for them. Also to provide efficient and usable environment for computational experiment we developed a software system which could in a semi-automatic way perform a testing of these methods. It allows a researcher to measure the effect of Vector-Free or other heuristics on different platforms and neural networks architectures. Also, this system supports comparing with external data, so researcher is able to compare effectiveness and speedup with other system types like GPU's versions of the methods above. In this research, we only applied this type of heuristic to this algorithms without taking any consideration which thing results in bad perfomance of modified methods, and don't provide any empirical boundaries for the errors or convergence. All experiments have been performed on the Microsoft Azure cloud platform, with 16 HD12v2 nodes.
</Content>
<field id="content">
One of the most computation complicated tasks during Neural networks development is a training process. It could be considered as high dimensional numerical optimization tasks. In the modern MapReduce systems, like Apache Spark, it's hard to efficiently implement traditional for Neural networks training gradient-based algorithms or Quasi-newton L-BFGS method, because there are too many non-linear or memory bound operations, could dramatically decrease the perfomance of your cluster and scalability of your training task. It's known that for L-BFGS methods there are Vector-Free heuristics which allows to reduce task complexity in terms of an amount of Map and Reduce operations, in the application for large-scale logistic regression tasks. Also it's unclear in which types of neural networks these approaches are applicable. In this research, we applied the heuristics which reduces the amount of the memory-bound and nonlinear operations, Vector-Free heuristic, to the modern numerical optimization algorithms, like L-BFGS, Adam, AdaGrad on Spark cluster. We tested modified versions of algorithms on the different types of NNs architectures: MLP, VGG-16, LSTM, which covers popular neural network types and particular tasks for them. Also to provide efficient and usable environment for computational experiment we developed a software system which could in a semi-automatic way perform a testing of these methods. It allows a researcher to measure the effect of Vector-Free or other heuristics on different platforms and neural networks architectures. Also, this system supports comparing with external data, so researcher is able to compare effectiveness and speedup with other system types like GPU's versions of the methods above. In this research, we only applied this type of heuristic to this algorithms without taking any consideration which thing results in bad perfomance of modified methods, and don't provide any empirical boundaries for the errors or convergence. All experiments have been performed on the Microsoft Azure cloud platform, with 16 HD12v2 nodes.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Kamil</FirstName>
<FamilyName>Khamitov</FamilyName>
<Email>berserq0123@gmail.com</Email>
<Affiliation>Lomonosov Moscow State Univercity</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nina</FirstName>
<FamilyName>Popova</FamilyName>
<Email>popova@cs.msu.su</Email>
<Affiliation>Lomonosov Moscow State Univercity</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Kamil</FirstName>
<FamilyName>Khamitov</FamilyName>
<Email>berserq0123@gmail.com</Email>
<Affiliation>Lomonosov Moscow State Univercity</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>267</Id>
<Title>
Particle identification in ground-based gamma-ray astronomy using convolutional neural networks
</Title>
<Content>
Modern detectors of cosmic gamma rays are a special type of imaging telescopes (Cherenkov telescopes) supplied with cameras with relatively large number of photomultiplier-based pixels. For example, the camera of the TAIGA telescope has 560 pixels of hexagonal structure. Images in such cameras can be analyzed by various deep learning techniques to extract numerous physical and geometrical parameters and/or for incoming particle identification. We implement for this purpose the most powerful deep learning technique for image analysis, the so cold convolutional neural networks (CNN). In this work we present the results of tests with two open source machine learning libraries, PyTorch and TensorFlow, as possible platforms for particle identification in imaging Cherenkov telescopes. Monte Carlo simulation was performed to analyze images of gamma rays and other (background) particles as well as estimate identification accuracy. Further steps of implementation and improvement of this technique are discussed.
</Content>
<field id="content">
Modern detectors of cosmic gamma rays are a special type of imaging telescopes (Cherenkov telescopes) supplied with cameras with relatively large number of photomultiplier-based pixels. For example, the camera of the TAIGA telescope has 560 pixels of hexagonal structure. Images in such cameras can be analyzed by various deep learning techniques to extract numerous physical and geometrical parameters and/or for incoming particle identification. We implement for this purpose the most powerful deep learning technique for image analysis, the so cold convolutional neural networks (CNN). In this work we present the results of tests with two open source machine learning libraries, PyTorch and TensorFlow, as possible platforms for particle identification in imaging Cherenkov telescopes. Monte Carlo simulation was performed to analyze images of gamma rays and other (background) particles as well as estimate identification accuracy. Further steps of implementation and improvement of this technique are discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgeny</FirstName>
<FamilyName>Postnikov</FamilyName>
<Email>evgeny.post@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Stanislav</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>s.p.polyakov@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Shipilov</FamilyName>
<Email>justforprince@gmail.com</Email>
<Affiliation>ISU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Zhurov</FamilyName>
<Email>sidney28@ya.ru</Email>
<Affiliation>ISU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Evgeny</FirstName>
<FamilyName>Postnikov</FamilyName>
<Email>evgeny.post@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>268</Id>
<Title>Data Preprocessing for Credit Scoring</Title>
<Content>
Data mining suggests a variety of powerful methods for extracting useful information from data sets. Data scientists are trying to compare existing methods and evaluate them according to their accuracy. Although, the performance and quality of extracted knowledge depends, first of all, on quality and suitability of data used. It is well-known that low quality of data leads to low quality of extracted knowledge. The major goal of data preprocessing stage is preparation of final data set with reduced noise, filled-in missing values, selected features, etc. This paper focuses specifically on data preprocessing for credit scoring.
</Content>
<field id="content">
Data mining suggests a variety of powerful methods for extracting useful information from data sets. Data scientists are trying to compare existing methods and evaluate them according to their accuracy. Although, the performance and quality of extracted knowledge depends, first of all, on quality and suitability of data used. It is well-known that low quality of data leads to low quality of extracted knowledge. The major goal of data preprocessing stage is preparation of final data set with reduced noise, filled-in missing values, selected features, etc. This paper focuses specifically on data preprocessing for credit scoring.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Irina</FirstName>
<FamilyName>Demchenko</FamilyName>
<Email>kazakyavichyusis@yandex.ru</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Irina</FirstName>
<FamilyName>Demchenko</FamilyName>
<Email>kazakyavichyusis@yandex.ru</Email>
<Affiliation>Tomsk Polytechnic University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>269</Id>
<Title>
Design and implementation of a service for performing HPC computations in cloud environment.
</Title>
<Content>
Cloud computing became a routine tool for scientists in many domains. In order to speed up an achievement of scientific results a cloud service for execution of distributed applications was developed. It obviates users from manually creating and configuring virtual cluster environment or using batch scheduler and allows them only to specify input parameters to perform their computations. One of the key parameters that this service aims to help users with is virtual cluster configuration. For most applications it is difficult to tell the optimal number of cluster nodes, amounts of their threads per node and memory so that application would have a minimal execution time. In this work an approach to optimization of cluster configuration has been proposed and software system for launching HPC application in a cloud has been presented.
</Content>
<field id="content">
Cloud computing became a routine tool for scientists in many domains. In order to speed up an achievement of scientific results a cloud service for execution of distributed applications was developed. It obviates users from manually creating and configuring virtual cluster environment or using batch scheduler and allows them only to specify input parameters to perform their computations. One of the key parameters that this service aims to help users with is virtual cluster configuration. For most applications it is difficult to tell the optimal number of cluster nodes, amounts of their threads per node and memory so that application would have a minimal execution time. In this work an approach to optimization of cluster configuration has been proposed and software system for launching HPC application in a cloud has been presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ruslan</FirstName>
<FamilyName>Kuchumov</FamilyName>
<Email>kuchumovri@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korkhov</FamilyName>
<Email>vladimir@csa.ru</Email>
<Affiliation>St. Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ruslan</FirstName>
<FamilyName>Kuchumov</FamilyName>
<Email>kuchumovri@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>270</Id>
<Title>
Experiments with JupyterHub at the Saint Petersburg State University
</Title>
<Content>
The talk focuses on our experience with JupyterHub and JupyterLab, the ways we extend Jupyter and how we abuse JupyterHub to spawn something other than signle-user Jupyter notebook servers. Our project has started as a copy of CERN SWAN environment, but it evolves independently. However, we are still using CVMFS to load Jupyter kernels and other software and EOS to store user home directories.
</Content>
<field id="content">
The talk focuses on our experience with JupyterHub and JupyterLab, the ways we extend Jupyter and how we abuse JupyterHub to spawn something other than signle-user Jupyter notebook servers. Our project has started as a copy of CERN SWAN environment, but it evolves independently. However, we are still using CVMFS to load Jupyter kernels and other software and EOS to store user home directories.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Erokhin</FirstName>
<FamilyName>Andrey</FamilyName>
<Email>andrey.erokhin@cern.ch</Email>
<Affiliation>SPbSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Erokhin</FirstName>
<FamilyName>Andrey</FamilyName>
<Email>andrey.erokhin@cern.ch</Email>
<Affiliation>SPbSU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>271</Id>
<Title>
Participation of Russian institutions in the processing and storage of ALICE data.
</Title>
<Content>
В докладе представлены результаты работы российских институтов в обработке данных эксперимента ALICE в течеие 2-х последних лет во время 2-го этапа работы Большого адронного коллайдера (БАК). Рассмотрены основные проблемы и задачи, стоящие как перед ALICE Grid Computing, так и перед его российским сегментом перед третьим этапом работы БАК. Также представлены планы подготовки к работе БАК в режиме HL (Высокой светимости). Рассмотрены проблемы поддержки и модернизации существующих ресурсов и коммуникаций. The report presents the results of the work of Russian institutes in the processing of ALICE experiment data during the last 2 years of the LHC RUN2. The main issues and tasks facing both ALICE Grid Computing and its Russian segment before the start of RUN3 (2020 -onward) are considered. Plans and preparations for the operation of the LHC in the HL (High luminosity) mode are presented. The challenges of support and modernization of the existing resources and networking are discussed.
</Content>
<field id="content">
В докладе представлены результаты работы российских институтов в обработке данных эксперимента ALICE в течеие 2-х последних лет во время 2-го этапа работы Большого адронного коллайдера (БАК). Рассмотрены основные проблемы и задачи, стоящие как перед ALICE Grid Computing, так и перед его российским сегментом перед третьим этапом работы БАК. Также представлены планы подготовки к работе БАК в режиме HL (Высокой светимости). Рассмотрены проблемы поддержки и модернизации существующих ресурсов и коммуникаций. The report presents the results of the work of Russian institutes in the processing of ALICE experiment data during the last 2 years of the LHC RUN2. The main issues and tasks facing both ALICE Grid Computing and its Russian segment before the start of RUN3 (2020 -onward) are considered. Plans and preparations for the operation of the LHC in the HL (High luminosity) mode are presented. The challenges of support and modernization of the existing resources and networking are discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>272</Id>
<Title>
APPROACHES TO THE AUTOMATED DEPLOYMENT OF THE CLOUD INFRASTRUCTURE OF GEOGRAPHICALLY DISTRIBUTED DATA CENTERS
</Title>
<Content>
University ITMO (ifmo.ru) is designing the system for cloud of geographically distributed data centers under centralized administration to control the distributed virtual storage, virtual data links, virtual machines, and data center infrastructure management. The system needs to be tolerant to hardware and software failures of any type. The integrated set of programs is developed to implement mentioned goals. Each program of the set is relatively independent agent in form of VM or container which can run on different hardware servers. Any agent might send the request for specific service to another agent with developed protocol. The cloud system of distributed data centers assumes well known functionality: creation, management, and provision of services with defined SLA. In presented approach most of above functions is implemented in form of mentioned agents. The installation of the system in a number of data centers is implemented with a range of automated deployment steps. Many FOSS components like Openstack, CEPH, SALT, Grafana/Kibana, Zabbix, RabbitMQ, etc were used as toolkits in this design. The developed cloud is now under heavy testing/developing.
</Content>
<field id="content">
University ITMO (ifmo.ru) is designing the system for cloud of geographically distributed data centers under centralized administration to control the distributed virtual storage, virtual data links, virtual machines, and data center infrastructure management. The system needs to be tolerant to hardware and software failures of any type. The integrated set of programs is developed to implement mentioned goals. Each program of the set is relatively independent agent in form of VM or container which can run on different hardware servers. Any agent might send the request for specific service to another agent with developed protocol. The cloud system of distributed data centers assumes well known functionality: creation, management, and provision of services with defined SLA. In presented approach most of above functions is implemented in form of mentioned agents. The installation of the system in a number of data centers is implemented with a range of automated deployment steps. Many FOSS components like Openstack, CEPH, SALT, Grafana/Kibana, Zabbix, RabbitMQ, etc were used as toolkits in this design. The developed cloud is now under heavy testing/developing.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Petr</FirstName>
<FamilyName>Fedchenkov</FamilyName>
<Email>pvfedchenkov@corp.ifmo.ru</Email>
<Affiliation>ITMO</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Samokhin</FamilyName>
<Email>samon@corp.ifmo.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Khoruzhnikov</FamilyName>
<Email>xse@vuztc.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Lazo</FamilyName>
<Email>oll@vuztc.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>273</Id>
<Title>
Direct Simulation of the Charge Transfer along Oligonucleotides at T=300K
</Title>
<Content>
At present, the attention of researchers is attracted to the possible mechanisms of charge transfer in quasy-1D biomacromolecules, such as DNA, in connection with the potential use of this nano-objects in nanobioelectronics. Biophysical experiments on the hole transfer from guanine G (donor) to guanine triplet GGG (acceptor), separated by adenine-thymine (A-T) bridges of various lengths, demonstrate that the rate of charge transfer between donor and acceptor decreases exponentially with increasing separation only if the guanines are separated by no more than three base pairs; if more bridging base pairs are present, the transfer rates exhibit only a weak distance dependence. We performed direct numerical experiment on the charge transfer from donor to the acceptor along bridge, consisting of homogeneous sites. The model is based on the semi-classical Holstein Hamiltonian. The Holstein polaron model is simple but relevant for explaining charge transfer in DNA. To take into account the temperature, Langevin thermostat is used. For computation we chose some parameter values as for DNA model: the charge donor is guanine G, acceptor - guanine triplet GGG, and the bridge consists of adenines A or thimines T with number of sites N. We modeled different length of bridge N from 1 to 26 sites (length of the whole chain is from 5 to 30 sites). For each N we calculated a set of 100 samples at temperature 300 K, and estimated time-dependencies averaged over the ensemble. The sample is a trajectory of system with its own initial data and pseudorandom time series, which modeling medium thermal fluctuations. Initial data for classical sites are chosen from Maxwell distribution corresponding to T = 300 K, and the charge at moment t = 0 is localized on the donor. We calculate the samples until the probabilities of charge distribution on the sites become similar to the thermodynamic equilibrium state, and estimate the time t_TDE to reach this state. Results of the simulation demonstrate that: for short chains (Nменьше4 for bridges of adenines and Nменьше5 for thymine bridges) the value of t_TDE increases exponentially with the increasing N; for big N, t_TDE values are almost the same. In suggestion that the charge transfer rate is a reciprocal of the time t_TDE, the results of computer modeling and data of biophysical experiments have a qualitative similarity. We are grateful to the HybriLIT group of JINR for computational resources. The work is partially supported by the Russian Foundation for Basic Research, projects no. 16-07-00305, 17-07-00801, and Russian Science Foundation, grant 16-11-10163.
</Content>
<field id="content">
At present, the attention of researchers is attracted to the possible mechanisms of charge transfer in quasy-1D biomacromolecules, such as DNA, in connection with the potential use of this nano-objects in nanobioelectronics. Biophysical experiments on the hole transfer from guanine G (donor) to guanine triplet GGG (acceptor), separated by adenine-thymine (A-T) bridges of various lengths, demonstrate that the rate of charge transfer between donor and acceptor decreases exponentially with increasing separation only if the guanines are separated by no more than three base pairs; if more bridging base pairs are present, the transfer rates exhibit only a weak distance dependence. We performed direct numerical experiment on the charge transfer from donor to the acceptor along bridge, consisting of homogeneous sites. The model is based on the semi-classical Holstein Hamiltonian. The Holstein polaron model is simple but relevant for explaining charge transfer in DNA. To take into account the temperature, Langevin thermostat is used. For computation we chose some parameter values as for DNA model: the charge donor is guanine G, acceptor - guanine triplet GGG, and the bridge consists of adenines A or thimines T with number of sites N. We modeled different length of bridge N from 1 to 26 sites (length of the whole chain is from 5 to 30 sites). For each N we calculated a set of 100 samples at temperature 300 K, and estimated time-dependencies averaged over the ensemble. The sample is a trajectory of system with its own initial data and pseudorandom time series, which modeling medium thermal fluctuations. Initial data for classical sites are chosen from Maxwell distribution corresponding to T = 300 K, and the charge at moment t = 0 is localized on the donor. We calculate the samples until the probabilities of charge distribution on the sites become similar to the thermodynamic equilibrium state, and estimate the time t_TDE to reach this state. Results of the simulation demonstrate that: for short chains (Nменьше4 for bridges of adenines and Nменьше5 for thymine bridges) the value of t_TDE increases exponentially with the increasing N; for big N, t_TDE values are almost the same. In suggestion that the charge transfer rate is a reciprocal of the time t_TDE, the results of computer modeling and data of biophysical experiments have a qualitative similarity. We are grateful to the HybriLIT group of JINR for computational resources. The work is partially supported by the Russian Foundation for Basic Research, projects no. 16-07-00305, 17-07-00801, and Russian Science Foundation, grant 16-11-10163.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Fialko</FamilyName>
<Email>fialka@impb.ru</Email>
<Affiliation>IMPB RAS - the Branch of KIAM RAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Lakhno</FamilyName>
<Email>lak@impb.ru</Email>
<Affiliation>IMPB RAS – the Branch of KIAM RAS</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nadezhda</FirstName>
<FamilyName>Fialko</FamilyName>
<Email>fialka@impb.ru</Email>
<Affiliation>IMPB RAS - the Branch of KIAM RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>274</Id>
<Title>The distributed grid site of Institute of Physics</Title>
<Content>
The Computing Center of the Institute of Physics (IOP) of the Czech Academy of Sciences serves a broad spectrum of users with various computing needs. The Computing Center hosts a WLCG Tier-2 for ATLAS and ALICE experiments. There are also other supported experiments from astroparticle physics, namely Cherenkov Telescope Array and Pierre Auger Observatory. Center also supports OSG stack for the NOvA and DUNE experiments. Computing resources are also utilized by local users from IOP through HTCondor batch system. Hosted storage capacity is divided between grid services (DPM and XrootD) and locally accessible NFS storage. Computing resources are distributed among several locations in the Czech Republic. This contribution will describe mentioned topics in more detail. It will also give insight in our experience with different classes of hardware and with different approaches of administration and monitoring of all services.
</Content>
<field id="content">
The Computing Center of the Institute of Physics (IOP) of the Czech Academy of Sciences serves a broad spectrum of users with various computing needs. The Computing Center hosts a WLCG Tier-2 for ATLAS and ALICE experiments. There are also other supported experiments from astroparticle physics, namely Cherenkov Telescope Array and Pierre Auger Observatory. Center also supports OSG stack for the NOvA and DUNE experiments. Computing resources are also utilized by local users from IOP through HTCondor batch system. Hosted storage capacity is divided between grid services (DPM and XrootD) and locally accessible NFS storage. Computing resources are distributed among several locations in the Czech Republic. This contribution will describe mentioned topics in more detail. It will also give insight in our experience with different classes of hardware and with different approaches of administration and monitoring of all services.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexandr</FirstName>
<FamilyName>Mikula</FamilyName>
<Email>mikula@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences; CESNET</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Martin</FirstName>
<FamilyName>Adam</FamilyName>
<Email>madam@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jiří</FirstName>
<FamilyName>Chudoba</FamilyName>
<Email>chudoba@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dagmar</FirstName>
<FamilyName>Adamová</FamilyName>
<Email>adamova@ujf.cas.cz</Email>
<Affiliation>Nuclear Physics Institute of the Czech Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Jana</FirstName>
<FamilyName>Uhlířová</FamilyName>
<Email>uhlirova@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Horák</FamilyName>
<Email>horakp@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Vokáč</FamilyName>
<Email>petr.vokac@fjfi.cvut.cz</Email>
<Affiliation>Faculty of Nuclear Sciences and Physical Engineering of the Czech Technical University in Prague</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexandr</FirstName>
<FamilyName>Mikula</FamilyName>
<Email>mikula@fzu.cz</Email>
<Affiliation>Institute of Physics of the Czech Academy of Sciences; CESNET</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>275</Id>
<Title>
Large scale simulations with parallel annealing algorithm
</Title>
<Content>
Population annealing algorithm designed for the simulations of the statistical mechanics systems with rugged free energy landscape. We report on the realization of the algorithm for the use on the hybrid computing architecture combining CPU and GPGPU. Algorithm is fully scalable. We report application of the developed realization to several interesting problems. Algorithm can be applied to any system of statistical mechanics, described by partition function.
</Content>
<field id="content">
Population annealing algorithm designed for the simulations of the statistical mechanics systems with rugged free energy landscape. We report on the realization of the algorithm for the use on the hybrid computing architecture combining CPU and GPGPU. Algorithm is fully scalable. We report application of the developed realization to several interesting problems. Algorithm can be applied to any system of statistical mechanics, described by partition function.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Lev</FirstName>
<FamilyName>Shchur</FamilyName>
<Email>levshchur@gmail.com</Email>
<Affiliation>Landau Institute for Theoretical Physics, Science Center in Chernogolovka</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Lev</FirstName>
<FamilyName>Shchur</FamilyName>
<Email>levshchur@gmail.com</Email>
<Affiliation>Landau Institute for Theoretical Physics, Science Center in Chernogolovka</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>276</Id>
<Title>ALICE DCS preparation for Run 3</Title>
<Content>
The ALICE experiment is havy ion collision detector at the CERN LHC. Its goal to study extreme phase of matter – called quark-gluon plasma. It is collaboration of 41 countries and more than 1800 scientists. A large number of complex subsystems requires supervision and control system. ALICE Control Coordination (ACC) is the functional unit mandated to coordinate the execution of the Detector control system (DCS). In 2020, the ALICE experiment at CERN will start collecting data with upgraded detector. The ALICE upgrade addresses the challenge of reading out and inspecting the Pb-Pb collisions at rates of 50 kHz, sampling the pp and p-Pb at up to 200 kHz. ALICE O2 project meres online and offline into one large system with ~8400 optical links, data rate 1.1 TB/s, data storage ~60PB/year. From DCS O2 requires continuous data flow with ~100 000 conditions parameters for event reconstruction. Data has to be injected into each 20ms data frame. DCS-O2 interface consists of electronics and software modules for configuring CRU controllers and provide continuous dataflow to O2 system. In this talk, we will describe the architecture and functionality of the ADAPOS mechanism. We will discuss the requirements and results obtained during the test campaign. We will also provide a description of a new front-end access mechanism allowing for detector control in parallel to the data acquisition.
</Content>
<field id="content">
The ALICE experiment is havy ion collision detector at the CERN LHC. Its goal to study extreme phase of matter – called quark-gluon plasma. It is collaboration of 41 countries and more than 1800 scientists. A large number of complex subsystems requires supervision and control system. ALICE Control Coordination (ACC) is the functional unit mandated to coordinate the execution of the Detector control system (DCS). In 2020, the ALICE experiment at CERN will start collecting data with upgraded detector. The ALICE upgrade addresses the challenge of reading out and inspecting the Pb-Pb collisions at rates of 50 kHz, sampling the pp and p-Pb at up to 200 kHz. ALICE O2 project meres online and offline into one large system with ~8400 optical links, data rate 1.1 TB/s, data storage ~60PB/year. From DCS O2 requires continuous data flow with ~100 000 conditions parameters for event reconstruction. Data has to be injected into each 20ms data frame. DCS-O2 interface consists of electronics and software modules for configuring CRU controllers and provide continuous dataflow to O2 system. In this talk, we will describe the architecture and functionality of the ADAPOS mechanism. We will discuss the requirements and results obtained during the test campaign. We will also provide a description of a new front-end access mechanism allowing for detector control in parallel to the data acquisition.
</field>
<field id="summary">
A new mechanism, called ADAPOS has been developed. Its role is to collect condition parameters from the distributed SCADA system and provide this data to O2 - A new combined online-offline facility.
</field>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Kurepin</FamilyName>
<Email>alexander.kurepin@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andre</FirstName>
<FamilyName>Augustinus</FamilyName>
<Email>andre.augustinus@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Peter</FirstName>
<FamilyName>Chochula</FamilyName>
<Email>peter.chochula@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ombretta</FirstName>
<FamilyName>Pinazza</FamilyName>
<Email>ombretta.pinazza@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Mateusz</FirstName>
<FamilyName>Lechman</FamilyName>
<Email>mateusz.lechman@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Peter Matthew</FirstName>
<FamilyName>Bond</FamilyName>
<Email>p.bond@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Kevin</FirstName>
<FamilyName>Cifuentes Salas</FamilyName>
<Email>kevin.cifuentes.salas@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>John</FirstName>
<FamilyName>Larry Lang</FamilyName>
<Email>john.larry.lang@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Kurepin</FamilyName>
<Email>alexander.kurepin@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>277</Id>
<Title>
Agent Technology Situational Express Analysis in Assessment of Technological Development Level of the BRICS Countries
</Title>
<Content>
Stages of development and operation of specialized agent system concerning collection and analysis of the BRICS countries' scientific publications are considered in this paper. The data are extracted from more than 60 sources of authoritative publications in fields of Chemistry, Physics, Genetics, Biochemistry, Ecology, Geology etc. Algorithms for data analysis used in the system directed to reveal scientometric indicators and factographic information. The fact analyzed scientific publications are indexed by a referential database Web of Science indicates credibility level of the material. However, the form of Web of Science providing information imposes its limitations, that can be overcome with the help of specialized agents in the inner loop of the system. Aggregation of the material is done in a centralized database. However, there is also a mechanism using prepared SQL queries and a separate function for forming tables of the proper format for data output as MS Excel format, that is appropriate for an end user. The work result let to assess the development level of certain technologies and research in the BRICS countries in a short time. And since the system has a certain degree of autonomy a constant monitoring of scientific and technical activities in the BRICS countries is possible. It is concluded that the use of agent technologies for collection and processing of materials in this field significantly accelerates the analysis of scientific and technical publications in comparison with manual mode, and also have a high degree concretization of particular indicators for scientific activity in the analyzed field of publication activity.
</Content>
<field id="content">
Stages of development and operation of specialized agent system concerning collection and analysis of the BRICS countries' scientific publications are considered in this paper. The data are extracted from more than 60 sources of authoritative publications in fields of Chemistry, Physics, Genetics, Biochemistry, Ecology, Geology etc. Algorithms for data analysis used in the system directed to reveal scientometric indicators and factographic information. The fact analyzed scientific publications are indexed by a referential database Web of Science indicates credibility level of the material. However, the form of Web of Science providing information imposes its limitations, that can be overcome with the help of specialized agents in the inner loop of the system. Aggregation of the material is done in a centralized database. However, there is also a mechanism using prepared SQL queries and a separate function for forming tables of the proper format for data output as MS Excel format, that is appropriate for an end user. The work result let to assess the development level of certain technologies and research in the BRICS countries in a short time. And since the system has a certain degree of autonomy a constant monitoring of scientific and technical activities in the BRICS countries is possible. It is concluded that the use of agent technologies for collection and processing of materials in this field significantly accelerates the analysis of scientific and technical publications in comparison with manual mode, and also have a high degree concretization of particular indicators for scientific activity in the analyzed field of publication activity.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgeny</FirstName>
<FamilyName>Tretyakov</FamilyName>
<Email>estretyakov@mephi.ru</Email>
<Affiliation>NRNU "MEPhI"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Diana</FirstName>
<FamilyName>Koshlan</FamilyName>
<Email>dkoshlan@yandex.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Boris</FirstName>
<FamilyName>Onykij</FamilyName>
<Email>bnonykij@mephi.ru</Email>
<Affiliation>National Researcn Nuclear University MEPhI (NRNU MEPhI)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Artamonov</FamilyName>
<Email>chrkssk@yandex.ru</Email>
<Affiliation>National Research Nuclear University MEPhI (Moscow Engineering Physics Institute)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Diana</FirstName>
<FamilyName>Koshlan</FamilyName>
<Email>dkoshlan@yandex.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>278</Id>
<Title>
Properties of The Parallel Discrete Event Simulation Algorithms on Small-World Communication Networks
</Title>
<Content>
We discuss synchronization aspects in the method of large-scale simulation, known as parallel discrete event simulation (PDES). We build models of the evolution of simulation time profile in two PDES algorithms, in conservative algorithm and in optimistic one. The models capture the essential properties of the algorithms, namely, the scalability and the degree of desynchronization. We investigate the models on small-world communication networks (SW), which constructed as regular lattices with addition of small fraction of long-range communication links. SW networks are characterized by the small length of average shortest path and by the large value of clustering coefficient. We show that synchronization is better, when processing elements are arranged in SW topology, rather than in regular lattices. In PDES algorithms on SW network the desynchronization remains constant in the limit of infinite number of processing elements, and the same time the average utilization remains positive. We also find, that the degree of clustering in networks has no influence on the synchronization between processing elements, and the synchronization is mainly affected by the length of average shortest path. We present the results of our simulations and compare them with the case-study simulations.
</Content>
<field id="content">
We discuss synchronization aspects in the method of large-scale simulation, known as parallel discrete event simulation (PDES). We build models of the evolution of simulation time profile in two PDES algorithms, in conservative algorithm and in optimistic one. The models capture the essential properties of the algorithms, namely, the scalability and the degree of desynchronization. We investigate the models on small-world communication networks (SW), which constructed as regular lattices with addition of small fraction of long-range communication links. SW networks are characterized by the small length of average shortest path and by the large value of clustering coefficient. We show that synchronization is better, when processing elements are arranged in SW topology, rather than in regular lattices. In PDES algorithms on SW network the desynchronization remains constant in the limit of infinite number of processing elements, and the same time the average utilization remains positive. We also find, that the degree of clustering in networks has no influence on the synchronization between processing elements, and the synchronization is mainly affected by the length of average shortest path. We present the results of our simulations and compare them with the case-study simulations.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Liliia</FirstName>
<FamilyName>Ziganurova</FamilyName>
<Email>ziganurova@gmail.com</Email>
<Affiliation>Scientific Center in Chernogolovka, National Research University Higher School of Economics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Lev</FirstName>
<FamilyName>Shchur</FamilyName>
<Email>levshchur@gmail.com</Email>
<Affiliation>National Research University Higher School of Economics, Landau Institute for Theoretical Physics</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Liliia</FirstName>
<FamilyName>Ziganurova</FamilyName>
<Email>ziganurova@gmail.com</Email>
<Affiliation>Scientific Center in Chernogolovka, National Research University Higher School of Economics</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>279</Id>
<Title>
Optimisation of TensorFlow applications on the workstation Intel® Xeon® Platinum
</Title>
<Content>
Платформа TensorFlow является одним из наиболее развитых наборов программных продуктов с открытым кодом для задач машинного обучения. С другой стороны, рабочие станции на базе процессоров Intel® Xeon® Platinum представляются перспективным аппаратным решением для задач машинного обучения. Их отличительная черта состоит в комбинации из трех важных элементов. Во-первых, это большое число тяжелых ядер в одном CPU, более двух десятков. В нашем случае, это 26 ядер в каждом из двух процессоров 8164, плюс hyper-threading. Во-вторых, это наличие двух устройств AVX-512 (Advanced Vector Extension 512), которые дают возможность работы с 512-битными регистрами. Теоретически это позволяет ускорить вычисления на 32-битными числами в 16 раз. В третьих, это очень большой размер памяти, на одной материнской плате 1.5 ТВ высокоскоростной памяти DDR4, которая поддерживается большим кэшем второго уровня. Такое устройство одновременно предоставляет большую скорость вычислений и работу с данными большого объема в оперативной памяти. В перспективе, это позволяет проводить анализ сложных проблем с большим объемом данных. Мы обсуждаем результаты тестирования некоторых приложений с использованием платформы TensorFlow фирмы Google и библиотеки Intel® Math Kernel Libraries (Intel® MKL).
</Content>
<field id="content">
Платформа TensorFlow является одним из наиболее развитых наборов программных продуктов с открытым кодом для задач машинного обучения. С другой стороны, рабочие станции на базе процессоров Intel® Xeon® Platinum представляются перспективным аппаратным решением для задач машинного обучения. Их отличительная черта состоит в комбинации из трех важных элементов. Во-первых, это большое число тяжелых ядер в одном CPU, более двух десятков. В нашем случае, это 26 ядер в каждом из двух процессоров 8164, плюс hyper-threading. Во-вторых, это наличие двух устройств AVX-512 (Advanced Vector Extension 512), которые дают возможность работы с 512-битными регистрами. Теоретически это позволяет ускорить вычисления на 32-битными числами в 16 раз. В третьих, это очень большой размер памяти, на одной материнской плате 1.5 ТВ высокоскоростной памяти DDR4, которая поддерживается большим кэшем второго уровня. Такое устройство одновременно предоставляет большую скорость вычислений и работу с данными большого объема в оперативной памяти. В перспективе, это позволяет проводить анализ сложных проблем с большим объемом данных. Мы обсуждаем результаты тестирования некоторых приложений с использованием платформы TensorFlow фирмы Google и библиотеки Intel® Math Kernel Libraries (Intel® MKL).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Svetlana</FirstName>
<FamilyName>Shikota</FamilyName>
<Email>svetlana.shikota@gmail.com</Email>
<Affiliation>Science Center in Chernogolovka</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Lev</FirstName>
<FamilyName>Shchur</FamilyName>
<Email>lshchur@hse.ru</Email>
<Affiliation>Professor, Higher School of Economics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Russkov</FamilyName>
<Email>russkov@inbox.ru</Email>
<Affiliation>Science Center in Chernogolovka</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Svetlana</FirstName>
<FamilyName>Shikota</FamilyName>
<Email>svetlana.shikota@gmail.com</Email>
<Affiliation>Science Center in Chernogolovka</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>280</Id>
<Title>
Сверточная нейронная сеть в системе стереозрения мобильного робота
</Title>
<Content>
Распознавания образов – научная дисциплина, целью которой является классификация объектов. Сами объекты называются образами или паттернами. Возможность распознавания опирается на схожесть однотипных объектов. Несмотря на то, что все предметы и ситуации уникальны в строгом смысле, между некоторыми из них всегда можно найти сходства по тому или иному признаку. Отсюда возникает понятие классификации – разбиения всего множества объектов на непересекающиеся подмножества – классы, элементы которых имеют некоторые схожие свойства, отличающие их от элементов других классов. И, таким образом, задачей распознавания является отнесение рассматриваемых объектов или явлений по их описанию к нужным классам. Высокие показатели качества распознавания образов достигаются за счет инвариантного распознавания. Несмотря на изменчивость образов, относящихся к одному и тому же классу, классификация нового образа при инвариантном распознавании может быть осуществлена правильно. Разработка и совершенствование методов компьютерного зрения позволяет расширить круг выполняемых компьютерами задач и сделать машинную переработку информации более интеллектуальной. Задача инвариантного распознавания образов на сегодняшний день остаётся важной нерешённой задачей, относящейся к задачам искусственного интеллекта. Когнитивные способности человека очень тяжело смоделировать на вычислительной технике, не существует единой и эффективной теории, которая бы объясняла, как человек способен с большой точностью распознавать объекты внешнего мира. Представляемая система распознавания образов базируется на технологии стереозрения. Модуль распознавания выделяет объекты и осуществляет слежение за ними. Однако стоит отметить, что при изменении условий окружающей среды (изменение освещения) наблюдается значительное снижение качества распознавания. Изобилие систем машинного зрения не устраняет главные недостатки систем распознавания – погрешность распознавания при изменении ракурса объекта, изменение освещения, чувствительность ПО и т.д. На сегодняшний день лучшие результаты в распознавании образов получают с помощью сверточных нейронных сетей (СНС). При достаточно большом размере СНС имеют небольшое количество настраиваемых параметров, довольно быстро обучаются. Именно поэтому на начальном этапе интеллектуализации системы распознавания было решено использовать сверточную нейронную сеть. В настоящее время довольно подробно описано множество алгоритмов и методик компьютерного зрения и распознавания образов. Данные алгоритмы и методы имеют определенные недостатки, которые перечислены выше. Поэтому разработка универсального и гораздо более эффективного алгоритма распознавания – первостепенная задача исследователя в области компьютерного зрения. На данный момент разрабатывается технология распознавания, базирующаяся на мягких и квантовых вычислениях. С помощью данной технологии представляется возможным повысить эффективность процесса распознавания.
</Content>
<field id="content">
Распознавания образов – научная дисциплина, целью которой является классификация объектов. Сами объекты называются образами или паттернами. Возможность распознавания опирается на схожесть однотипных объектов. Несмотря на то, что все предметы и ситуации уникальны в строгом смысле, между некоторыми из них всегда можно найти сходства по тому или иному признаку. Отсюда возникает понятие классификации – разбиения всего множества объектов на непересекающиеся подмножества – классы, элементы которых имеют некоторые схожие свойства, отличающие их от элементов других классов. И, таким образом, задачей распознавания является отнесение рассматриваемых объектов или явлений по их описанию к нужным классам. Высокие показатели качества распознавания образов достигаются за счет инвариантного распознавания. Несмотря на изменчивость образов, относящихся к одному и тому же классу, классификация нового образа при инвариантном распознавании может быть осуществлена правильно. Разработка и совершенствование методов компьютерного зрения позволяет расширить круг выполняемых компьютерами задач и сделать машинную переработку информации более интеллектуальной. Задача инвариантного распознавания образов на сегодняшний день остаётся важной нерешённой задачей, относящейся к задачам искусственного интеллекта. Когнитивные способности человека очень тяжело смоделировать на вычислительной технике, не существует единой и эффективной теории, которая бы объясняла, как человек способен с большой точностью распознавать объекты внешнего мира. Представляемая система распознавания образов базируется на технологии стереозрения. Модуль распознавания выделяет объекты и осуществляет слежение за ними. Однако стоит отметить, что при изменении условий окружающей среды (изменение освещения) наблюдается значительное снижение качества распознавания. Изобилие систем машинного зрения не устраняет главные недостатки систем распознавания – погрешность распознавания при изменении ракурса объекта, изменение освещения, чувствительность ПО и т.д. На сегодняшний день лучшие результаты в распознавании образов получают с помощью сверточных нейронных сетей (СНС). При достаточно большом размере СНС имеют небольшое количество настраиваемых параметров, довольно быстро обучаются. Именно поэтому на начальном этапе интеллектуализации системы распознавания было решено использовать сверточную нейронную сеть. В настоящее время довольно подробно описано множество алгоритмов и методик компьютерного зрения и распознавания образов. Данные алгоритмы и методы имеют определенные недостатки, которые перечислены выше. Поэтому разработка универсального и гораздо более эффективного алгоритма распознавания – первостепенная задача исследователя в области компьютерного зрения. На данный момент разрабатывается технология распознавания, базирующаяся на мягких и квантовых вычислениях. С помощью данной технологии представляется возможным повысить эффективность процесса распознавания.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Ulyanov</FamilyName>
<Email>ulyanovsv@mail.ru</Email>
<Affiliation>Viktorovich</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Kirill</FirstName>
<FamilyName>Koshelev</FamilyName>
<Email>kirill_koshelev18@rambler.ru</Email>
<Affiliation>Viktorovich</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Kirill</FirstName>
<FamilyName>Koshelev</FamilyName>
<Email>kirill_koshelev18@rambler.ru</Email>
<Affiliation>Viktorovich</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>281</Id>
<Title>
The interoperability problem during the implementation of the FOURTH PARADIGM
</Title>
<Content>
As it is known, at the present time worldwide there is a transition to the so called FOURTH PARADIGM in the methods and means of scientific researches [1]. The essence of the FOURTH PARADIGM is the intensive use of information technologies for the simultaneous using of large amounts of experimental data, numerical simulation results and accumulated knowledge. It is obvious that this requires using of high-performance distributed environment, which includes individual supercomputers, clusters, GRID-systems, cloud computing systems and end-user personal computers. It is also quite obvious that in such a purely heterogeneous environment, which should be classified as a System of Systems (SOS), there is a problem of compatibility and interaction of heterogeneous software and hardware platforms, called "interoperability problem". The interoperability problem should be solved on the basis of the use of profiles - sets of ICT-standards, this problem is dealt with by many organizations and individual researchers around the world, but it is never solved until the end, due to the great complexity. In particular, therefore, the issue of interoperability and development of standards are included in the RAS Program of fundamental researches for 2013-2020 (clause 34). The authors investigate the problem of interoperability for more than 10 years and have developed several standards. The main result should be considered the proposed unified approach to ensuring interoperability for information systems (IS) of the widest class [2], which is subsequently issued in the form of GOST R 55062-2012. Further, the authors applied this approach to specific areas: e-science, e-education, e-health, e-libraries, e-military. Research was also conducted for GRID and cloud computing systems. The authors have regularly reported their results at previous JINR conferences since 2010 [3]. All of the above classes of IS are components of the SoS. Therefore, using these developments and foreign experience [4], we have now started to solve the problem of interoperability in SoS. The problem is very difficult and we have only preliminary results. This work is done with the support of the Program №. 27 of fundamental researches of the RAS Presidium. Literature: 1. The Fourth Paradigm Data-Intensive Scientific Discovery. Microsoft research Redmond, WA 2. Gulyaev Yu. V., Zhuravlev E. E., Oleynikov A.Ya. Methodology standardization to ensure interoperability of information systems wide class. Zhurnal Radioelektroniki - Journal of Radio Electronics. 2012. N2. Available at: http://jre.cplire.ru/win/mar12/2/text.pdf, accessed: 14.12.2017 3. S. V. Ivanov, A. Y. Oleynikov. Methodology and algorithm for selecting standards for interoperability profile in cloud computing. Proceedings of the 7th International conference " Distributed computing and Grid technology in science and education, Dubna, JINR, 4-9, July 2016 pp.264-268. 4. Information system Development: improving enterprise communication. (chapters 7 and 9). Proceedings from the 22nd annual meeting (ISD2013) held in Seville, Spain, from September 2 to 4, 2013
</Content>
<field id="content">
As it is known, at the present time worldwide there is a transition to the so called FOURTH PARADIGM in the methods and means of scientific researches [1]. The essence of the FOURTH PARADIGM is the intensive use of information technologies for the simultaneous using of large amounts of experimental data, numerical simulation results and accumulated knowledge. It is obvious that this requires using of high-performance distributed environment, which includes individual supercomputers, clusters, GRID-systems, cloud computing systems and end-user personal computers. It is also quite obvious that in such a purely heterogeneous environment, which should be classified as a System of Systems (SOS), there is a problem of compatibility and interaction of heterogeneous software and hardware platforms, called "interoperability problem". The interoperability problem should be solved on the basis of the use of profiles - sets of ICT-standards, this problem is dealt with by many organizations and individual researchers around the world, but it is never solved until the end, due to the great complexity. In particular, therefore, the issue of interoperability and development of standards are included in the RAS Program of fundamental researches for 2013-2020 (clause 34). The authors investigate the problem of interoperability for more than 10 years and have developed several standards. The main result should be considered the proposed unified approach to ensuring interoperability for information systems (IS) of the widest class [2], which is subsequently issued in the form of GOST R 55062-2012. Further, the authors applied this approach to specific areas: e-science, e-education, e-health, e-libraries, e-military. Research was also conducted for GRID and cloud computing systems. The authors have regularly reported their results at previous JINR conferences since 2010 [3]. All of the above classes of IS are components of the SoS. Therefore, using these developments and foreign experience [4], we have now started to solve the problem of interoperability in SoS. The problem is very difficult and we have only preliminary results. This work is done with the support of the Program №. 27 of fundamental researches of the RAS Presidium. Literature: 1. The Fourth Paradigm Data-Intensive Scientific Discovery. Microsoft research Redmond, WA 2. Gulyaev Yu. V., Zhuravlev E. E., Oleynikov A.Ya. Methodology standardization to ensure interoperability of information systems wide class. Zhurnal Radioelektroniki - Journal of Radio Electronics. 2012. N2. Available at: http://jre.cplire.ru/win/mar12/2/text.pdf, accessed: 14.12.2017 3. S. V. Ivanov, A. Y. Oleynikov. Methodology and algorithm for selecting standards for interoperability profile in cloud computing. Proceedings of the 7th International conference " Distributed computing and Grid technology in science and education, Dubna, JINR, 4-9, July 2016 pp.264-268. 4. Information system Development: improving enterprise communication. (chapters 7 and 9). Proceedings from the 22nd annual meeting (ISD2013) held in Seville, Spain, from September 2 to 4, 2013
</field>
<field id="summary">
The problem of interoperability in the implementation of the fourth paradigm of scientific research is considered. It is indicated that the information infrastructure in the implementation of the fourth paradigm belongs to the class System of Systems –SoS. This information infrastructure is a purely heterogeneous environment in which the problem of interoperability is both very relevant and very complex. Preliminary results on solving the problem of interoperability for SoS, obtained on the basis of the approach developed by the authors and using international experience, are presented.
</field>
<PrimaryAuthor>
<FirstName>ALEXANDER</FirstName>
<FamilyName>OLEYNIKOV</FamilyName>
<Email>olein@cplire.ru</Email>
<Affiliation>IRE RAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Kamenshchikov</FamilyName>
<Email>prostonau@mail.ru</Email>
<Affiliation>Kotelnikov Institute of Radioengineering and Electronics of Russian Academy of Sciences</Affiliation>
</Co-Author>
<Speaker>
<FirstName>ALEXANDER</FirstName>
<FamilyName>OLEYNIKOV</FamilyName>
<Email>olein@cplire.ru</Email>
<Affiliation>IRE RAS</Affiliation>
</Speaker>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kamenshchikov</FamilyName>
<Email>prostonau@mail.ru</Email>
<Affiliation>Kotelnikov Institute of Radioengineering and Electronics of Russian Academy of Sciences</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>282</Id>
<Title>
Russian-Language speech recognition system based on deepspeech
</Title>
<Content>
The paper examines the practical issues in developing a speech-to-text system using deep neural networks. The development of a Russian-language speech recognition system based on DeepSpeech architecture is described. The Mozilla company’s open source implementation of DeepSpeech for the English language was used as a starting point. The system was trained in a containerized environment using the Docker technology. It allowed to describe the entire process of component assembly from the source code, including a number of optimization techniques for CPU and GPU. Docker also allows to easily reproduce computation optimization tests on alternative infrastructures. We examined the use of TensorFlow XLA technology that optimizes linear algebra computations in the course of neural network training. The number of nodes in the internal layers of neural network was optimized based on the word error rate (WER) obtained on a test data set, having regard to GPU memory limitations. We studied the use of probabilistic language models with various maximum lengths of word sequences and selected the model that shows the best WER. Our study resulted in a Russian-language acoustic model having been trained based on a data set comprising audio and subtitles from YouTube video clips. The language model was built based on the texts of subtitles and publicly available Russian-language corpus of Wikipedia’s popular articles. The resulting system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</Content>
<field id="content">
The paper examines the practical issues in developing a speech-to-text system using deep neural networks. The development of a Russian-language speech recognition system based on DeepSpeech architecture is described. The Mozilla company’s open source implementation of DeepSpeech for the English language was used as a starting point. The system was trained in a containerized environment using the Docker technology. It allowed to describe the entire process of component assembly from the source code, including a number of optimization techniques for CPU and GPU. Docker also allows to easily reproduce computation optimization tests on alternative infrastructures. We examined the use of TensorFlow XLA technology that optimizes linear algebra computations in the course of neural network training. The number of nodes in the internal layers of neural network was optimized based on the word error rate (WER) obtained on a test data set, having regard to GPU memory limitations. We studied the use of probabilistic language models with various maximum lengths of word sequences and selected the model that shows the best WER. Our study resulted in a Russian-language acoustic model having been trained based on a data set comprising audio and subtitles from YouTube video clips. The language model was built based on the texts of subtitles and publicly available Russian-language corpus of Wikipedia’s popular articles. The resulting system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>George</FirstName>
<FamilyName>Fedoseev</FamilyName>
<Email>george.fedoseev@me.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>A.</FirstName>
<FamilyName>Shaleva</FamilyName>
<Email>n.n.n@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Olga</FirstName>
<FamilyName>Sedova</FamilyName>
<Email>n.n@gmail.com</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>283</Id>
<Title>
BOINC-based comparison of the geoacoustic inversion algorithms efficiency
</Title>
<Content>
The BOINC-based volunteer computing project Acoustics@home was employed to study the accuracy of the sound speed profile reconstruction in a shallow-water waveguide using a dispersion-based geoacoustic inversion scheme. This problem was transformed into a problem of black-box minimization of a certain mismatch function. According to the first approach, a sound speed profile is considered a piecewise-linear function with fixed uniformly-spaced nodes. At these nodes, the values of sound speed are obtained in the course of inversion. In the second approach the depths of the sound speed profile nodes are also considered inversion parameters, however, their number must be smaller than in the first approach due to the computational complexity limitation. Several large-scale computational experiments reveal that for the considered problem the second approach leads to a more accurate sound speed profile estimation. This study was supported by the Council for Grants of the President of the Russian Federation (grant No. MK-2262.2017.5), the Russian Foundation for Basic research (grants No. 16-05-01074-a, No. 16-07-00155-a), and the POI FEB RAS Program 'Nonlinear dynamical processes in the ocean and atmosphere'.
</Content>
<field id="content">
The BOINC-based volunteer computing project Acoustics@home was employed to study the accuracy of the sound speed profile reconstruction in a shallow-water waveguide using a dispersion-based geoacoustic inversion scheme. This problem was transformed into a problem of black-box minimization of a certain mismatch function. According to the first approach, a sound speed profile is considered a piecewise-linear function with fixed uniformly-spaced nodes. At these nodes, the values of sound speed are obtained in the course of inversion. In the second approach the depths of the sound speed profile nodes are also considered inversion parameters, however, their number must be smaller than in the first approach due to the computational complexity limitation. Several large-scale computational experiments reveal that for the considered problem the second approach leads to a more accurate sound speed profile estimation. This study was supported by the Council for Grants of the President of the Russian Federation (grant No. MK-2262.2017.5), the Russian Foundation for Basic research (grants No. 16-05-01074-a, No. 16-07-00155-a), and the POI FEB RAS Program 'Nonlinear dynamical processes in the ocean and atmosphere'.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Zaikin</FamilyName>
<Email>zaikin.icc@gmail.com</Email>
<Affiliation>Matrosov Institute for System Dynamics and Control Theory SB RAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Petrov</FamilyName>
<Email>petrov@poi.dvo.ru</Email>
<Affiliation>V. I. Il'ichev Pacific Oceanological Institute FEB RAS</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ilya</FirstName>
<FamilyName>Kurochkin</FamilyName>
<Email>qurochkin@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Zaikin</FamilyName>
<Email>zaikin.icc@gmail.com</Email>
<Affiliation>Matrosov Institute for System Dynamics and Control Theory SB RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>284</Id>
<Title>
Контекстная графическая среда пространственной визуализации результатов вычислительных экспериментов в механике сплошных сред
</Title>
<Content>
В исследовательском проектировании, построении и последующем анализе достоверности моделируемых процессов в ресурсоемких вычислительных экспериментах, что особо востребуется при изучении нестационарных процессов в механике сплошных сред, становится весьма актуальным использование открытой и легко модифицируемой программной среды для пространственной визуализации быстротекущих физических явлений непосредственно в ходе суперкомпьютерных расчетов. Важным условием такой визуализации является минимальность воздействия на вычислительные процессы, с возможностью внешнего влияния на реологические параметры моделируемой физической среды и критерии динамической или гибридной перестройки вычислительных процессов. Практически все современные вычислительные комплексы обладают встроенными графическими средствами, обеспечивающими быструю визуализацию пространственных геометрических объектов с использованием независимых многоядерных процессоров, которые в полное мере способны обеспечивать решение сформулированной задачи для параллельной визуализации текущих результатов без существенного влияния на основные вычислительные процессы. В настоящем исследовании рассматривается вариант построения программного комплекса на базе графической среды программирования OpenGL, окружаемой инструментальными средствами для работы со временем и интервальными таймерами, устройствами ввода информации и представления текстовых данных на предельно низком уровне прямого ввода/вывода информации и обработки прерываний в OS Windows.
</Content>
<field id="content">
В исследовательском проектировании, построении и последующем анализе достоверности моделируемых процессов в ресурсоемких вычислительных экспериментах, что особо востребуется при изучении нестационарных процессов в механике сплошных сред, становится весьма актуальным использование открытой и легко модифицируемой программной среды для пространственной визуализации быстротекущих физических явлений непосредственно в ходе суперкомпьютерных расчетов. Важным условием такой визуализации является минимальность воздействия на вычислительные процессы, с возможностью внешнего влияния на реологические параметры моделируемой физической среды и критерии динамической или гибридной перестройки вычислительных процессов. Практически все современные вычислительные комплексы обладают встроенными графическими средствами, обеспечивающими быструю визуализацию пространственных геометрических объектов с использованием независимых многоядерных процессоров, которые в полное мере способны обеспечивать решение сформулированной задачи для параллельной визуализации текущих результатов без существенного влияния на основные вычислительные процессы. В настоящем исследовании рассматривается вариант построения программного комплекса на базе графической среды программирования OpenGL, окружаемой инструментальными средствами для работы со временем и интервальными таймерами, устройствами ввода информации и представления текстовых данных на предельно низком уровне прямого ввода/вывода информации и обработки прерываний в OS Windows.
</field>
<field id="summary">
В реализации унифицированного объектно-ориентированного комплекса программ, названного Window-Place, представляется многооконный интерфейс Window для работы с внешними устройствами и компьютерной аппаратурой, с наложением текстовых и графических страниц Place, непосредственно поддерживающих контекстно-зависимые примитивы и операции OpenGL. Встроенные виртуальные процедуры для масштабирования наложенных страниц, обработки программных и аппаратных прерываний и др., создают интуитивно понятное и визуально естественное поведение графических площадок и текстовых страниц. При этом полиморфизм производных классов для прикладных пользовательских вычислительных объектов, допускает подмену всех базовых виртуальных функций, что может быть полезным для ускорения вычислений за счет отмены или упрощения изначально встроенных в Window-Place процедур при управлении процессами визуализации непосредственно в ходе ресурсоемкого суперкомпьютерного эксперимента.
</field>
<PrimaryAuthor>
<FirstName>Vasily</FirstName>
<FamilyName>Khramushin</FamilyName>
<Email>v.khram@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vasily</FirstName>
<FamilyName>Khramushin</FamilyName>
<Email>v.khram@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>285</Id>
<Title>
ОРГАНИЗАЦИЯ ДОСТУПА К ЭКСПЕРИМЕНТАЛЬНЫМ ДАННЫМ УСТАНОВКИ ИТЭР В РЕЖИМЕ УДАЛЕННОЙ ПУЛЬТОВОЙ
</Title>
<Content>
Тезисы доклада В настоящее время в России, на базе Проектного Центра ИТЭР, создан прототип Центра удаленного участия в экспериментах на крупных физических установках - Remote Participation Center. Цель – создание единой исследовательской среды научно-исследовательских центров, лабораторий и университетов, участвующих в исследованиях по управляемому термоядерному синтезу. Основной задачей Центра также является отработка систем управления диагностических комплексов, поставляемых Российской Федерацией в Международный проект ИТЭР. Проект ИТЭР (ITER – International Thermonuclear Experimental Reactor) в настоящее время является одним из наиболее сложных международных научно-технических мега проектов. Собственно, установка сооружается во Франции в Центре атомных исследований Кадараш. В проекте участвуют семь стран: Россия, Объединённая Европа, Китай, Индия, Корея, США, Япония. Стоимость проекта около 20 млрд. долларов (Доля России ~2 млрд. долларов). Завершение строительства (первая плазма) в 2026 году. В основу проекта заложен принцип создания высокотемпературной плазмы (150 млн. градусов) на основе установки ТОКАМАК. Предполагается, что ИТЭР будет производить порядка 10-15 Пбайт экспериментальной информации в год. В докладе представлен обзор работ по организации удаленного доступа к экспериментальным данным и дистанционному управлению диагностическим оборудованием на современных термоядерных установках, а также рассматриваются вопросы организации передачи больших потоков данных в условиях ограниченной пропускной способности линий передачи данных. Доклад представляет интерес для физиков и инженеров, работающих на крупных физических установках в области информационных технологий. Работа выполнена по Контракту с Государственной Корпорацией РОСАТОМ №Н.4а.241.9Б.17.1001. Ключевые слова: Управляемый термоядерный синтез, токамак, проект ИТЭР.
</Content>
<field id="content">
Тезисы доклада В настоящее время в России, на базе Проектного Центра ИТЭР, создан прототип Центра удаленного участия в экспериментах на крупных физических установках - Remote Participation Center. Цель – создание единой исследовательской среды научно-исследовательских центров, лабораторий и университетов, участвующих в исследованиях по управляемому термоядерному синтезу. Основной задачей Центра также является отработка систем управления диагностических комплексов, поставляемых Российской Федерацией в Международный проект ИТЭР. Проект ИТЭР (ITER – International Thermonuclear Experimental Reactor) в настоящее время является одним из наиболее сложных международных научно-технических мега проектов. Собственно, установка сооружается во Франции в Центре атомных исследований Кадараш. В проекте участвуют семь стран: Россия, Объединённая Европа, Китай, Индия, Корея, США, Япония. Стоимость проекта около 20 млрд. долларов (Доля России ~2 млрд. долларов). Завершение строительства (первая плазма) в 2026 году. В основу проекта заложен принцип создания высокотемпературной плазмы (150 млн. градусов) на основе установки ТОКАМАК. Предполагается, что ИТЭР будет производить порядка 10-15 Пбайт экспериментальной информации в год. В докладе представлен обзор работ по организации удаленного доступа к экспериментальным данным и дистанционному управлению диагностическим оборудованием на современных термоядерных установках, а также рассматриваются вопросы организации передачи больших потоков данных в условиях ограниченной пропускной способности линий передачи данных. Доклад представляет интерес для физиков и инженеров, работающих на крупных физических установках в области информационных технологий. Работа выполнена по Контракту с Государственной Корпорацией РОСАТОМ №Н.4а.241.9Б.17.1001. Ключевые слова: Управляемый термоядерный синтез, токамак, проект ИТЭР.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>i.semenov@iterrf.ru</Email>
<Affiliation>Project Center ITER</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ekaterina</FirstName>
<FamilyName>Mironova</FamilyName>
<Email>e.mironova@iterrf.ru</Email>
<Affiliation>Project Center ITER</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Portone</FamilyName>
<Email>s.portone@iterrf.ru</Email>
<Affiliation>Project Center ITER</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oleg</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>o.semenov@iterrf.ru</Email>
<Affiliation>Project Center ITER</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>i.semenov@iterrf.ru</Email>
<Affiliation>Project Center ITER</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>286</Id>
<Title>
Semantic information management: the approach to semantic assets development lifecycle
</Title>
<Content>
The application of semantic integration methods meets challenges, which arise during collaboration between IT-specialists and domain experts at the model building stage. These challenges can affect correct formalization of the domain as well as the outcome of the integration in distributed information systems as a whole. The creation of a collaborative platform for semantic integration which provides the (re)use of semantic assets (SA) is suggested to overcome the lack of semantic interoperability. The analysis of the limitations existing in standard SA management leads the authors to propose the collaborative approach, based on an extended lifecycle of semantic assets. The authors consider the implementation of the platform based on the Asset Description Metadata schema extension to be a valid option.
</Content>
<field id="content">
The application of semantic integration methods meets challenges, which arise during collaboration between IT-specialists and domain experts at the model building stage. These challenges can affect correct formalization of the domain as well as the outcome of the integration in distributed information systems as a whole. The creation of a collaborative platform for semantic integration which provides the (re)use of semantic assets (SA) is suggested to overcome the lack of semantic interoperability. The analysis of the limitations existing in standard SA management leads the authors to propose the collaborative approach, based on an extended lifecycle of semantic assets. The authors consider the implementation of the platform based on the Asset Description Metadata schema extension to be a valid option.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Akatkin</FamilyName>
<Email>uakatkin@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Michael</FirstName>
<FamilyName>Bich</FamilyName>
<Email>misha@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>287</Id>
<Title>
ANALYSIS OF THE FEATURES OF THE OPTIMAL LOGICAL STRUCTURE OF DISTRIBUTED DATABASES
</Title>
<Content>
The questions of constructing optimal logical structure of a distributed database (DDB) are considered. Solving these issues will make it possible to increase the speed of processing requests in DDB in comparison with a traditional database. Optimal logical structure of DDB will ensure the efficiency of the information system on computational resources. The problem of constructing an optimal logical structure of DDB is reduced to the problem of quadratic integer programming. As a result of its solution, the local network of the DDB is decomposed into a number of clusters that have minimal information connectivity with each other. In particular, such tasks arise for the organization of systems for processing huge amounts of information from the Large Hadron Collider. In these systems various DDBs are used to store information about: the system of triggers of data collection from physical experimental installations (ATLAS, CMS, LHCb, Alice), the geometry and the operating conditions of the detector while collecting experimental data.
</Content>
<field id="content">
The questions of constructing optimal logical structure of a distributed database (DDB) are considered. Solving these issues will make it possible to increase the speed of processing requests in DDB in comparison with a traditional database. Optimal logical structure of DDB will ensure the efficiency of the information system on computational resources. The problem of constructing an optimal logical structure of DDB is reduced to the problem of quadratic integer programming. As a result of its solution, the local network of the DDB is decomposed into a number of clusters that have minimal information connectivity with each other. In particular, such tasks arise for the organization of systems for processing huge amounts of information from the Large Hadron Collider. In these systems various DDBs are used to store information about: the system of triggers of data collection from physical experimental installations (ATLAS, CMS, LHCb, Alice), the geometry and the operating conditions of the detector while collecting experimental data.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Elena</FirstName>
<FamilyName>Nurmatova</FamilyName>
<Email>ev8@mail.ru</Email>
<Affiliation>University “Dubna”, Protvino branch</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Gusev</FamilyName>
<Email>victor@gusev.ru</Email>
<Affiliation>NRC "Kurchatov Institute" - IHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>viktor@kotliar.ru</Email>
<Affiliation>NRC "Kurchatov Institute" - IHEP)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Nurmatova</FamilyName>
<Email>ev8@mail.ru</Email>
<Affiliation>University “Dubna”, Protvino branch</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
</abstract>
<abstract>
<Id>288</Id>
<Title>
Advanced global network services to support research excelence
</Title>
<Content>
With the focus on cloud services and the benefits of using the exclusive contracts with major cloud providers made by the effort of the community, GÉANT provides unique access to the cloud resources for all its members. Based on CERN experience and data generated by LHC, especially with CMS experiment, working with physics community has been always challenging for GEANT so as the motivation to provide best services possible to accelerate top research on European and thanks to cooperation with overseas partners also at global scale. As there is a strong cooperation between physical research teams in Geneva and Dubna, GÉANT is investigating ways to optimize the connection between the research institutions and labs. This paper (and hopefully presentation) will focus on possible solutions of interconnection of the cooperating research teams in JINR and CERN in order to support their research activities. The situation in Russia was so far more complicated than in other countries due to the existence of more National Research and Education Networks (aka NRENs) providing services to different customers and they were not so keen to cooperate in order to be able to deliver optimal supportive environment to the community it deserves.
</Content>
<field id="content">
With the focus on cloud services and the benefits of using the exclusive contracts with major cloud providers made by the effort of the community, GÉANT provides unique access to the cloud resources for all its members. Based on CERN experience and data generated by LHC, especially with CMS experiment, working with physics community has been always challenging for GEANT so as the motivation to provide best services possible to accelerate top research on European and thanks to cooperation with overseas partners also at global scale. As there is a strong cooperation between physical research teams in Geneva and Dubna, GÉANT is investigating ways to optimize the connection between the research institutions and labs. This paper (and hopefully presentation) will focus on possible solutions of interconnection of the cooperating research teams in JINR and CERN in order to support their research activities. The situation in Russia was so far more complicated than in other countries due to the existence of more National Research and Education Networks (aka NRENs) providing services to different customers and they were not so keen to cooperate in order to be able to deliver optimal supportive environment to the community it deserves.
</field>
<field id="summary">
This paper (and hopefully presentation) will focus on possible solutions of interconnection of the cooperating research teams in JINR and CERN in order to support their research activities.
</field>
<PrimaryAuthor>
<FirstName>Rudolf</FirstName>
<FamilyName>Vohnout</FamilyName>
<Email>rudolf.vohnout@cesnet.cz</Email>
<Affiliation>GÉANT/CESNET</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vincenzo</FirstName>
<FamilyName>Capone</FamilyName>
<Email>vincenzo.capone@geant.org</Email>
<Affiliation>GÉANT</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Rudolf</FirstName>
<FamilyName>Vohnout</FamilyName>
<Email>rudolf.vohnout@cesnet.cz</Email>
<Affiliation>GÉANT/CESNET</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>289</Id>
<Title>Combined Explicit-Implicit Taylor Series Methods</Title>
<Content>
Hamiltonian systems arise in natural sciences and are used as mathematical models for many practical problems. Due to their wide applications, a large class of numerical methods, usually symplectic ones, has been developed. The most commonly used is Verlet method which is second order, fast, and simple to implement. Here we consider a novel idea proposed and developed in [1]. It is based on Taylor series expansion and produces a large class of methods of various orders of accuracy. The idea of the method is to combine Taylor expansions about the forward and current time levels. Construction of such methods is simple and moreover, they inherit the desired for Hamiltonian systems properties of symmetry and energy conservation. When high order of accuracy is needed, these new methods have lower computational cost than Verlet method. In some problems of Computational Dynamics, at a given stage of the process, incorporation of a large set of initial conditions or a large set of parameters is required. This motivates us to parallelize the numerical algorithms. Here we consider instruction level parallelism, namely, vectorized instructions combined with OpenMP threads. A comparison between the classical Verlet method and the Combined Taylor Series Methods on some Hamiltonian systems has been made, with main focus on the time-accuracy diagrams. The results illustrate the strengths and the weaknesses of the two different approaches. The work was financially supported by RFBR grant No. 17-01-00661-a and by a grant of the Plenipotentiary Representative of the Republic of Bulgaria at the JINR. [1] Akishin, P. G., Puzynin, I. V., Vinitsky, S. I. (1997). A hybrid numerical method for analysis of dynamics of the classical Hamiltonian systems. Computers and Mathematics with Applications, 34(2-4), 45-73
</Content>
<field id="content">
Hamiltonian systems arise in natural sciences and are used as mathematical models for many practical problems. Due to their wide applications, a large class of numerical methods, usually symplectic ones, has been developed. The most commonly used is Verlet method which is second order, fast, and simple to implement. Here we consider a novel idea proposed and developed in [1]. It is based on Taylor series expansion and produces a large class of methods of various orders of accuracy. The idea of the method is to combine Taylor expansions about the forward and current time levels. Construction of such methods is simple and moreover, they inherit the desired for Hamiltonian systems properties of symmetry and energy conservation. When high order of accuracy is needed, these new methods have lower computational cost than Verlet method. In some problems of Computational Dynamics, at a given stage of the process, incorporation of a large set of initial conditions or a large set of parameters is required. This motivates us to parallelize the numerical algorithms. Here we consider instruction level parallelism, namely, vectorized instructions combined with OpenMP threads. A comparison between the classical Verlet method and the Combined Taylor Series Methods on some Hamiltonian systems has been made, with main focus on the time-accuracy diagrams. The results illustrate the strengths and the weaknesses of the two different approaches. The work was financially supported by RFBR grant No. 17-01-00661-a and by a grant of the Plenipotentiary Representative of the Republic of Bulgaria at the JINR. [1] Akishin, P. G., Puzynin, I. V., Vinitsky, S. I. (1997). A hybrid numerical method for analysis of dynamics of the classical Hamiltonian systems. Computers and Mathematics with Applications, 34(2-4), 45-73
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Hristov</FamilyName>
<Email>ivanh@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Zafar</FirstName>
<FamilyName>Tukhliev</FamilyName>
<Email>zafar@jinr.ru</Email>
<Affiliation>JOINT INSTITUTE FOR NUCLEAR RESEARCH</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Stefka</FirstName>
<FamilyName>Dimova</FamilyName>
<Email>dimova@fmi.uni-sofia.bg</Email>
<Affiliation>FMI, Sofia University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Radoslava</FirstName>
<FamilyName>Hristova</FamilyName>
<Email>radoslava@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>I</FirstName>
<FamilyName>Puzynin</FamilyName>
<Email>i@puzynin.ru</Email>
<Affiliation>LIT, JINR, Dubna, Russia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>T</FirstName>
<FamilyName>Puzynina</FamilyName>
<Email>t@puzynina.ru</Email>
<Affiliation>LIT, JINR, Dubna, Russia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>N</FirstName>
<FamilyName>Shegunov</FamilyName>
<Email>n@shegunov.ru</Email>
<Affiliation>University of Sofia, Bulgaria</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Zarif</FirstName>
<FamilyName>Sharipov</FamilyName>
<Email>zarif@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<ContributionType>Poster presentations</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>290</Id>
<Title>
Time Series and Data Analysis Based on Hybrid models of Deep Neural Networks and Neuro-Fuzzy Networks
</Title>
<Content>
In this paper we consider approach to data analysis and time series forecasting based on hybrid models. This models contains a Deep NN models and Neuro-Fuzzy networks. We are show an overview of new approaches for data science field - time series and data analysis. Also, we propose our models of DL and Neuro-Fuzzy Networks for this task. Finally we show possibility of using this models for data science tasks.
</Content>
<field id="content">
In this paper we consider approach to data analysis and time series forecasting based on hybrid models. This models contains a Deep NN models and Neuro-Fuzzy networks. We are show an overview of new approaches for data science field - time series and data analysis. Also, we propose our models of DL and Neuro-Fuzzy Networks for this task. Finally we show possibility of using this models for data science tasks.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Yarushev</FamilyName>
<Email>sergey.yarushev@icloud.com</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Averkin</FamilyName>
<Email>averkin2003@inbox.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Yarushev</FamilyName>
<Email>sergey.yarushev@icloud.com</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Averkin</FamilyName>
<Email>averkin2003@inbox.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>291</Id>
<Title>COMPASS Production System: Processing on HPC</Title>
<Content>
Since the fall of 2017 COMPASS processes data on heterogeneous computing environment, which includes computing resources at CERN and JINR. Computing sites of the infrastructure work under management of workload management system called PanDA (Production and Distributed Analysis System). At the end of December 2017, integration of BlueWaters HPC to run COMPASS production jobs has begun. Despite an ordinary computing site, each HPC has many specific features, which make it unique, such as: hardware, batch system type, job submission and user policies, et cetera. That is why there is no ready solution out of the box for any HPC, development and adaptation is needed in each particular case. PanDA Pilot has a version for processing on HPCs, called Multi-Job Pilot, which was prepared to run simulation jobs for ATLAS. To run COMPASS production jobs, an extension of Multi-Job Pilot was performed. Details of the new computing resource integration into COMPASS Production System are described in the report.
</Content>
<field id="content">
Since the fall of 2017 COMPASS processes data on heterogeneous computing environment, which includes computing resources at CERN and JINR. Computing sites of the infrastructure work under management of workload management system called PanDA (Production and Distributed Analysis System). At the end of December 2017, integration of BlueWaters HPC to run COMPASS production jobs has begun. Despite an ordinary computing site, each HPC has many specific features, which make it unique, such as: hardware, batch system type, job submission and user policies, et cetera. That is why there is no ready solution out of the box for any HPC, development and adaptation is needed in each particular case. PanDA Pilot has a version for processing on HPCs, called Multi-Job Pilot, which was prepared to run simulation jobs for ATLAS. To run COMPASS production jobs, an extension of Multi-Job Pilot was performed. Details of the new computing resource integration into COMPASS Production System are described in the report.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>292</Id>
<Title>INP BSU grid site</Title>
<Content>
The main goal of INP BSU grid site is to provide access for scientists and students from our institute to computational power of WLCG and make contribution to data processing. We are involved in the two greatest modern experiments in particle physics – CMS and ATLAS at the Large Hadron Collider at CERN. INP BSU grid site is the only certified and production EGI grid site in Belarus. Also we integrated our cloud resources with JINR cloud. An overview of INP BSU computational facilities usage and development is presented.
</Content>
<field id="content">
The main goal of INP BSU grid site is to provide access for scientists and students from our institute to computational power of WLCG and make contribution to data processing. We are involved in the two greatest modern experiments in particle physics – CMS and ATLAS at the Large Hadron Collider at CERN. INP BSU grid site is the only certified and production EGI grid site in Belarus. Also we integrated our cloud resources with JINR cloud. An overview of INP BSU computational facilities usage and development is presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Mossolov</FamilyName>
<Email>mos@hep.by</Email>
<Affiliation>Institute for Nuclear Problems, Belarusian State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Yermak</FamilyName>
<Email>dmierk@hep.by</Email>
<Affiliation>Institute for Nuclear Problems of Belarusian State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vitaly</FirstName>
<FamilyName>Yermolchyk</FamilyName>
<Email>yermolchyk@hep.by</Email>
<Affiliation>INP BSU</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vitaly</FirstName>
<FamilyName>Yermolchyk</FamilyName>
<Email>yermolchyk@hep.by</Email>
<Affiliation>INP BSU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>293</Id>
<Title>
JINR Multifunctional Information and Computing Complex: Status and Perspectives
</Title>
<Content>
JINR possesses a complex informational-computational infrastructure. The uninterrupted functioning of all its elements at the right level is mandatory for the fulfillment of the JINR scientific research programmes. The support of this infrastructure fully functional is a major task of the Laboratory of Information Technologies. Follows from the noticeable diversity of the scientific targets defined by the JINR research the Multifunctional Information and Computing Complex (MICC) was developed as distributed computing infrastructure fulfilling all the needs. The MICC should meet the requirements for a modern highly performant scientific computing complex: multi-functionality, high performance, task adapted data storage system, high reliability and availability, information security, scalability, customized software environment for different existing user groups, high performance telecommunications and modern local network. Dedicated MICC components are: the CMS Tier1 grid site; JINR Tier2 grid site providing support to the virtual organizations (VOs) concerning the JINR participation in the LHC experiments (ATLAS, ALICE, CMS, LHCb), other VOs within large-scale international collaborations with the JINR groups and, traditionally, the sequential computing tasks of non-grid JINR; cloud computing structure aimed at expanding the range of services provided to the users and at creating an integrated cloud environment of the JINR Member States; high performance heterogeneous computing platform HybriLIT, the main part of which is supercomputer “Govorun”. A brief status overview of each component is presented. Particular attention is given to the development of distributed computations performed in collaboration with CERN, BNL, FNAL, FAIR, China, and JINR Member States. We present our plans to further develop MICC as a center for scientific computing within the multidisciplinary research environment of JINR and JINR Member States, and particularly for megascience projects, such as NICA.
</Content>
<field id="content">
JINR possesses a complex informational-computational infrastructure. The uninterrupted functioning of all its elements at the right level is mandatory for the fulfillment of the JINR scientific research programmes. The support of this infrastructure fully functional is a major task of the Laboratory of Information Technologies. Follows from the noticeable diversity of the scientific targets defined by the JINR research the Multifunctional Information and Computing Complex (MICC) was developed as distributed computing infrastructure fulfilling all the needs. The MICC should meet the requirements for a modern highly performant scientific computing complex: multi-functionality, high performance, task adapted data storage system, high reliability and availability, information security, scalability, customized software environment for different existing user groups, high performance telecommunications and modern local network. Dedicated MICC components are: the CMS Tier1 grid site; JINR Tier2 grid site providing support to the virtual organizations (VOs) concerning the JINR participation in the LHC experiments (ATLAS, ALICE, CMS, LHCb), other VOs within large-scale international collaborations with the JINR groups and, traditionally, the sequential computing tasks of non-grid JINR; cloud computing structure aimed at expanding the range of services provided to the users and at creating an integrated cloud environment of the JINR Member States; high performance heterogeneous computing platform HybriLIT, the main part of which is supercomputer “Govorun”. A brief status overview of each component is presented. Particular attention is given to the development of distributed computations performed in collaboration with CERN, BNL, FNAL, FAIR, China, and JINR Member States. We present our plans to further develop MICC as a center for scientific computing within the multidisciplinary research environment of JINR and JINR Member States, and particularly for megascience projects, such as NICA.
</field>
<field id="summary">V. Trofimov</field>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Dolbilov</FamilyName>
<Email>dolbilov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Podgainy</FamilyName>
<Email>podgainy@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oksana</FirstName>
<FamilyName>Streltsova</FamilyName>
<Email>strel@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Trofimov</FamilyName>
<Email>tvv@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<Speaker>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>294</Id>
<Title>
Govorun supercomputer engineering infrastructure . Monitoring system of engineering infrastructure.
</Title>
<Content>
A complex engineering infrastructure has been developed to support Govorun supercomputer that is expansion of the HybriLIT heterogeneous cluster. This infrastructure combines integration of two solutions on cooling systems: air cooling system for the GPU-component and water cooling system for the CPU-component based on the solution of the RSC Group. The report provides a review of the engineering infrastructure of the supercomputer, and it is important to note that a special emphasis is put on the water cooling system. Review on the monitoring system based on the solution of the "RSC BazIS" which allows managing both separate nodes and all nodes of the infrastructure component will be also presented in the report.
</Content>
<field id="content">
A complex engineering infrastructure has been developed to support Govorun supercomputer that is expansion of the HybriLIT heterogeneous cluster. This infrastructure combines integration of two solutions on cooling systems: air cooling system for the GPU-component and water cooling system for the CPU-component based on the solution of the RSC Group. The report provides a review of the engineering infrastructure of the supercomputer, and it is important to note that a special emphasis is put on the water cooling system. Review on the monitoring system based on the solution of the "RSC BazIS" which allows managing both separate nodes and all nodes of the infrastructure component will be also presented in the report.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Vorontsov</FamilyName>
<Email>vorontsov@jinr.ru</Email>
<Affiliation>LIT, JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Dolbilov</FamilyName>
<Email>dolbilov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Podgainy</FamilyName>
<Email>podgainy@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Oksana</FirstName>
<FamilyName>Streltsova</FamilyName>
<Email>strel@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Vorontsov</FamilyName>
<Email>vorontsov@jinr.ru</Email>
<Affiliation>LIT, JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>295</Id>
<Title>
Development of JINR Tier-1 service monitoring system
</Title>
<Content>
Tier-1 center for CMS in JINR has been successfully operating since 2015. Monitoring is an important aspect of ensuring its performance. Hardware monitoring of the Tier-1 center had been introduced at construction time and was constantly upgraded with the center. The scientific community makes use of the resources through Grid services that depend on more low-level services. A dedicated monitoring system has been developed to keep an eye on the state of all services related to Tier-1 operations. The main object of the monitoring system is to collect data from different sources, process it and provide a comprehensive overview on a web page. The mechanism was implemented to allow determining status by analyzing collected data. The notion of event was introduced to allow reactions on ongoing changes of all services. The whole system consists of core libraries and monitoring modules. A monitoring module may unite functionality related to data collection, analysis, visualization, possible statuses and events, and reactions on events. This allows building flexible monitoring modules which together form a Tier-1 service monitoring system.
</Content>
<field id="content">
Tier-1 center for CMS in JINR has been successfully operating since 2015. Monitoring is an important aspect of ensuring its performance. Hardware monitoring of the Tier-1 center had been introduced at construction time and was constantly upgraded with the center. The scientific community makes use of the resources through Grid services that depend on more low-level services. A dedicated monitoring system has been developed to keep an eye on the state of all services related to Tier-1 operations. The main object of the monitoring system is to collect data from different sources, process it and provide a comprehensive overview on a web page. The mechanism was implemented to allow determining status by analyzing collected data. The notion of event was introduced to allow reactions on ongoing changes of all services. The whole system consists of core libraries and monitoring modules. A monitoring module may unite functionality related to data collection, analysis, visualization, possible statuses and events, and reactions on events. This allows building flexible monitoring modules which together form a Tier-1 service monitoring system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Mitsyn</FamilyName>
<Email>vvm@mammoth.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Strizh</FamilyName>
<Email>strizh@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>296</Id>
<Title>NRC "KI" participation in DataLake project</Title>
<Content>
WLCG DataLake RandD project aims at exploring an evolution of distributed storage while bearing in mind very high demands of HL-LHC era. Its primary objective is to optimize hardware usage and operational costs of a storage system deployed across distributed centers connected by fat networks and operated as a single service. Such storage would host a large fraction of the WLCG data and optimize the cost, eliminating inefficiencies due to fragmentation. In this talk we will explain NRC "KI" role in the DataLake project with highlight on our goals, achievements and future plans.
</Content>
<field id="content">
WLCG DataLake RandD project aims at exploring an evolution of distributed storage while bearing in mind very high demands of HL-LHC era. Its primary objective is to optimize hardware usage and operational costs of a storage system deployed across distributed centers connected by fat networks and operated as a single service. Such storage would host a large fraction of the WLCG data and optimize the cost, eliminating inefficiencies due to fragmentation. In this talk we will explain NRC "KI" role in the DataLake project with highlight on our goals, achievements and future plans.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Zarochentsev</FamilyName>
<Email>andrey.zar@gmail.com</Email>
<Affiliation>SPbSU</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>297</Id>
<Title>File Transfer Service at Exabyte scale</Title>
<Content>
The File Transfer Service (FTS) is an open source solution for data transfers developed at CERN. It has been used for almost 4 years for the CERN LHC experiments data distribution in the WLCG infrastructure and during this period the usage has been extended to non-CERN and non-HEP users, reaching in 2017 almost an Exabyte of transferred data volume. The talk will focus on the service architecture and main features, like the transfer optimizer, the multi protocol support, cloud extensions and monitoring. The ongoing and future activities around FTS are also going to be presented, like the integration with OpenID Connect and CDMI.
</Content>
<field id="content">
The File Transfer Service (FTS) is an open source solution for data transfers developed at CERN. It has been used for almost 4 years for the CERN LHC experiments data distribution in the WLCG infrastructure and during this period the usage has been extended to non-CERN and non-HEP users, reaching in 2017 almost an Exabyte of transferred data volume. The talk will focus on the service architecture and main features, like the transfer optimizer, the multi protocol support, cloud extensions and monitoring. The ongoing and future activities around FTS are also going to be presented, like the integration with OpenID Connect and CDMI.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>andrea</FirstName>
<FamilyName>Manzi</FamilyName>
<Email>andrea.manzi@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>andrea</FirstName>
<FamilyName>Manzi</FamilyName>
<Email>andrea.manzi@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>298</Id>
<Title>
THE SERVICE FOR PARALLEL APPLICATIONS BASED ON THE JINR CLOUD AND HYBRILIT RESOURCES
</Title>
<Content>
Cloud computing became a routine tool for scientists in many domains. The JINR cloud infrastructure provides JINR users computational resources for performing various scientific calculations. In order to speed up achievements of scientific results the JINR cloud service for parallel applications was developed. It is a application-specific web-interface. It consists of several components and implements a flexible and modular architecture which allows to utilize both more applications and various types of resources, as computational backends. Besides this architecture increases the utilization of cloud idle resources. This service allows scientist to focus on his research domain by interacting with the service in a convenient way via browser and abstracting away from underlying infrastructure as well as its maintenance. A user just set a required values for his job via web-interface and specify a location for uploading a result. The computational workload are done on the VMs deployed in the JINR cloud infrastructure. But It is planned in the nearest future to add a HybriLIT heterogeneous cluster as one more computational back-end of the JINR SaaS service. An example of using the CloudandHybriLIT resources in the scientific computing is the study of superconducting processes in the stacked long Josephson junctions (LJJ). LJJ systems are undergone the intensive research because of a perspective of practical applications in nano-electronics and quantum computing. Respective mathematical model is described by a system of the sine-Gordon type partial differential equations where the spatial derivatives are approximated with help of standard finite difference formulas and the resulting system of ODEs is numerically solved by means of the 4th order Runge-Kutta procedure. Parallel MPI-implementation of the numerical algorithm was developed.
</Content>
<field id="content">
Cloud computing became a routine tool for scientists in many domains. The JINR cloud infrastructure provides JINR users computational resources for performing various scientific calculations. In order to speed up achievements of scientific results the JINR cloud service for parallel applications was developed. It is a application-specific web-interface. It consists of several components and implements a flexible and modular architecture which allows to utilize both more applications and various types of resources, as computational backends. Besides this architecture increases the utilization of cloud idle resources. This service allows scientist to focus on his research domain by interacting with the service in a convenient way via browser and abstracting away from underlying infrastructure as well as its maintenance. A user just set a required values for his job via web-interface and specify a location for uploading a result. The computational workload are done on the VMs deployed in the JINR cloud infrastructure. But It is planned in the nearest future to add a HybriLIT heterogeneous cluster as one more computational back-end of the JINR SaaS service. An example of using the CloudandHybriLIT resources in the scientific computing is the study of superconducting processes in the stacked long Josephson junctions (LJJ). LJJ systems are undergone the intensive research because of a perspective of practical applications in nano-electronics and quantum computing. Respective mathematical model is described by a system of the sine-Gordon type partial differential equations where the spatial derivatives are approximated with help of standard finite difference formulas and the resulting system of ODEs is numerically solved by means of the 4th order Runge-Kutta procedure. Parallel MPI-implementation of the numerical algorithm was developed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Elena</FirstName>
<FamilyName>Zemlyanaya</FamilyName>
<Email>elena@jinr.ru</Email>
<Affiliation>leading researcher</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Pavel</FirstName>
<FamilyName>Goncharov</FamilyName>
<Email>kaliostrogoblin3@gmail.com</Email>
<Affiliation>Sukhoi State Technical University of Gomel, Gomel, Belarus</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Nechaevskiy</FamilyName>
<Email>nechav@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ruslan</FirstName>
<FamilyName>Kuchumov</FamilyName>
<Email>kuchumovri@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oksana</FirstName>
<FamilyName>Streltsova</FamilyName>
<Email>strel@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Maksim</FirstName>
<FamilyName>Bashashin</FamilyName>
<Email>mbashashin@jinr.ru</Email>
<Affiliation>Laboratory of Information Technologies, JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Gennady</FirstName>
<FamilyName>Ososkov</FamilyName>
<Email>ososkov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Sokolov</FamilyName>
<Email>isokolov@jinr.ru</Email>
<Affiliation>Alexandrovich</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Sokolov</FamilyName>
<Email>isokolov@jinr.ru</Email>
<Affiliation>Alexandrovich</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>299</Id>
<Title>
Texture generation for archaeological reconstructions
</Title>
<Content>
The paper describes a solution that reconstructs the texture in 3D models of archeological monuments and performs their visualization. The software we have developed allows to model the outward surface of objects in various states of preservation. Drawings and photographs of preserved wall fragments and stonework elements are used in the modelling process. Our work resulted in development of a texturing system that reconstructs textures of a given object based on photographs and fragments of drawings. The major distinguishing feature of the system is that it can reconstruct textures using limited and low-quality input data. For instance, the input data fed to the system may consist of photographs of an object taken with an ordinary camera (e.g., with a smartphone). In developing the system, we used OpenCV, CGAL and AwesomeBump open source computer vision packages.
</Content>
<field id="content">
The paper describes a solution that reconstructs the texture in 3D models of archeological monuments and performs their visualization. The software we have developed allows to model the outward surface of objects in various states of preservation. Drawings and photographs of preserved wall fragments and stonework elements are used in the modelling process. Our work resulted in development of a texturing system that reconstructs textures of a given object based on photographs and fragments of drawings. The major distinguishing feature of the system is that it can reconstruct textures using limited and low-quality input data. For instance, the input data fed to the system may consist of photographs of an object taken with an ordinary camera (e.g., with a smartphone). In developing the system, we used OpenCV, CGAL and AwesomeBump open source computer vision packages.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anna</FirstName>
<FamilyName>Fatkina</FamilyName>
<Email>fatkina.a.i@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Selivanov</FamilyName>
<Email>dmitry@batbox.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Selivanov</FamilyName>
<Email>dmitry@batbox.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>300</Id>
<Title>
GPGPU implementation of Schrödinger's Smoke for Unity3D
</Title>
<Content>
The paper describes an algorithm for Eulerian simulation of incompressible fluids—Schrödinger's Smoke. The algorithm is based on representing models as a system of particles. Each particle represents a small portion of a fluid or amorphous material. A particle has a certain ‘lifespan’, during which it may undergo various changes. The Schrödinger's Smoke solver algorithm was implemented in Unity3D environment. We used Particle System and Сompute Shader techniques to transfer the bulk of computational load relating to simulation of physical processes to GPGPU—it allowed real-time interaction with the model. The solution we developed allows to model such effects as interactions between vortex rings —i.e., their collisions and overlapping—with a high degree of physical accuracy.
</Content>
<field id="content">
The paper describes an algorithm for Eulerian simulation of incompressible fluids—Schrödinger's Smoke. The algorithm is based on representing models as a system of particles. Each particle represents a small portion of a fluid or amorphous material. A particle has a certain ‘lifespan’, during which it may undergo various changes. The Schrödinger's Smoke solver algorithm was implemented in Unity3D environment. We used Particle System and Сompute Shader techniques to transfer the bulk of computational load relating to simulation of physical processes to GPGPU—it allowed real-time interaction with the model. The solution we developed allows to model such effects as interactions between vortex rings —i.e., their collisions and overlapping—with a high degree of physical accuracy.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anastasia</FirstName>
<FamilyName>Iashnikova</FamilyName>
<Email>iashnikova@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Olga</FirstName>
<FamilyName>Sedova</FamilyName>
<Email>olya-sedova@mail.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anastasia</FirstName>
<FamilyName>Iashnikova</FamilyName>
<Email>iashnikova@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>301</Id>
<Title>
Russian-language speech recognition system based on DeepSpeech
</Title>
<Content>
The paper examines the practical issues in developing a speech-to-text system using deep neural networks. The development of a Russian-language speech recognition system based on DeepSpeech architecture is described. The Mozilla company’s open source implementation of DeepSpeech for the English language was used as a starting point. The system was trained in a containerized environment using the Docker technology. It allowed to describe the entire process of component assembly from the source code, including a number of optimization techniques for CPU and GPU. Docker also allows to easily reproduce computation optimization tests on alternative infrastructures. We examined the use of TensorFlow XLA technology that optimizes linear algebra computations in the course of neural network training. The number of nodes in the internal layers of neural network was optimized based on the word error rate (WER) obtained on a test data set, having regard to GPU memory limitations. We studied the use of probabilistic language models with various maximum lengths of word sequences and selected the model that shows the best WER. Our study resulted in a Russian-language acoustic model having been trained based on a data set comprising audio and subtitles from YouTube video clips. The language model was built based on the texts of subtitles and publicly available Russian-language corpus of Wikipedia’s popular articles. The resulting system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</Content>
<field id="content">
The paper examines the practical issues in developing a speech-to-text system using deep neural networks. The development of a Russian-language speech recognition system based on DeepSpeech architecture is described. The Mozilla company’s open source implementation of DeepSpeech for the English language was used as a starting point. The system was trained in a containerized environment using the Docker technology. It allowed to describe the entire process of component assembly from the source code, including a number of optimization techniques for CPU and GPU. Docker also allows to easily reproduce computation optimization tests on alternative infrastructures. We examined the use of TensorFlow XLA technology that optimizes linear algebra computations in the course of neural network training. The number of nodes in the internal layers of neural network was optimized based on the word error rate (WER) obtained on a test data set, having regard to GPU memory limitations. We studied the use of probabilistic language models with various maximum lengths of word sequences and selected the model that shows the best WER. Our study resulted in a Russian-language acoustic model having been trained based on a data set comprising audio and subtitles from YouTube video clips. The language model was built based on the texts of subtitles and publicly available Russian-language corpus of Wikipedia’s popular articles. The resulting system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>G.</FirstName>
<FamilyName>Fedoseev</FamilyName>
<Email>fedoseev@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anna</FirstName>
<FamilyName>Shaleva</FamilyName>
<Email>shaleva@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>O.</FirstName>
<FamilyName>Sedova</FamilyName>
<Email>olya-sedova@mail.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anna</FirstName>
<FamilyName>Shaleva</FamilyName>
<Email>shaleva@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>302</Id>
<Title>
Building corpora of transcribed speech from open access sources
</Title>
<Content>
Currently there are hardly any open access corpora of transcribed speech in Russian that can be effectively used to train those speech recognition systems that are based on deep neural networks—e.g., DeepSpeech. This paper examines the methods to automatically build massive corpora of transcribed speech from open access sources in the internet, such as radio transcripts and subtitles to video clips. Our study is focused on a method to build a speech corpus using the materials extracted from the YouTube video hosting. YouTube provides two types of subtitles: those uploaded by a video’s author and those obtained through automatic recognition by speech recognition algorithms. Both have their specifics: author subtitles may have timing inaccuracies, while automatically recognized subtitles may have recognition errors. We used the YouTube Search API to obtain the links to various Russian-language video clips with subtitles available—words from a Russian dictionary served as an input. We examined two strategies to extract audio recordings with transcripts corresponding to them: by using both types of subtitles or only those that were produced through automatic recognition. The voice activity detector algorithm was employed to automatically separate the segments. Our study resulted in creating transcribed speech corpora in Russian containing 1000 hours of audio recordings. We also assessed the quality of obtained data by using a part of it to train a Russian-language automatic speech recognition system based on DeepSpeech architecture. Upon training, the system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</Content>
<field id="content">
Currently there are hardly any open access corpora of transcribed speech in Russian that can be effectively used to train those speech recognition systems that are based on deep neural networks—e.g., DeepSpeech. This paper examines the methods to automatically build massive corpora of transcribed speech from open access sources in the internet, such as radio transcripts and subtitles to video clips. Our study is focused on a method to build a speech corpus using the materials extracted from the YouTube video hosting. YouTube provides two types of subtitles: those uploaded by a video’s author and those obtained through automatic recognition by speech recognition algorithms. Both have their specifics: author subtitles may have timing inaccuracies, while automatically recognized subtitles may have recognition errors. We used the YouTube Search API to obtain the links to various Russian-language video clips with subtitles available—words from a Russian dictionary served as an input. We examined two strategies to extract audio recordings with transcripts corresponding to them: by using both types of subtitles or only those that were produced through automatic recognition. The voice activity detector algorithm was employed to automatically separate the segments. Our study resulted in creating transcribed speech corpora in Russian containing 1000 hours of audio recordings. We also assessed the quality of obtained data by using a part of it to train a Russian-language automatic speech recognition system based on DeepSpeech architecture. Upon training, the system was tested on a data set consisting of audio recordings of Russian literature available on voxforge.com—the best WER demonstrated by the system was 18%.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Iakushkin</FamilyName>
<Email>oleg.jakushkin@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>G.</FirstName>
<FamilyName>Fedoseev</FamilyName>
<Email>fedoseev@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anna</FirstName>
<FamilyName>Shaleva</FamilyName>
<Email>shaleva@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anna</FirstName>
<FamilyName>Shaleva</FamilyName>
<Email>shaleva@grid2018.jinr.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>303</Id>
<Title>
The problem of symbolic-numeric computation of the eigenvalues and eigenfunctions of the leaky modes in a regular homogeneous open waveguide
</Title>
<Content>
In this paper the algorithm of finding eigenvalues and eigenfunctions for the leaky modes in a three-layer planar dielectric waveguide is considered. The problem on the eigenmodes of open three-layer waveguides is formulated as the Sturm-Liouville problem with the corresponding boundary and asymptotic conditions. In the case of guided and radiation modes of open waveguides, the Sturm-Liouville problem is formulated for self-adjoint second-order operators on the axis and the corresponding eigenvalues are real quantities for dielectric media. The search for eigenvalues and eigenfunctions corresponding to the leaky modes involves several difficulties: the boundary conditions for the leaky modes are not self-adjoint, so that the eigenvalues can turn out to be complex quantities. The problem of finding eigenvalues and eigenfunctions will be associated with finding the complex roots of the nonlinear dispersion equation. In the present paper, an original scheme based on the method of finding the minimum of a function of several variables is used to find the eigenvalues. The paper describes the algorithm for searching for eigenvalues, the algorithm uses both symbolic transformations and numerical calculations. Based on the developed algorithm, the dispersion relation for the slowly leaky mode of a three-layer open waveguide was calculated in the Maple computer algebra system using CUDA(R) technology to accelerate certain routines.
</Content>
<field id="content">
In this paper the algorithm of finding eigenvalues and eigenfunctions for the leaky modes in a three-layer planar dielectric waveguide is considered. The problem on the eigenmodes of open three-layer waveguides is formulated as the Sturm-Liouville problem with the corresponding boundary and asymptotic conditions. In the case of guided and radiation modes of open waveguides, the Sturm-Liouville problem is formulated for self-adjoint second-order operators on the axis and the corresponding eigenvalues are real quantities for dielectric media. The search for eigenvalues and eigenfunctions corresponding to the leaky modes involves several difficulties: the boundary conditions for the leaky modes are not self-adjoint, so that the eigenvalues can turn out to be complex quantities. The problem of finding eigenvalues and eigenfunctions will be associated with finding the complex roots of the nonlinear dispersion equation. In the present paper, an original scheme based on the method of finding the minimum of a function of several variables is used to find the eigenvalues. The paper describes the algorithm for searching for eigenvalues, the algorithm uses both symbolic transformations and numerical calculations. Based on the developed algorithm, the dispersion relation for the slowly leaky mode of a three-layer open waveguide was calculated in the Maple computer algebra system using CUDA(R) technology to accelerate certain routines.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Drevitskiy</FamilyName>
<Email>adrevitskiy@gmail.com</Email>
<Affiliation>Peoples’ Friendship University of Russia</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Divakov</FamilyName>
<Email>dmitriy.divakov@gmail.com</Email>
<Affiliation>Peoples' Friendship University of Russia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Drevitskiy</FamilyName>
<Email>adrevitskiy@gmail.com</Email>
<Affiliation>Peoples’ Friendship University of Russia</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>304</Id>
<Title>Cloud Meta-Scheduler for Dynamic VM Reallocation</Title>
<Content>
Clouds gave us a more flexible way of sharing computing resources between users and combine computation-intensive workloads with other types of workloads. Due to the variety of workloads in such environments and to their dynamic nature, the hosts are often underloaded. In this talk we give a review of an approach to improve hardware utilization in IaaS clouds through dynamic reallocation of VMs (enabled by live-migration technology) and overcommitment. The software framework presented would allow one to use it as a meta-scheduler with the built-in simple algorithms for optimizing cloud workloads distribution or to implement custom schemes of dynamic reallocation and consolidation of virtual machines.
</Content>
<field id="content">
Clouds gave us a more flexible way of sharing computing resources between users and combine computation-intensive workloads with other types of workloads. Due to the variety of workloads in such environments and to their dynamic nature, the hosts are often underloaded. In this talk we give a review of an approach to improve hardware utilization in IaaS clouds through dynamic reallocation of VMs (enabled by live-migration technology) and overcommitment. The software framework presented would allow one to use it as a meta-scheduler with the built-in simple algorithms for optimizing cloud workloads distribution or to implement custom schemes of dynamic reallocation and consolidation of virtual machines.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>305</Id>
<Title>
Creation of cloud infrastructure of INP'S Astana branch - private establishment «NULITS» and its integration with the distributed JINR cloud infrastructure
</Title>
<Content>
The article is devoted to the project of creating the cloud infrastructure of the Astana branch of the Institute of Nuclear Physics and the private establishment «Nazarbayev University Library and IT Services» (Republic of Kazakhstan, Astana) on the basis of the resources of both organizations, its integration with the distributed cloud infrastructure of the Joint Institute for Nuclear Research (Russian Federation, Dubna). Investigates the motivation and implementation of the cloud infrastructure, discusses various mechanisms for cloud integration, and outlines plans for using created infrastructure.
</Content>
<field id="content">
The article is devoted to the project of creating the cloud infrastructure of the Astana branch of the Institute of Nuclear Physics and the private establishment «Nazarbayev University Library and IT Services» (Republic of Kazakhstan, Astana) on the basis of the resources of both organizations, its integration with the distributed cloud infrastructure of the Joint Institute for Nuclear Research (Russian Federation, Dubna). Investigates the motivation and implementation of the cloud infrastructure, discusses various mechanisms for cloud integration, and outlines plans for using created infrastructure.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Mazhitov</FamilyName>
<Email>mmazhitov@nu.edu.kz</Email>
<Affiliation>
Private establishment «Nazarbayev University Library and IT Services»
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Boris</FirstName>
<FamilyName>Potapchuk</FamilyName>
<Email>bpotapchuk@nu.edu.kz</Email>
<Affiliation>Private establishment «Nazarbayev University Library and IT Services»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>sbelov@nu.edu.kz</Email>
<Affiliation>Private establishment «Nazarbayev University Library and IT Services»</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yelena</FirstName>
<FamilyName>Mazhitova</FamilyName>
<Email>emazhitova@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Mazhitov</FamilyName>
<Email>mmazhitov@nu.edu.kz</Email>
<Affiliation>Private establishment «Nazarbayev University Library and IT Services»</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>306</Id>
<Title>
Numerical solution of diffraction problem on the joint of two open three-layered waveguides
</Title>
<Content>
This paper describes the algorithm for the numerical solution of the diffraction problem of waveguide modes at the joint point of two open planar waveguides. For the planar structures under consideration, we can formulate a scalar diffraction problem, which is a boundary value problem for the Helmholtz equation with a variable coefficient in two-dimensional space. The problem on the eigenmodes of an open three-layered waveguide is the Sturm-Liouville problem for a second-order operator with piecewise constant potential on the axis, where the potential is proportional to the refractive index. The described problem is singular and has a mixed spectrum: the discrete part of the spectrum corresponds to the guided waveguide modes, the continuous part of the spectrum to the radiative modes. The presence of a continuous part of the spectrum complicates the numerical solution of the diffraction problem, since the eigenfunctions from the region of the continuous spectrum do not integrate on the axis, and therefore Galerkin's method can not be used in this definition. One of the ways to adapt the Galerkin method for the problem solution is to limit artificially the area, which is equivalent to placing the open waveguide in question in a hollow closed waveguide whose boundaries are distanced from the real boundaries of the waveguide layer of the open waveguide. As a result of the described approach, we obtain a diffraction problem on a finite interval and with a discrete spectrum, which can be solved by the projection method. The described method is realized in the Maple computer algebra system using CUDA(R) technology to accelerate certain routines.
</Content>
<field id="content">
This paper describes the algorithm for the numerical solution of the diffraction problem of waveguide modes at the joint point of two open planar waveguides. For the planar structures under consideration, we can formulate a scalar diffraction problem, which is a boundary value problem for the Helmholtz equation with a variable coefficient in two-dimensional space. The problem on the eigenmodes of an open three-layered waveguide is the Sturm-Liouville problem for a second-order operator with piecewise constant potential on the axis, where the potential is proportional to the refractive index. The described problem is singular and has a mixed spectrum: the discrete part of the spectrum corresponds to the guided waveguide modes, the continuous part of the spectrum to the radiative modes. The presence of a continuous part of the spectrum complicates the numerical solution of the diffraction problem, since the eigenfunctions from the region of the continuous spectrum do not integrate on the axis, and therefore Galerkin's method can not be used in this definition. One of the ways to adapt the Galerkin method for the problem solution is to limit artificially the area, which is equivalent to placing the open waveguide in question in a hollow closed waveguide whose boundaries are distanced from the real boundaries of the waveguide layer of the open waveguide. As a result of the described approach, we obtain a diffraction problem on a finite interval and with a discrete spectrum, which can be solved by the projection method. The described method is realized in the Maple computer algebra system using CUDA(R) technology to accelerate certain routines.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Veniamin</FirstName>
<FamilyName>Chupritskiy</FamilyName>
<Email>vkchupr@gmail.com</Email>
<Affiliation>Konstantinovich</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Divakov</FamilyName>
<Email>dmitriy.divakov@gmail.com</Email>
<Affiliation>Peoples' Friendship University of Russia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Veniamin</FirstName>
<FamilyName>Chupritskiy</FamilyName>
<Email>vkchupr@gmail.com</Email>
<Affiliation>Konstantinovich</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
</abstract>
<abstract>
<Id>307</Id>
<Title>
Integration of the BOINC system and additional software packages
</Title>
<Content>
Currently, the BOINC system [1] is the most well-known voluntary computing system. Many researchers use BOINC to solve scientific problems. The BOINC software allows them to automate the process of sending tasks to the computing node, starting it and returning the results. To solve many scientific problems, the BOINC system requires additional software. The need for integration of the BOINC system and additional software arises in the following cases: a) generation of computing tasks on the server side; b) processing of results on the server side; c) running additional software components on the side of the compute node; d) interface for visualization and display of results. The report considers various aspects of the integration of the BOINC system and additional software. Describes the approaches used for integration. Projects USPEX@HOME [2] and XANSONS4COD@HOME [3] are considered too. The practice of applying standard approaches is discussed and new ideas are proposed. References 1. D. P. Anderson "BOINC: A system for public-resource computing and storage. In Grid Computing", Proceedings. Fifth IEEE/ACM International Workshop, 4--10 (IEEE, November 2004). 2. Nikolay P. Khrapov, Valery V. Rozen, Artem I. Samtsevich, Mikhail A. Posypkin, Vladimir A. Sukhomlin, Artem R. Oganov. Using virtualization to protect the proprietary material science applications in volunteer computing. Open Eng. 2018, v.8, pp. 57-60. 3. Vladislav S. Neverov, Nikolay P. Khrapov. “XANSONS for COD”: a new small BOINC project in crystallography. Open Eng. 2018, v.8, pp. 102-108.
</Content>
<field id="content">
Currently, the BOINC system [1] is the most well-known voluntary computing system. Many researchers use BOINC to solve scientific problems. The BOINC software allows them to automate the process of sending tasks to the computing node, starting it and returning the results. To solve many scientific problems, the BOINC system requires additional software. The need for integration of the BOINC system and additional software arises in the following cases: a) generation of computing tasks on the server side; b) processing of results on the server side; c) running additional software components on the side of the compute node; d) interface for visualization and display of results. The report considers various aspects of the integration of the BOINC system and additional software. Describes the approaches used for integration. Projects USPEX@HOME [2] and XANSONS4COD@HOME [3] are considered too. The practice of applying standard approaches is discussed and new ideas are proposed. References 1. D. P. Anderson "BOINC: A system for public-resource computing and storage. In Grid Computing", Proceedings. Fifth IEEE/ACM International Workshop, 4--10 (IEEE, November 2004). 2. Nikolay P. Khrapov, Valery V. Rozen, Artem I. Samtsevich, Mikhail A. Posypkin, Vladimir A. Sukhomlin, Artem R. Oganov. Using virtualization to protect the proprietary material science applications in volunteer computing. Open Eng. 2018, v.8, pp. 57-60. 3. Vladislav S. Neverov, Nikolay P. Khrapov. “XANSONS for COD”: a new small BOINC project in crystallography. Open Eng. 2018, v.8, pp. 102-108.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Khrapov</FamilyName>
<Email>nkhrapov@gmail.com</Email>
<Affiliation>Institute for Information Transmission Problems</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nikolay</FirstName>
<FamilyName>Khrapov</FamilyName>
<Email>nkhrapov@gmail.com</Email>
<Affiliation>Institute for Information Transmission Problems</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>308</Id>
<Title>
Многомерный анализ данных о продажах на основе технологии OLAP
</Title>
<Content>
В работе исследованы вопросы многомерного анализа данных на основе технологии семейства Business Intelligence и применение этой технологии для анализа продаж. Изучены OLAP технологии и требования к ним, способы реализации, на примере Business Intelligence. Рассмотрены основные положения технологии бизнес интеллекта в Visual Studio, внутренние интерфейсы Microsoft SQL Server. Разработана система управления базами данных для многомерного анализа данных в сфере продаж розничной сети бытовой и электронной техники, увеличивающая эффективность работы менеджеров, аналитиков компании.
</Content>
<field id="content">
В работе исследованы вопросы многомерного анализа данных на основе технологии семейства Business Intelligence и применение этой технологии для анализа продаж. Изучены OLAP технологии и требования к ним, способы реализации, на примере Business Intelligence. Рассмотрены основные положения технологии бизнес интеллекта в Visual Studio, внутренние интерфейсы Microsoft SQL Server. Разработана система управления базами данных для многомерного анализа данных в сфере продаж розничной сети бытовой и электронной техники, увеличивающая эффективность работы менеджеров, аналитиков компании.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Сауле</FirstName>
<FamilyName>Сарсимбаева</FamilyName>
<Email>sarsi@mail.ru</Email>
<Affiliation>Актюбинский региональный государственный университет имени К. Жубанова</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>cht@fmi.uni-sofia.bg</Email>
<Affiliation>University of Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>309</Id>
<Title>
Supporting Efficient Execution of Many-Task Applications with Everest
</Title>
<Content>
Distributed computing systems are widely used for execution of loosely coupled many-task applications. There are two important classes of such applications. Bag-of-tasks applications, e.g., parameter sweeps or Monte Carlo simulations, represent a set of independent tasks. Workflows, which are used for automation of complex computational and data processing pipelines, consist of multiple tasks with control or data dependencies. The report discusses the common problems related to the efficient execution of such applications on distributed computing resources and the relevant solutions implemented within the Everest platform. Everest [1-3] is a web-based distributed computing platform which provides users with tools to publish and share computing applications as web services. The platform also manages the execution of applications on remote computing resources. Everest implements the PaaS model by providing its functionality via remote web and REST interfaces. A single instance of the platform can be accessed by many users in order to create, run and share applications with each other. Instead of using a dedicated computing infrastructure, Everest performs the execution of applications on external resources attached by users. The platform supports integration with standalone servers, clusters, grid infrastructures, desktop grids and clouds. A user can specify multiple resources, possibly of different type, for running an application. Everest provides multiple tools for execution of many-task applications. First, it includes a general-purpose service for execution of bag-of-tasks applications such as parameter sweeps. The application tasks are described using a simple declarative notation. Second, it is possible to dynamically add new tasks or invoke other applications from a running application via the REST API. This allows users to run complex many-task applications such as workflows. In this case, the dependencies between tasks are managed internally by a user application. While this approach provides maximum flexibility, it does not allow passing the complete task graph to the platform to enable scheduling optimizations. To overcome this limitation, an new interface for submitting workflows has been added recently. The application tasks are executed by Everest on computing resources specified by a user. The efficiency of application execution, i.e. the execution time, critically depends on the methods used for task scheduling [4]. Everest implements a two-level scheduling mechanism that allows to plug-in different scheduling algorithms. First, the available resources are fairly distributed among the running applications. Then, the application-level scheduler selects tasks for running on provided resources. The separate schedulers are implemented for bags-of-tasks and workflows, which are based on MaxMin and DLS algorithms respectively. The used algorithms require the estimates of task execution and data transfer times. Currently, these estimates are computed based on the statistics from previous task and application executions. The other features that are essential for efficient execution of many-task applications include accounting for local resource policies and automatic recovery of failed tasks. For example, the limit on the maximum number of jobs per user imposed by an HPC cluster administrators may not allow to fully utilize the resource when running a single job per Everest task. An advanced adapter for Slurm manager has been developed which allows to solve this problem by submitting complex jobs consisting of multiple tasks. When dealing with failed tasks, Everest distinguishes between critical and recoverable faults. In the latter case, the task is retried multiple times, and the resources with many failures are blacklisted. To account for temporary network failures between Everest and resources, the tasks running on the disconnected resource are not rescheduled immediately to avoid wasting compute time. 1. Everest. http://everest.distcomp.org/ 2. Sukhoroslov O., Volkov S., Afanasiev A. A Web-Based Platform for Publication and Distributed Execution of Computing Applications // 14th International Symposium on Parallel and Distributed Computing (ISPDC). IEEE, 2015, pp. 175-184. 3. Sergey Smirnov, Oleg Sukhoroslov, and Sergey Volkov. Integration and Combined Use of Distributed Computing Resources with Everest // Procedia Computer Science, Volume 101, 2016, pp. 359-368. 4. Nazarenko A., Sukhoroslov O. An Experimental Study of Workflow Scheduling Algorithms for Heterogeneous Systems. In: Malyshkin V. (eds) Parallel Computing Technologies. PaCT 2017. Lecture Notes in Computer Science, vol 10421. Springer, Cham, 2017, pp. 327-341.
</Content>
<field id="content">
Distributed computing systems are widely used for execution of loosely coupled many-task applications. There are two important classes of such applications. Bag-of-tasks applications, e.g., parameter sweeps or Monte Carlo simulations, represent a set of independent tasks. Workflows, which are used for automation of complex computational and data processing pipelines, consist of multiple tasks with control or data dependencies. The report discusses the common problems related to the efficient execution of such applications on distributed computing resources and the relevant solutions implemented within the Everest platform. Everest [1-3] is a web-based distributed computing platform which provides users with tools to publish and share computing applications as web services. The platform also manages the execution of applications on remote computing resources. Everest implements the PaaS model by providing its functionality via remote web and REST interfaces. A single instance of the platform can be accessed by many users in order to create, run and share applications with each other. Instead of using a dedicated computing infrastructure, Everest performs the execution of applications on external resources attached by users. The platform supports integration with standalone servers, clusters, grid infrastructures, desktop grids and clouds. A user can specify multiple resources, possibly of different type, for running an application. Everest provides multiple tools for execution of many-task applications. First, it includes a general-purpose service for execution of bag-of-tasks applications such as parameter sweeps. The application tasks are described using a simple declarative notation. Second, it is possible to dynamically add new tasks or invoke other applications from a running application via the REST API. This allows users to run complex many-task applications such as workflows. In this case, the dependencies between tasks are managed internally by a user application. While this approach provides maximum flexibility, it does not allow passing the complete task graph to the platform to enable scheduling optimizations. To overcome this limitation, an new interface for submitting workflows has been added recently. The application tasks are executed by Everest on computing resources specified by a user. The efficiency of application execution, i.e. the execution time, critically depends on the methods used for task scheduling [4]. Everest implements a two-level scheduling mechanism that allows to plug-in different scheduling algorithms. First, the available resources are fairly distributed among the running applications. Then, the application-level scheduler selects tasks for running on provided resources. The separate schedulers are implemented for bags-of-tasks and workflows, which are based on MaxMin and DLS algorithms respectively. The used algorithms require the estimates of task execution and data transfer times. Currently, these estimates are computed based on the statistics from previous task and application executions. The other features that are essential for efficient execution of many-task applications include accounting for local resource policies and automatic recovery of failed tasks. For example, the limit on the maximum number of jobs per user imposed by an HPC cluster administrators may not allow to fully utilize the resource when running a single job per Everest task. An advanced adapter for Slurm manager has been developed which allows to solve this problem by submitting complex jobs consisting of multiple tasks. When dealing with failed tasks, Everest distinguishes between critical and recoverable faults. In the latter case, the task is retried multiple times, and the resources with many failures are blacklisted. To account for temporary network failures between Everest and resources, the tasks running on the disconnected resource are not rescheduled immediately to avoid wasting compute time. 1. Everest. http://everest.distcomp.org/ 2. Sukhoroslov O., Volkov S., Afanasiev A. A Web-Based Platform for Publication and Distributed Execution of Computing Applications // 14th International Symposium on Parallel and Distributed Computing (ISPDC). IEEE, 2015, pp. 175-184. 3. Sergey Smirnov, Oleg Sukhoroslov, and Sergey Volkov. Integration and Combined Use of Distributed Computing Resources with Everest // Procedia Computer Science, Volume 101, 2016, pp. 359-368. 4. Nazarenko A., Sukhoroslov O. An Experimental Study of Workflow Scheduling Algorithms for Heterogeneous Systems. In: Malyshkin V. (eds) Parallel Computing Technologies. PaCT 2017. Lecture Notes in Computer Science, vol 10421. Springer, Cham, 2017, pp. 327-341.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Sukhoroslov</FamilyName>
<Email>oleg.sukhoroslov@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Smirnov</FamilyName>
<Email>sasmir@gmail.com</Email>
<Affiliation>
Institute for Information Transmission Problems of the Russian Academy of Sciences
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Volkov</FamilyName>
<Email>fizteh.volkov@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Sukhoroslov</FamilyName>
<Email>oleg.sukhoroslov@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>310</Id>
<Title>New features of the JINR cloud</Title>
<Content>
The report covers details on such aspects of the JINR cloud development as migration to RAFT-based algorithm high availability setup, Ceph-based storage back-end for VM images and DIRAC-based grid platform for external partner clouds integration into distributed computational cloud environment.
</Content>
<field id="content">
The report covers details on such aspects of the JINR cloud development as migration to RAFT-based algorithm high availability setup, Ceph-based storage back-end for VM images and DIRAC-based grid platform for external partner clouds integration into distributed computational cloud environment.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yelena</FirstName>
<FamilyName>Mazhitova</FamilyName>
<Email>emazhitova@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>roman@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>311</Id>
<Title>
The ATLAS Production System Predictive Analytics service: an approach for intelligent task analysis
</Title>
<Content>
The second generation of the Production System (ProdSys2) of the ATLAS experiment (LHC, CERN), in conjunction with the workload management system - PanDA (Production and Distributed Analysis), represents a complex set of computing components that are responsible for defining, organizing, scheduling, starting and executing payloads in a distributed computing infrastructure. ProdSys2/PanDA are responsible for all stages of (re)processing, analysis and modeling of raw and derived data, as well as simulation of physical processes and functioning of the detector using Monte Carlo methods. The prototype of the ProdSys2 Predictive Analytics (P2PA) is an essential part of the growing analytical service for the ProdSys2 and will play the key role in the ATLAS computing. P2PA uses such tools as Time-To-Complete (TTC) estimation towards units of the processing (i.e., tasks, chains and groups of tasks) to control the processing state and rate, and to be able to highlight abnormal operations and executions (e.g., to discover stalled processes). It uses methods and techniques of machine learning to obtain corresponding predictive models and metrics that are aimed to characterize the current system's state and its changes over the close period of time.
</Content>
<field id="content">
The second generation of the Production System (ProdSys2) of the ATLAS experiment (LHC, CERN), in conjunction with the workload management system - PanDA (Production and Distributed Analysis), represents a complex set of computing components that are responsible for defining, organizing, scheduling, starting and executing payloads in a distributed computing infrastructure. ProdSys2/PanDA are responsible for all stages of (re)processing, analysis and modeling of raw and derived data, as well as simulation of physical processes and functioning of the detector using Monte Carlo methods. The prototype of the ProdSys2 Predictive Analytics (P2PA) is an essential part of the growing analytical service for the ProdSys2 and will play the key role in the ATLAS computing. P2PA uses such tools as Time-To-Complete (TTC) estimation towards units of the processing (i.e., tasks, chains and groups of tasks) to control the processing state and rate, and to be able to highlight abnormal operations and executions (e.g., to discover stalled processes). It uses methods and techniques of machine learning to obtain corresponding predictive models and metrics that are aimed to characterize the current system's state and its changes over the close period of time.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Borodin</FamilyName>
<Email>mborodin@cern.ch</Email>
<Affiliation>The University of Iowa (US)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Golubkov</FamilyName>
<Email>dmitry.v.golubkov@cern.ch</Email>
<Affiliation>Institute for High Energy Physics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexei</FirstName>
<FamilyName>Klimentov</FamilyName>
<Email>alexei.klimentov@cern.ch</Email>
<Affiliation>Brookhaven National Lab</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>National Research Centre «Kurchatov Institute»</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>312</Id>
<Title>
The BigPanDA self-monitoring alarm system for ATLAS
</Title>
<Content>
The BigPanDA monitoring system is a Web application created to deliver the real-time analytics, covering many aspects of the ATLAS experiment distributed computing. The system serves about 35000 requests daily and provides critical information used as input for various decisions: from distribution of the payload among available resources to issue tracking related to any of 350k jobs running simultaneously. It evolves intensively; in particular, in 2017, the system received 933 commits, delivering new features and expanding the scope of the presented data. The experience of operating BigPanDA in 24/7 mode led to development of a multilevel self-monitoring alarm system. This ELK-stack based solution covers all critical components of the BigPanda: from user authentication to management of the number of connections to the DB backend. The developed solution provides an intelligent error analysis, delivering to the operators only those notifications that need human intervention. We describe the architecture, principal features, and operation experience of self-monitoring, as well as its adaptation possibilities.
</Content>
<field id="content">
The BigPanDA monitoring system is a Web application created to deliver the real-time analytics, covering many aspects of the ATLAS experiment distributed computing. The system serves about 35000 requests daily and provides critical information used as input for various decisions: from distribution of the payload among available resources to issue tracking related to any of 350k jobs running simultaneously. It evolves intensively; in particular, in 2017, the system received 933 commits, delivering new features and expanding the scope of the presented data. The experience of operating BigPanDA in 24/7 mode led to development of a multilevel self-monitoring alarm system. This ELK-stack based solution covers all critical components of the BigPanda: from user authentication to management of the number of connections to the DB backend. The developed solution provides an intelligent error analysis, delivering to the operators only those notifications that need human intervention. We describe the architecture, principal features, and operation experience of self-monitoring, as well as its adaptation possibilities.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Tatiana</FirstName>
<FamilyName>Korchuganova</FamilyName>
<Email>tatianakorchuganova@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Siarhei</FirstName>
<FamilyName>Padolski</FamilyName>
<Email>siarhei.padolski@cern.ch</Email>
<Affiliation>Brookhaven National Laboratory</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>313</Id>
<Title>
Current status of software development for the MPD and BM@N experiments
</Title>
<Content>
The software for detector simulation, reconstruction and analysis of physics data is an essential part of each high-energy physics experiment. A new generation of the experiments for the relativistic nuclear physics is expected to be started up in the nearest years at the Nuclotron-based Ion Collider facility (NICA) being under construction at the Joint Institute for Nuclear Research in Dubna: the fixed target experiment BM@N (Baryonic Matter at Nuclotron), whose technical runs were started in 2015, and the future MPD (Multi-Purpose Detector) experiment on ion collisions, which will operate at the storage rings of the NICA facility. The event data model of the experiments is shown. The status of the software frameworks MpdRoot and BmnRoot developed for the MPD and BM@N is considered. For these tasks many additional systems, such as raw data converter, monitoring systems, event display, databases, parallelization tools and others have been developed.
</Content>
<field id="content">
The software for detector simulation, reconstruction and analysis of physics data is an essential part of each high-energy physics experiment. A new generation of the experiments for the relativistic nuclear physics is expected to be started up in the nearest years at the Nuclotron-based Ion Collider facility (NICA) being under construction at the Joint Institute for Nuclear Research in Dubna: the fixed target experiment BM@N (Baryonic Matter at Nuclotron), whose technical runs were started in 2015, and the future MPD (Multi-Purpose Detector) experiment on ion collisions, which will operate at the storage rings of the NICA facility. The event data model of the experiments is shown. The status of the software frameworks MpdRoot and BmnRoot developed for the MPD and BM@N is considered. For these tasks many additional systems, such as raw data converter, monitoring systems, event display, databases, parallelization tools and others have been developed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
</abstract>
<abstract>
<Id>314</Id>
<Title>
Tier-1 centre at NRC «Kurchatov institute» between LHC Run2 and Run3
</Title>
<Content>
The issues of development and modernization of the Tier-1 center at the National Research Center "Kurchatov Institute" are considered in accordance with the changing requirements of experiments at the Large Hadron Collider. Increasing requirements for computing resources, drived by increase in simulations, led to an increase in their volumes, which in turn required the development of automation of the processes of managing the installation and configuration of the software. To improve the quality of the software management system and minimize the risk of errors during the nodes automatic configuration a code review system was implemented. A new system for in‑demand provision of additional computing resources from supercomputing cluster was developed and implemented.
</Content>
<field id="content">
The issues of development and modernization of the Tier-1 center at the National Research Center "Kurchatov Institute" are considered in accordance with the changing requirements of experiments at the Large Hadron Collider. Increasing requirements for computing resources, drived by increase in simulations, led to an increase in their volumes, which in turn required the development of automation of the processes of managing the installation and configuration of the software. To improve the quality of the software management system and minimize the risk of errors during the nodes automatic configuration a code review system was implemented. A new system for in‑demand provision of additional computing resources from supercomputing cluster was developed and implemented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Tkachenko</FamilyName>
<Email>tia@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Eygene</FirstName>
<FamilyName>Ryabinkin</FamilyName>
<Email>rea@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Rogovskiy</FamilyName>
<Email>a.rogovsky@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ilya</FirstName>
<FamilyName>Lyalin</FamilyName>
<Email>lyalin@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Tkachenko</FamilyName>
<Email>tia@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
</abstract>
<abstract>
<Id>315</Id>
<Title>Experience with ITEP-FRRC HPC facility</Title>
<Content>
ITEP-FRRC HPC facility was built in 2011-2014 as a common project of State Atomic Energy Corporation ROSATOM and Helmholtz Association of German Research Canters. It utilizes the concept of “green computing” which was invented by GSI/FAIR scientists. Facility is used for FAIR related studies by various groups from ITEP and other Russian physics centers. After 7 years of successful HPC facility operation we want to summarize the experience we got running the hardware and supporting the requested software.
</Content>
<field id="content">
ITEP-FRRC HPC facility was built in 2011-2014 as a common project of State Atomic Energy Corporation ROSATOM and Helmholtz Association of German Research Canters. It utilizes the concept of “green computing” which was invented by GSI/FAIR scientists. Facility is used for FAIR related studies by various groups from ITEP and other Russian physics centers. After 7 years of successful HPC facility operation we want to summarize the experience we got running the hardware and supporting the requested software.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>ivan</FirstName>
<FamilyName>korolko</FamilyName>
<Email>ivan.korolko@cern.ch</Email>
<Affiliation>ITEP - NICKI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Prokudin</FamilyName>
<Email>mikhail.prokudin@cern.ch</Email>
<Affiliation>ITEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>victor</FirstName>
<FamilyName>kolosov</FamilyName>
<Email>victor.kolosov@itep.ru</Email>
<Affiliation>ITEP - NICKI</Affiliation>
</Co-Author>
<Speaker>
<FirstName>ivan</FirstName>
<FamilyName>korolko</FamilyName>
<Email>ivan.korolko@cern.ch</Email>
<Affiliation>ITEP - NICKI</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>316</Id>
<Title>Data Knowledge Base for the ATLAS collaboration</Title>
<Content>
ATLAS experiment at the CERN LHC is one of the most data-intensive modern scientific apparatus. To help managing all the experimental and modelling data, multiple information systems were created during the experiment's lifetime (more than 25 years). Each such system addresses one or several tasks of data and workload management, as well as information lookup, using specific sets of metadata (data about data). Growing data volumes and the computing infrastructure complexity require from researchers more and more complicated integration of different bits of metadata from different systems using different conditions. A common problem are multi-system join requests, which are not easy to implement in timely manner and, obviously, are less efficient than a request to a single system with integrated and pre-processed information would be. To address this issue, a joint team of researchers and developers from Kurchatov Institute and Tomsk Polytechnic University has initiated the Data Knowledge Base (DKB) RandD project in 2016. This project is aimed at knowledge acquisition and metadata integration, providing fast response for a variety of complicated requests, such as finding articles, based on same or similar data samples (search by links between objects), summary reports and monitoring tasks (aggregation requests), etc. In this report we will discuss main features and applications of the DKB prototype implemented by now, its integration with the ATLAS Workflow Management, and future perspectives of the project.
</Content>
<field id="content">
ATLAS experiment at the CERN LHC is one of the most data-intensive modern scientific apparatus. To help managing all the experimental and modelling data, multiple information systems were created during the experiment's lifetime (more than 25 years). Each such system addresses one or several tasks of data and workload management, as well as information lookup, using specific sets of metadata (data about data). Growing data volumes and the computing infrastructure complexity require from researchers more and more complicated integration of different bits of metadata from different systems using different conditions. A common problem are multi-system join requests, which are not easy to implement in timely manner and, obviously, are less efficient than a request to a single system with integrated and pre-processed information would be. To address this issue, a joint team of researchers and developers from Kurchatov Institute and Tomsk Polytechnic University has initiated the Data Knowledge Base (DKB) RandD project in 2016. This project is aimed at knowledge acquisition and metadata integration, providing fast response for a variety of complicated requests, such as finding articles, based on same or similar data samples (search by links between objects), summary reports and monitoring tasks (aggregation requests), etc. In this report we will discuss main features and applications of the DKB prototype implemented by now, its integration with the ATLAS Workflow Management, and future perspectives of the project.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Maria</FirstName>
<FamilyName>Grigorieva</FamilyName>
<Email>maria.grigorieva@cern.ch</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>317</Id>
<Title>
New methods of minimizing the errors in the software
</Title>
<Content>
One of the qualitative ways to minimize software errors is to check the code by as many users as possible (the "many eyes" principle). We propose a new approach which goes well with this technique. This is the minimization of human participation in writing program codes. The implementation of this approach has be made in such a way that the machine itself creates the program code according to the description provided by the user. Due to the re-assignment of writing the program code to the machine the process of its generation is simplified simultaneously and the number of program errors is reduced. The latter happens due to the reduction of the human factor influence. By simplifying the writing of the program code, the number of people capable of generating it increases and the period of training in programming and the time spent on writing a separate program are reduced. Our methods do not completely eliminate software errors, because they can be both in the user’s own description and in the interpreter. But, nevertheless, it is very important to maximally minimize the number of software errors, because already now the software determines many aspects of our life and the number of its applications is increasing. Even now, for example, such important areas of our life as health and finance may depend from the quality of software and the number of errors in it.
</Content>
<field id="content">
One of the qualitative ways to minimize software errors is to check the code by as many users as possible (the "many eyes" principle). We propose a new approach which goes well with this technique. This is the minimization of human participation in writing program codes. The implementation of this approach has be made in such a way that the machine itself creates the program code according to the description provided by the user. Due to the re-assignment of writing the program code to the machine the process of its generation is simplified simultaneously and the number of program errors is reduced. The latter happens due to the reduction of the human factor influence. By simplifying the writing of the program code, the number of people capable of generating it increases and the period of training in programming and the time spent on writing a separate program are reduced. Our methods do not completely eliminate software errors, because they can be both in the user’s own description and in the interpreter. But, nevertheless, it is very important to maximally minimize the number of software errors, because already now the software determines many aspects of our life and the number of its applications is increasing. Even now, for example, such important areas of our life as health and finance may depend from the quality of software and the number of errors in it.
</field>
<field id="summary">
In English There were written several programs which implement some ways to minimize errors in the “Perl” language. The author's certificates were received. Each of these programs includes a method to minimize errors. In total, these programs are three: * Dialog method. It is implemented so that the program itself asks questions to the user and selects the desired code according to his answers. This method has a significant drawback is to describe and predict all the options of the dialogue is impossible, because their number tends to infinity. * Method for determining the appropriate condition by the specified values. It is implemented with the help of “Gnuplot " charting program. It consists in the fact the program chooses which chart to build based on the given data. * A method for interpreting a condition using special words. It consists in the fact that a special simplified language or a set of words is invented, with the help of which more complex conditions are written. This simplifies the process of writing code. In Russian Написано несколько программ, в которых реализованы некоторые способы минимизации ошибок на языке “Perl”. По ним были получены авторские свидетельства. Каждая из этих программ включает в себя метод минимизации ошибок. Всего этих программ 3: Диалоговый метод. Реализован так, что программа сама задаёт вопросы пользователю и по его ответам выбирает нужный код. Имеет существенный недостаток, все варианты диалога описать и предугадать нереально, потому что их число стремится к бесконечности. Метод определения подходящего условия по заданным значениям. Реализован с помощью программы для построения графиков “Gnuplot”. Заключается в том, что программа по заданным данным сама выбирает какой график нужно строить. Метод интерпретации условия с помощью специальных слов. Заключается в том, что придумывается специальный упрощённый язык или набор слов, с помощью которого записываются более сложные условия. Это упрощает процедуру написания кода.
</field>
<PrimaryAuthor>
<FirstName>Elizaveta</FirstName>
<FamilyName>Dorenskaya</FamilyName>
<Email>dorenskaya@itep.ru</Email>
<Affiliation>ITEP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Elizaveta</FirstName>
<FamilyName>Dorenskaya</FamilyName>
<Email>dorenskaya@itep.ru</Email>
<Affiliation>ITEP</Affiliation>
</Speaker>
<ContributionType>Plenary reports</ContributionType>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>318</Id>
<Title>Event building from free streaming data at the CBM</Title>
<Content>
The CBM will be the first experiment employing a new data threating technique. All data collected from the CBM detector will be transported to computer farm. Physical objects (such as tracks and vertexes) will be reconstructed at the real time and interesting events will be stored for further detailed analysis. A unit of data in this approach is a timeslice – all data collected from the detector in a given period of time. Each timeslice contains data from many heavy ions collisions and may be threated independently from other timeslices at different nodes of computing farm. Data produced by particles originating from individual heavy ions collision (event) should be used for physical analysis rather than free streaming data. Event building can be performed at different data levels. The simplest event building technique works at the level of individual activations of readout electronics channels (digis). Each digi contains information about activation time, readout channel number, etc. Event building can be divided on two steps. The first one is an event finding and the second one is an event composition, when data corresponding to found event collected from several subdetectors. Event finding is performed using data only from a given CBM subdetector. This subdetector should be fast, has good time resolution and low noise levels. In general, event is found if the number of digis in a given time window exceeds a given threshold which depends on colliding system and interaction rate. Currently STS and BFTC considered as a data sources for event finding. General event composition method for different subdetectors has been developed and tested. The digi from a given subdetector is attributed to the event if its time after correction lies in an acceptance time window. The acceptance time window should be extended according to time resolution of the subdetector. r/c should be subtracted from the digi time for the correction, where r is a distance between triggered readout channel and c is a speed of light. This event composition method works for all subdetectors except calorimeters.
</Content>
<field id="content">
The CBM will be the first experiment employing a new data threating technique. All data collected from the CBM detector will be transported to computer farm. Physical objects (such as tracks and vertexes) will be reconstructed at the real time and interesting events will be stored for further detailed analysis. A unit of data in this approach is a timeslice – all data collected from the detector in a given period of time. Each timeslice contains data from many heavy ions collisions and may be threated independently from other timeslices at different nodes of computing farm. Data produced by particles originating from individual heavy ions collision (event) should be used for physical analysis rather than free streaming data. Event building can be performed at different data levels. The simplest event building technique works at the level of individual activations of readout electronics channels (digis). Each digi contains information about activation time, readout channel number, etc. Event building can be divided on two steps. The first one is an event finding and the second one is an event composition, when data corresponding to found event collected from several subdetectors. Event finding is performed using data only from a given CBM subdetector. This subdetector should be fast, has good time resolution and low noise levels. In general, event is found if the number of digis in a given time window exceeds a given threshold which depends on colliding system and interaction rate. Currently STS and BFTC considered as a data sources for event finding. General event composition method for different subdetectors has been developed and tested. The digi from a given subdetector is attributed to the event if its time after correction lies in an acceptance time window. The acceptance time window should be extended according to time resolution of the subdetector. r/c should be subtracted from the digi time for the correction, where r is a distance between triggered readout channel and c is a speed of light. This event composition method works for all subdetectors except calorimeters.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Prokudin</FamilyName>
<Email>mikhail.prokudin@cern.ch</Email>
<Affiliation>ITEP</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>ivan</FirstName>
<FamilyName>korolko</FamilyName>
<Email>ivan.korolko@cern.ch</Email>
<Affiliation>ITEP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Prokudin</FamilyName>
<Email>mikhail.prokudin@cern.ch</Email>
<Affiliation>ITEP</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>319</Id>
<Title>
О согласовании вычислительного эксперимента при интерактивном моделировании гидромеханики корабля в штормовом море
</Title>
<Content>
При реализации сложных прикладных вычислительных экспериментов, в проектировании, разработке и построении программных комплексов востребуется особая логика синтеза числовых структур для описания физических явлений в тесной связи с требованиями эффективного применения моделирующих операций гидромеханики на фоне непрерывной графической визуализации всех пространственных процессов. В конкретной задаче формулируются требования к декларативному представлению трехмерной геометрии формы корпуса корабля, контурные линии которого описываются непрерывными, но неоднозначными функциями. Корпус подвергается вынужденным кинематическим перемещениям без деформации, под воздействием динамически нестабильного морского волнения, подверженного непрерывной трансформации в рамках трохоидальной теории групповых структур ветровых волн и зыби, параметры которых задаются по типовым записям судовых метеопостов. Условно статическое или явное описание динамики корабля и морского волнения приводит к функциональным методам реализации вычислительных операций, моделирующих нестационарную механику взаимодействия локальных фрагментов судовой обшивки с гребнями обрушающихся морских волн, как волн теоретически предельной высоты. Проектная взаимосвязь геометрических объектов, физических явлений и нестационарных процессов гидромеханики синтезируется с помощью троичной матрицы [1]: Корабль в шторм - корпус - волнение - визуализация: 1. геометрия: теоретический чертеж - группы трохоидальных волн - 3D графика OpenGL; 2. гидростатика: остойчивость - силовое воздействие волн - несвободная динамика и качка; 3. механика взаимодействия: излучение волн корпусом корабля - трансформация волн - механика волнообразования
</Content>
<field id="content">
При реализации сложных прикладных вычислительных экспериментов, в проектировании, разработке и построении программных комплексов востребуется особая логика синтеза числовых структур для описания физических явлений в тесной связи с требованиями эффективного применения моделирующих операций гидромеханики на фоне непрерывной графической визуализации всех пространственных процессов. В конкретной задаче формулируются требования к декларативному представлению трехмерной геометрии формы корпуса корабля, контурные линии которого описываются непрерывными, но неоднозначными функциями. Корпус подвергается вынужденным кинематическим перемещениям без деформации, под воздействием динамически нестабильного морского волнения, подверженного непрерывной трансформации в рамках трохоидальной теории групповых структур ветровых волн и зыби, параметры которых задаются по типовым записям судовых метеопостов. Условно статическое или явное описание динамики корабля и морского волнения приводит к функциональным методам реализации вычислительных операций, моделирующих нестационарную механику взаимодействия локальных фрагментов судовой обшивки с гребнями обрушающихся морских волн, как волн теоретически предельной высоты. Проектная взаимосвязь геометрических объектов, физических явлений и нестационарных процессов гидромеханики синтезируется с помощью троичной матрицы [1]: Корабль в шторм - корпус - волнение - визуализация: 1. геометрия: теоретический чертеж - группы трохоидальных волн - 3D графика OpenGL; 2. гидростатика: остойчивость - силовое воздействие волн - несвободная динамика и качка; 3. механика взаимодействия: излучение волн корпусом корабля - трансформация волн - механика волнообразования
</field>
<field id="summary">
Таким образом, по строкам логической матрицы располагаются геометрические объекты и физические явления в качестве независимых «существительных» или структур данных со связанным набором алгоритмов – методов быстрого доступа к данным. По столбцам формализуются операции гидромеханики, конкретное построение которых существенно зависит от текущего состояния гидромеханики корпуса, морского волнения и их взаимодействия. При этом, в последнем столбце концентрируются результирующие материалы, предназначенные для детальной визуализации и представления морскому или корабельному инженеру-исследователю, изучающему мореходные качества или создающему проекты перспективных кораблей для конкретных географических и навигационных условий эксплуатации.
</field>
<PrimaryAuthor>
<FirstName>Vasily</FirstName>
<FamilyName>Khramushin</FamilyName>
<Email>v.khram@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Degtyarev</FamilyName>
<Email>deg@csa.ru</Email>
<Affiliation>Professor</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Gankevich</FamilyName>
<Email>i.gankevich@spbu.ru</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vasily</FirstName>
<FamilyName>Khramushin</FamilyName>
<Email>v.khram@gmail.com</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>320</Id>
<Title>
Имитационная модель БРЛК с синтезированной апертурой антенны в сети распределенных вычислений MarGrid
</Title>
<Content>
Синтезирование апертуры представляет собой технический прием, позволяющий существенно повысить разрешающую способность радиолокатора в поперечном относительно направления полета направлении и получить детальное изображение радиолокационной карты местности, над которой совершает полет ЛА [1]. Для моделирования отражения сигнала от поверхности используется фацетная модель поверхности, представляющая поверхность в виде совокупности элементарных отражающих элементов, представляющих собой пластины конечных размеров, совпадающие с поверхностью крупномасштабных неровностей [2]. Отраженный сигнал от поверхности представляет собой сумму сигналов от всех облучаемых фацетов. Причем каждый парциальный сигнал имеет свою амплитуду, определяемую ориентацией локальной диаграммы обратного рассеяния, и свою случайную фазу. Сигнал на входе приемной антенны РЛС представляет собой сумму парциальных сигналов, отраженных от всех фацетов в облучаемой области [3]. При моделировании среды распространении сигнала используется явление рефракции, которое объясняется изменением диэлектрической проницаемости и, соответственно показателя преломления воздуха с высотой. Нижние слои воздуха рассматриваются как среда, диэлектрическая проницаемость которой изменяется с высотой вследствие разрежения воздуха. При прохождении луча наблюдается его отклонение от прямолинейного распространения, вызванное наличием градиента показателя преломления. Cложность моделирования заключается в большом объеме исходных данных. Например, для несущей частоты 10ГГц (X – диапазон) плотность фацетов составит 44.4 эл./м2. Для каждого фацета всей подстилающей поверхности на каждый момент излучения антенны требуется определить: принадлежность диаграмме направленности антенны, в тени или нет, дальность до антенны, угол падения луча, фазу, мощность сигнала, которая зависит от поляризации излучения и от взаимного направления поляризации при излучении и приеме. Для решения проблемы разработано клиент-серверное приложение для распределенного моделирования радиолокационной системы с синтезированной апертуры антенны. Поскольку, каждый фацет можно обрабатывать независимо используется параллелизм на уровне данных. Каждому клиенту соответствует определенная область подстилающей поверхности. Список используемой литературы: 1.	Лнтипов В.Н. Радиолокационные станции с цифровым синтезированием апертуры антенны / В.Н. Лнтипов и др.; Под ред. В.Т.Горяинова. – М.: Радио и связь, 1988. – 304 с. 2.	Баскаков А.И. Локационные методы исследования объектов и сред: учебник для студ. Учреждений высш. проф. Образования / А.И. Баскаков, Т.С. Жутяева, Ю.И. Лукашенко; под. Ред. А.И. Баскакова. – М.: Издательский центр «Академия», 2011. – 384 с. 3.	Карасев Д.В. Математическое моделирование процесса формирования радиолокационного изображения для полно- поляризационных радаров с синтезированной апертурой / Д.В. Карасев, В.А. Карпов, В.И. Безродный, Н.А. Кокоихина. - Сборник трудов XXIII Международной научно-технической конференции, 2017. – 874-880 с.
</Content>
<field id="content">
Синтезирование апертуры представляет собой технический прием, позволяющий существенно повысить разрешающую способность радиолокатора в поперечном относительно направления полета направлении и получить детальное изображение радиолокационной карты местности, над которой совершает полет ЛА [1]. Для моделирования отражения сигнала от поверхности используется фацетная модель поверхности, представляющая поверхность в виде совокупности элементарных отражающих элементов, представляющих собой пластины конечных размеров, совпадающие с поверхностью крупномасштабных неровностей [2]. Отраженный сигнал от поверхности представляет собой сумму сигналов от всех облучаемых фацетов. Причем каждый парциальный сигнал имеет свою амплитуду, определяемую ориентацией локальной диаграммы обратного рассеяния, и свою случайную фазу. Сигнал на входе приемной антенны РЛС представляет собой сумму парциальных сигналов, отраженных от всех фацетов в облучаемой области [3]. При моделировании среды распространении сигнала используется явление рефракции, которое объясняется изменением диэлектрической проницаемости и, соответственно показателя преломления воздуха с высотой. Нижние слои воздуха рассматриваются как среда, диэлектрическая проницаемость которой изменяется с высотой вследствие разрежения воздуха. При прохождении луча наблюдается его отклонение от прямолинейного распространения, вызванное наличием градиента показателя преломления. Cложность моделирования заключается в большом объеме исходных данных. Например, для несущей частоты 10ГГц (X – диапазон) плотность фацетов составит 44.4 эл./м2. Для каждого фацета всей подстилающей поверхности на каждый момент излучения антенны требуется определить: принадлежность диаграмме направленности антенны, в тени или нет, дальность до антенны, угол падения луча, фазу, мощность сигнала, которая зависит от поляризации излучения и от взаимного направления поляризации при излучении и приеме. Для решения проблемы разработано клиент-серверное приложение для распределенного моделирования радиолокационной системы с синтезированной апертуры антенны. Поскольку, каждый фацет можно обрабатывать независимо используется параллелизм на уровне данных. Каждому клиенту соответствует определенная область подстилающей поверхности. Список используемой литературы: 1.	Лнтипов В.Н. Радиолокационные станции с цифровым синтезированием апертуры антенны / В.Н. Лнтипов и др.; Под ред. В.Т.Горяинова. – М.: Радио и связь, 1988. – 304 с. 2.	Баскаков А.И. Локационные методы исследования объектов и сред: учебник для студ. Учреждений высш. проф. Образования / А.И. Баскаков, Т.С. Жутяева, Ю.И. Лукашенко; под. Ред. А.И. Баскакова. – М.: Издательский центр «Академия», 2011. – 384 с. 3.	Карасев Д.В. Математическое моделирование процесса формирования радиолокационного изображения для полно- поляризационных радаров с синтезированной апертурой / Д.В. Карасев, В.А. Карпов, В.И. Безродный, Н.А. Кокоихина. - Сборник трудов XXIII Международной научно-технической конференции, 2017. – 874-880 с.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Bezrodny</FamilyName>
<Email>vova.bezrodny@gmail.com</Email>
<Affiliation>Mari State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anatolii</FirstName>
<FamilyName>Leukhin</FamilyName>
<Email>leukhinan@list.ru</Email>
<Affiliation>Mari State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrei</FirstName>
<FamilyName>Voronin</FamilyName>
<Email>v.andrei@protonmail.com</Email>
<Affiliation>Mari State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Bezrodny</FamilyName>
<Email>vova.bezrodny@gmail.com</Email>
<Affiliation>Mari State University</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>321</Id>
<Title>
CRIC: the information system for LHC Distributed Computing
</Title>
<Content>
The Worldwide LHC Computing Grid infrastructure links about 200 participating computing centers affiliated with several partner projects. It is built by integrating heterogeneous compute and storage resources in diverse data centers all over the world and provides CPU and storage capacity to the LHC experiments to perform data processing and physics analysis at petabytes scale data operations. Moreover the experiments extend the capability of WLCG distributed environment by actively connecting opportunistic Cloud platforms, HPC and volunteer resources. In order to be effectively being used by the experiments, these distributed resources should be well described, which implies easy service discovery and detailed description of service configuration. CRIC represents the evolution of ATLAS Grid Information System (AGIS) into the common experiment independent high-level information framework which has been evolved in order to serve not just ATLAS Collaboration needs for the description of distributed environment but any other virtual organization relying on large scale distributed infrastructure as well as the WLCG on the global scope. CRIC collects information from various information providers, complements it with experiment-specific configuration required for computing operations, performs data validation and provides coherent view and topology description to the LHC VOs for service discovery and usage configuration. In this contribution we describe the design and overall architecture of the system, recent developments and most important aspects of the CRIC framework components implementation and features like flexible definition of information models, built-in collectors, user interfaces, advanced fine-granular authentication/authorization and others.
</Content>
<field id="content">
The Worldwide LHC Computing Grid infrastructure links about 200 participating computing centers affiliated with several partner projects. It is built by integrating heterogeneous compute and storage resources in diverse data centers all over the world and provides CPU and storage capacity to the LHC experiments to perform data processing and physics analysis at petabytes scale data operations. Moreover the experiments extend the capability of WLCG distributed environment by actively connecting opportunistic Cloud platforms, HPC and volunteer resources. In order to be effectively being used by the experiments, these distributed resources should be well described, which implies easy service discovery and detailed description of service configuration. CRIC represents the evolution of ATLAS Grid Information System (AGIS) into the common experiment independent high-level information framework which has been evolved in order to serve not just ATLAS Collaboration needs for the description of distributed environment but any other virtual organization relying on large scale distributed infrastructure as well as the WLCG on the global scope. CRIC collects information from various information providers, complements it with experiment-specific configuration required for computing operations, performs data validation and provides coherent view and topology description to the LHC VOs for service discovery and usage configuration. In this contribution we describe the design and overall architecture of the system, recent developments and most important aspects of the CRIC framework components implementation and features like flexible definition of information models, built-in collectors, user interfaces, advanced fine-granular authentication/authorization and others.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Anisenkov</FamilyName>
<Email>alexey.anisenkov@cern.ch</Email>
<Affiliation>BINP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Anisenkov</FamilyName>
<Email>alexey.anisenkov@cern.ch</Email>
<Affiliation>BINP</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>322</Id>
<Title>
Mechanisms for ensuring the integrity of information in distributed computing systems in the long-term period of time
</Title>
<Content>
The article discusses issues of ensuring the integrity of information over a long period of time. This task was not raised earlier. However, experience shows that in the long periods of time in electronic archives there can be an uncontrolled change in information and even its disappearance. Attacks on the integrity of electronic archives can be targeted. This requires the creation of information technology to ensure the integrity of archives. The work is devoted to the mechanism of the integrity of information in the electronic archive by creating a distributed managed trusted environment. This allows you to track the processes, data, user actions and make decisions about the choice of the owners of the archive, restore the archive with a partial loss of information in it and meet attacks on the integrity of the archive. Keywords: information integrity, electronic archive, long period of time, attack.
</Content>
<field id="content">
The article discusses issues of ensuring the integrity of information over a long period of time. This task was not raised earlier. However, experience shows that in the long periods of time in electronic archives there can be an uncontrolled change in information and even its disappearance. Attacks on the integrity of electronic archives can be targeted. This requires the creation of information technology to ensure the integrity of archives. The work is devoted to the mechanism of the integrity of information in the electronic archive by creating a distributed managed trusted environment. This allows you to track the processes, data, user actions and make decisions about the choice of the owners of the archive, restore the archive with a partial loss of information in it and meet attacks on the integrity of the archive. Keywords: information integrity, electronic archive, long period of time, attack.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Anatoly</FirstName>
<FamilyName>Minzov</FamilyName>
<Email>minzovas@mpei.ru</Email>
<Affiliation>MPEI</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>A.Y.</FirstName>
<FamilyName>Nevskii</FamilyName>
<Email>nevskyay@mpei.ru</Email>
<Affiliation>
National Research University "MEI", Moscow, Russian Federation
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>O.R.</FirstName>
<FamilyName>Baronov</FamilyName>
<Email>baronovor@mpei.ru</Email>
<Affiliation>
National Research University "MEI", Moscow, Russian Federation
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anatoly</FirstName>
<FamilyName>Minzov</FamilyName>
<Email>minzovas@mpei.ru</Email>
<Affiliation>MPEI</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
</abstract>
<abstract>
<Id>323</Id>
<Title>
Evolution of Data Storage and Handling at Italian WLCG Data Center
</Title>
<Content>
The Italian WLCG Data Center which is hosted and managed by National Center of INFN for Informatics and Telecommunications (CNAF) located in Bologna, is the one of the biggest European computing centers for WLCG. Operational since 2005 it provides storage resources for all the Large Hadron Collider (LHC) experiments, as well as for many other scientific collaborations. In 12 years of operations with increasing demands and growing resources the data storage and handling services underwent some evolutionary changes. Here we will discuss our experience we have collected during this years and will present our vision for future developments in the Data Storage and Handling services for WLCG-oriented sites.
</Content>
<field id="content">
The Italian WLCG Data Center which is hosted and managed by National Center of INFN for Informatics and Telecommunications (CNAF) located in Bologna, is the one of the biggest European computing centers for WLCG. Operational since 2005 it provides storage resources for all the Large Hadron Collider (LHC) experiments, as well as for many other scientific collaborations. In 12 years of operations with increasing demands and growing resources the data storage and handling services underwent some evolutionary changes. Here we will discuss our experience we have collected during this years and will present our vision for future developments in the Data Storage and Handling services for WLCG-oriented sites.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Sapunenko</FamilyName>
<Email>vladimir.sapunenko@cnaf.infn.it</Email>
<Affiliation>INFN</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Luca</FirstName>
<FamilyName>Dell'Agnello</FamilyName>
<Email>luca.dellagnello@cnaf.infn.it</Email>
<Affiliation>INFN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Gaetano</FirstName>
<FamilyName>Maron</FamilyName>
<Email>gaetano.maron@lnl.infn.it</Email>
<Affiliation>INFN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Daniele</FirstName>
<FamilyName>Cesini</FamilyName>
<Email>daniele.cesini@cnaf.infn.it</Email>
<Affiliation>INFN</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Sapunenko</FamilyName>
<Email>vladimir.sapunenko@cnaf.infn.it</Email>
<Affiliation>INFN</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>324</Id>
<Title>
Information-software environment for the GOVORUN supercomuter
</Title>
<Content>
In order to increase the efficiency of the application development and of the computations using the resources of the GOVORUN supercomputer, an IT-ecosystem that includes a set of services for users is being actively developed. The maintenance of the current services and the development of new ones is a key prerequisite since they secure to the users modern tools for efficient organization of their work under rapidly evolving technologies.This article describes new additions to the information-software environment of the GOVORUN supercomputer.
</Content>
<field id="content">
In order to increase the efficiency of the application development and of the computations using the resources of the GOVORUN supercomputer, an IT-ecosystem that includes a set of services for users is being actively developed. The maintenance of the current services and the development of new ones is a key prerequisite since they secure to the users modern tools for efficient organization of their work under rapidly evolving technologies.This article describes new additions to the information-software environment of the GOVORUN supercomputer.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Shushanik</FirstName>
<FamilyName>Torosyan</FamilyName>
<Email>shusha.torosyan@gmail.com</Email>
<Affiliation>LIT</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Oksana</FirstName>
<FamilyName>Streltsova</FamilyName>
<Email>strel@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Podgainy</FamilyName>
<Email>podgainy@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Margarit</FirstName>
<FamilyName>Kirakosyan</FamilyName>
<Email>kmargarit@jinr.ru</Email>
<Affiliation>LIT</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Shushanik</FirstName>
<FamilyName>Torosyan</FamilyName>
<Email>shusha.torosyan@gmail.com</Email>
<Affiliation>LIT</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>325</Id>
<Title>HybriLIT monitoring system</Title>
<Content>
The heterogeneous cluster HybriLIT and Supercomputer Govorun are designed for the development of parallel applications and for carrying out parallel computations asked by a wide range of tasks arising in the scientific and applied research conducted by JINR. The efficient work on the cluster needs the implementation of service of statistics provided to the users. Even though tasks of monitoring of distributed computing and gathering its statistics are encountered more and more frequently, there is not so many well-known methods to do this. We developing web-service for hybrid heterogeneous cluster “HybriLIT”, that solves that task using Node.JS as it’s server and Angular for a presentment of data. Monitoring itself carried out by a sensor written on C++ with the using of libgtop library. At the moment functions of monitoring CPU load, memory load, network and GPU load of the computing node and browsing that data in both table and graphical form are already implemented. Also, there are diagrams of usage for different laboratories and users, information about currently running jobs and an archive table for a jobs that was computed on a cluster.
</Content>
<field id="content">
The heterogeneous cluster HybriLIT and Supercomputer Govorun are designed for the development of parallel applications and for carrying out parallel computations asked by a wide range of tasks arising in the scientific and applied research conducted by JINR. The efficient work on the cluster needs the implementation of service of statistics provided to the users. Even though tasks of monitoring of distributed computing and gathering its statistics are encountered more and more frequently, there is not so many well-known methods to do this. We developing web-service for hybrid heterogeneous cluster “HybriLIT”, that solves that task using Node.JS as it’s server and Angular for a presentment of data. Monitoring itself carried out by a sensor written on C++ with the using of libgtop library. At the moment functions of monitoring CPU load, memory load, network and GPU load of the computing node and browsing that data in both table and graphical form are already implemented. Also, there are diagrams of usage for different laboratories and users, information about currently running jobs and an archive table for a jobs that was computed on a cluster.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Kashunin</FamilyName>
<Email>miramir@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Belyakov</FamilyName>
<Email>dmitry@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>326</Id>
<Title>
Scalability of the Parallel Strongin Algorithm in the Problem of Optimizing a Molecular-Dynamic Force Field
</Title>
<Content>
Strongin's multifactorial global search algorithm (MGSA) allows one to find an absolute minimum of a function of multiple variables on a mesh. In this contribution a parallel program is presented that implements the algorithm above applied to ReaxFF MD force field parameter search. The MGSA converges the faster, the greater number of mesh points have computed target function value in them. In case of ReaxFF optimization computation time of a target function value significantly exceeds time of data exchange between parallel processes. One is able to speed up computation by obtaining not only one but several function values in various points simultaneously. Our software implements two levels of parallelism. To deal with function of multiple variables, one uses a scan for mapping a multidimensional domain of definition of a function into a one-dimensional segment. To decrease the effect of losing information of multidimensional points proximity, multiple scans are used, the number of which is denoted by N. The algorithm deals with each scan in parallel, computing function value in N different mesh points in a single iteration. This is the first level of parallelism. To define a mesh point of a next iteration, MGSA finds a subinterval with the most probable location of the minimum and computes a target function value in a certain point of this subinterval. If one computes a function value not only in the most probable subinterval but also in (M-1) subintervals with less probability in parallel, one will be able to obtain function values in M different mesh points. This will accelerate the convergence, increasing the amount of data about the target function received every iteration. This is the second level of parallelism. Thus the two levels allow one to compute M * N function values every iteration. In this contribution we research scalability of our MGSA implementation, namely, the dependence of the number of algorithm iterations it needs to converge on the number of CPU cores used, separately for each level of parallelism.
</Content>
<field id="content">
Strongin's multifactorial global search algorithm (MGSA) allows one to find an absolute minimum of a function of multiple variables on a mesh. In this contribution a parallel program is presented that implements the algorithm above applied to ReaxFF MD force field parameter search. The MGSA converges the faster, the greater number of mesh points have computed target function value in them. In case of ReaxFF optimization computation time of a target function value significantly exceeds time of data exchange between parallel processes. One is able to speed up computation by obtaining not only one but several function values in various points simultaneously. Our software implements two levels of parallelism. To deal with function of multiple variables, one uses a scan for mapping a multidimensional domain of definition of a function into a one-dimensional segment. To decrease the effect of losing information of multidimensional points proximity, multiple scans are used, the number of which is denoted by N. The algorithm deals with each scan in parallel, computing function value in N different mesh points in a single iteration. This is the first level of parallelism. To define a mesh point of a next iteration, MGSA finds a subinterval with the most probable location of the minimum and computes a target function value in a certain point of this subinterval. If one computes a function value not only in the most probable subinterval but also in (M-1) subintervals with less probability in parallel, one will be able to obtain function values in M different mesh points. This will accelerate the convergence, increasing the amount of data about the target function received every iteration. This is the second level of parallelism. Thus the two levels allow one to compute M * N function values every iteration. In this contribution we research scalability of our MGSA implementation, namely, the dependence of the number of algorithm iterations it needs to converge on the number of CPU cores used, separately for each level of parallelism.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Stepanova</FirstName>
<FamilyName>Margarita</FamilyName>
<Email>mstep@mms.nw.ru</Email>
<Affiliation>SPbSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Konstantin</FirstName>
<FamilyName>Shefov</FamilyName>
<Email>k.s.shefov@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Konstantin</FirstName>
<FamilyName>Shefov</FamilyName>
<Email>k.s.shefov@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>327</Id>
<Title>
Merging multidimensional histograms via hypercube algorithm
</Title>
<Content>
Scientists in high energy physics produce their output mostly in form of histograms. Set of histograms are saved in output file for each grid job. As the next step is to merge these files/histograms to one file where scientist can produce final plots for publication. Merging of these out files may be done sequentially as one job or do it in parallel via binary tree algorithm as it is done by many users. Using histogram with low dimensions (1D or 2D) one can fit in memory with final merged objects. On the other side, if dimensions or binning of histograms are increaced, sparse implementation of histogram has to be used in analysis and final object might grow so much that user will not be able to merge or open final merged object because it will not fit in memory at some point. Our task is merge these multidimensional histograms to N independed objects to multiple files, where each file will contain uniqe part of merged object sorted by some axis in histogram dimension. For optimalization reasons hypercube algorithm is used.
</Content>
<field id="content">
Scientists in high energy physics produce their output mostly in form of histograms. Set of histograms are saved in output file for each grid job. As the next step is to merge these files/histograms to one file where scientist can produce final plots for publication. Merging of these out files may be done sequentially as one job or do it in parallel via binary tree algorithm as it is done by many users. Using histogram with low dimensions (1D or 2D) one can fit in memory with final merged objects. On the other side, if dimensions or binning of histograms are increaced, sparse implementation of histogram has to be used in analysis and final object might grow so much that user will not be able to merge or open final merged object because it will not fit in memory at some point. Our task is merge these multidimensional histograms to N independed objects to multiple files, where each file will contain uniqe part of merged object sorted by some axis in histogram dimension. For optimalization reasons hypercube algorithm is used.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Bulatov</FamilyName>
<Email>andrey.bulatov20@gmail.com</Email>
<Affiliation>State University Dubna, JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Bulatov</FamilyName>
<Email>andrey.bulatov20@gmail.com</Email>
<Affiliation>State University Dubna, JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
8. High performance computing, CPU architectures, GPU, FPGA
</Track>
</abstract>
<abstract>
<Id>328</Id>
<Title>
Improving the load of supercomputers based on job migration using container virtualization
</Title>
<Content>
Modern supercomputers employ schedulers to distribute the queued jobs between the computational nodes and maximize their load. The jobs can vary in size (number of required computational units, planned time) and it is impossible to schedule them with 100% efficiency. Additionally, some jobs are completed ahead of time which can also decrease the load. In practice the average load of supercomputers seldom exceeds 90% and often is below 80%. Theoretically, any slots left empty by the scheduler can be used by jobs that require a single scheduling CPU slot and sufficiently short execution time. In practice, there are numerous computational problems that do not require parallelization and therefore need a single CPU slot but have significant time requirements. A queue of jobs solving these problems can be used to fill short schedule slots if the jobs are migrated to different nodes or saved and returned to the queue before the end of the allotted time. It is possible with the use of container virtualization that allows to start, stop, and migrate containers relatively quickly. We develop a simulation model of a system intended to increase the load of supercomputers using an additional queue of non-parallel jobs packed in containers. Here we present an estimation of the load of supercomputers using different scheduler algorithms. The estimation is based on simulation with a simulated queue consisting of jobs with randomly generated parameters (number of CPUs, planned time, actual completion time). The chosen distribution of the parameters is based on typical data of supercomputers. We also present an estimation of the expected load increase resulting from the use of container migration.
</Content>
<field id="content">
Modern supercomputers employ schedulers to distribute the queued jobs between the computational nodes and maximize their load. The jobs can vary in size (number of required computational units, planned time) and it is impossible to schedule them with 100% efficiency. Additionally, some jobs are completed ahead of time which can also decrease the load. In practice the average load of supercomputers seldom exceeds 90% and often is below 80%. Theoretically, any slots left empty by the scheduler can be used by jobs that require a single scheduling CPU slot and sufficiently short execution time. In practice, there are numerous computational problems that do not require parallelization and therefore need a single CPU slot but have significant time requirements. A queue of jobs solving these problems can be used to fill short schedule slots if the jobs are migrated to different nodes or saved and returned to the queue before the end of the allotted time. It is possible with the use of container virtualization that allows to start, stop, and migrate containers relatively quickly. We develop a simulation model of a system intended to increase the load of supercomputers using an additional queue of non-parallel jobs packed in containers. Here we present an estimation of the load of supercomputers using different scheduler algorithms. The estimation is based on simulation with a simulated queue consisting of jobs with randomly generated parameters (number of CPUs, planned time, actual completion time). The chosen distribution of the parameters is based on typical data of supercomputers. We also present an estimation of the expected load increase resulting from the use of container migration.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Stanislav</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>s.p.polyakov@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Stanislav</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>s.p.polyakov@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>6. Cloud computing, Virtualization</Track>
</abstract>
<abstract>
<Id>329</Id>
<Title>
Библиотеки и пакеты прикладных программ, доступные пользователям ЭВМ ОИЯИ
</Title>
<Content>
Библиотеки и пакеты прикладных программ, доступные пользователям ЭВМ ОИЯИ Попкова Л.В., А. П. Сапожников, Т. Ф. Сапожникова Объединённый институт ядерных исследований Лаборатория информационных технологий Информация о библиотеках и пакетах прикладных программ, поддерживаемых и сопровождаемых в ЛИТ ОИЯИ - JINRLIB, CERNLIB, CPCLIB - размещена на сайте http://wwwinfo.jinr.ru/programs/ . JINRLIB (http://www.jinr.ru/programs/jinrlib/) - библиотека программ, предназначенных для решения широкого круга математических и физических задач. Пополнение библиотеки происходит новыми программами, создаваемыми сотрудниками ЛИТ ОИЯИ и их коллаборантами. В зависимости от способа сопровождения и распространения библиотека делится на две части: одна часть распространяется в виде объектных модулей, другая - в виде автономных пакетов прикладных программ. Библиотеки объектных модулей готовятся на компьютерах Многофункционального информационно - вычислительного комплекса ОИЯИ с ОС Linux, а также на компьютерах с ОС Windows для всех доступных фортранных трансляторов. Программы, которые по разным причинам не могут распространяться в виде библиотек объектных модулей, также размещаются в JINRLIB. Вся информация, предоставленная автором программы, помещается на WWW-сайт. В настоящий момент насчитывается более 60 программных пакетов, большинство которых решает задачи автоматизации обработки экспериментальных данных и вычислительной математики. В последнее время происходит бурное развитие технологий программирования параллельных вычислений, в частности, MPI. Эта тенденция нашла свое отражение и в библиотеке JINRLIB, где создан раздел для программ с использованием технологии MPI. Специализированный WWW – сайт обеспечивает электронный доступ к библиотеке JINRLIB, где можно найти описания программ и программных пакетов, исходные тексты, библиотеки объектных модулей. Ведется каталог вновь поступивших программ и программных пакетов. Для получения статистики использования программ заведены счетчик посещений страницы и счетчик количества скачиваний текстов программ. По каталогу JINRLIB создана база данных авторов программ библиотеки, на основании которой строится таблица авторов со списком программ, имеется возможность получить список программ нужного автора. Библиотека CERNLIB (http://wwwinfo.cern.ch/asd/index.html) – большая коллекция программ, поддерживаемых и распространяемых на исходном языке, в объектном коде и в виде готовых программ. Большинство этих программ разработано в CERN и ориентировано на решение физических и математических проблем. Библиотека CERNLIB в CERN сейчас не поддерживается, но, учитывая интерес к ней FORTRAN-ориентированных пользователей, была выполнена пересборка наиболее востребованных программ библиотеки MATHLIB для OS Windows. CPCPL - международная библиотека программ журнала Computer Physics Communications (CPC) - в настоящее время является одним из самых представительных и хорошо организованных банков программ, решающих задачи физики, математики, химии и других смежных областей знаний. ОИЯИ имеет подписку на журнал СРС и библиотеку программ, и по лицензионному соглашению сотрудники ОИЯИ имеют доступ к общей информации и к программам библиотеки.
</Content>
<field id="content">
Библиотеки и пакеты прикладных программ, доступные пользователям ЭВМ ОИЯИ Попкова Л.В., А. П. Сапожников, Т. Ф. Сапожникова Объединённый институт ядерных исследований Лаборатория информационных технологий Информация о библиотеках и пакетах прикладных программ, поддерживаемых и сопровождаемых в ЛИТ ОИЯИ - JINRLIB, CERNLIB, CPCLIB - размещена на сайте http://wwwinfo.jinr.ru/programs/ . JINRLIB (http://www.jinr.ru/programs/jinrlib/) - библиотека программ, предназначенных для решения широкого круга математических и физических задач. Пополнение библиотеки происходит новыми программами, создаваемыми сотрудниками ЛИТ ОИЯИ и их коллаборантами. В зависимости от способа сопровождения и распространения библиотека делится на две части: одна часть распространяется в виде объектных модулей, другая - в виде автономных пакетов прикладных программ. Библиотеки объектных модулей готовятся на компьютерах Многофункционального информационно - вычислительного комплекса ОИЯИ с ОС Linux, а также на компьютерах с ОС Windows для всех доступных фортранных трансляторов. Программы, которые по разным причинам не могут распространяться в виде библиотек объектных модулей, также размещаются в JINRLIB. Вся информация, предоставленная автором программы, помещается на WWW-сайт. В настоящий момент насчитывается более 60 программных пакетов, большинство которых решает задачи автоматизации обработки экспериментальных данных и вычислительной математики. В последнее время происходит бурное развитие технологий программирования параллельных вычислений, в частности, MPI. Эта тенденция нашла свое отражение и в библиотеке JINRLIB, где создан раздел для программ с использованием технологии MPI. Специализированный WWW – сайт обеспечивает электронный доступ к библиотеке JINRLIB, где можно найти описания программ и программных пакетов, исходные тексты, библиотеки объектных модулей. Ведется каталог вновь поступивших программ и программных пакетов. Для получения статистики использования программ заведены счетчик посещений страницы и счетчик количества скачиваний текстов программ. По каталогу JINRLIB создана база данных авторов программ библиотеки, на основании которой строится таблица авторов со списком программ, имеется возможность получить список программ нужного автора. Библиотека CERNLIB (http://wwwinfo.cern.ch/asd/index.html) – большая коллекция программ, поддерживаемых и распространяемых на исходном языке, в объектном коде и в виде готовых программ. Большинство этих программ разработано в CERN и ориентировано на решение физических и математических проблем. Библиотека CERNLIB в CERN сейчас не поддерживается, но, учитывая интерес к ней FORTRAN-ориентированных пользователей, была выполнена пересборка наиболее востребованных программ библиотеки MATHLIB для OS Windows. CPCPL - международная библиотека программ журнала Computer Physics Communications (CPC) - в настоящее время является одним из самых представительных и хорошо организованных банков программ, решающих задачи физики, математики, химии и других смежных областей знаний. ОИЯИ имеет подписку на журнал СРС и библиотеку программ, и по лицензионному соглашению сотрудники ОИЯИ имеют доступ к общей информации и к программам библиотеки.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Tatiana</FirstName>
<FamilyName>Sapozhnikova</FamilyName>
<Email>tsap@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>L.V.</FirstName>
<FamilyName>Popkova</FamilyName>
<Email>lyuda@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Sapozhnikov</FamilyName>
<Email>sap@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Tatiana</FirstName>
<FamilyName>Sapozhnikova</FamilyName>
<Email>tsap@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>330</Id>
<Title>
Discrete and Global Optimization in Everest Distributed Environment by Loosely Coupled Branch-and-Bound Solvers
</Title>
<Content>
The report presents an new approach to solving rather hard discrete and global optimization problems in Everest, http://everest.distcomp.org, a web-based distributed computing platform. The platform enables convenient access to heterogeneous resources (standalone servers, high performance clusters etc.) by means of domain-specific computational web services, development and execution of many-task applications, and pooling of multiple resources for running distributed computations. Rather generic Everst-application had been implemented for solving discrete and global optimization problems - so called DDBNB, Domain Decomposition Branch-and-Bound, https://github.com/distcomp/ddbnb. DDBNB implements a kind of coarse-grained parallelization of Branch-and-Bound (BNB) method. It supports two strategies (including combined usage of both): decomposition of feasible domain into a set of sub-problems; multisearch (or concurrent) solving the same problem with different settings of the BNB-method. In both cases several BNB-solver's processes exchange incumbents, best values of goal function on feasible solutions, they found. DDBNB uses generic Everest messaging service and open source solvers : CBC, https://projects.coin-or.org/Cbc; SCIP, http://scip.zib.de. By now we got some experience in solving different optimization problems: Travelling Salesman Problem (TSP) as Mixed-Integer Linear Programming; global optimization problems with all continuous variables (so called Tammes and Thomson problems, both relate to sphere packing) and global optimization with continuous and binary variable (so called Flat Torus Packing problems). For TSP, computing experiments are presented, when DDBNB worked in different modes: Domain Decomposition only, multisearch only; combined mode. As to global optimization, all problems had been reduced to the form of mathematical programming problems with polynomial functions (solver SCIP supports solving of such class of problems). Here, only Domain Decomposition had been used and different methods of decomposition in the case of sphere's and torus's packing and corresponding computing experiments are presented. Formulation of original problems and decomposition strategies have been implemented via Pyomo, http://www.pyomo.org. All sub-problems have been passed to solvers as AMPL-stubs, (also known as NL-files). Usage of Everest platform enables to involve in experiments computing resources of Center for Distributed Computing, http://distcomp.ru, HPCs of NRC Kurchatov Institute and South Ural State University. The reported study was funded by RFBR according to the research projects #18-07-01175 and #18-07-00956.
</Content>
<field id="content">
The report presents an new approach to solving rather hard discrete and global optimization problems in Everest, http://everest.distcomp.org, a web-based distributed computing platform. The platform enables convenient access to heterogeneous resources (standalone servers, high performance clusters etc.) by means of domain-specific computational web services, development and execution of many-task applications, and pooling of multiple resources for running distributed computations. Rather generic Everst-application had been implemented for solving discrete and global optimization problems - so called DDBNB, Domain Decomposition Branch-and-Bound, https://github.com/distcomp/ddbnb. DDBNB implements a kind of coarse-grained parallelization of Branch-and-Bound (BNB) method. It supports two strategies (including combined usage of both): decomposition of feasible domain into a set of sub-problems; multisearch (or concurrent) solving the same problem with different settings of the BNB-method. In both cases several BNB-solver's processes exchange incumbents, best values of goal function on feasible solutions, they found. DDBNB uses generic Everest messaging service and open source solvers : CBC, https://projects.coin-or.org/Cbc; SCIP, http://scip.zib.de. By now we got some experience in solving different optimization problems: Travelling Salesman Problem (TSP) as Mixed-Integer Linear Programming; global optimization problems with all continuous variables (so called Tammes and Thomson problems, both relate to sphere packing) and global optimization with continuous and binary variable (so called Flat Torus Packing problems). For TSP, computing experiments are presented, when DDBNB worked in different modes: Domain Decomposition only, multisearch only; combined mode. As to global optimization, all problems had been reduced to the form of mathematical programming problems with polynomial functions (solver SCIP supports solving of such class of problems). Here, only Domain Decomposition had been used and different methods of decomposition in the case of sphere's and torus's packing and corresponding computing experiments are presented. Formulation of original problems and decomposition strategies have been implemented via Pyomo, http://www.pyomo.org. All sub-problems have been passed to solvers as AMPL-stubs, (also known as NL-files). Usage of Everest platform enables to involve in experiments computing resources of Center for Distributed Computing, http://distcomp.ru, HPCs of NRC Kurchatov Institute and South Ural State University. The reported study was funded by RFBR according to the research projects #18-07-01175 and #18-07-00956.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Voloshinov</FamilyName>
<Email>vladimir.voloshinov@gmail.com</Email>
<Affiliation>
Institute for Information Transmission Problems RAS
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Smirnov</FamilyName>
<Email>sasmir@gmail.com</Email>
<Affiliation>
Institute for Information Transmission Problems of the Russian Academy of Sciences
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Voloshinov</FamilyName>
<Email>vladimir.voloshinov@gmail.com</Email>
<Affiliation>
Institute for Information Transmission Problems RAS
</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
2. Operation, monitoring, optimization in distributed computing systems
</Track>
<Track>
9. Consolidation and integration of distributed resources
</Track>
</abstract>
<abstract>
<Id>331</Id>
<Title>
Исследование особенностей Интернет-трафика в магистральном канале
</Title>
<Content>
В работах [1, 2] анализировались статистические особенности потоков информации на входном шлюзе локальной сети среднего размера (250-300 компьютеров) и было показано, что при агрегировании измерений трафика формируется (начиная с некоторого порогового значения окна агрегирования: в нашем случае 1 с) статистическое распределение величины потока, которое не меняет своей формы при дальнейшем росте окна агрегирования (вплоть до 10 с). Было показано, что указанное распределение с высокой точностью аппроксимируется логнормальным распределением. Авторами работы [3] также проводилось агрегирование измерений трафика и был сделан вывод о том, что “гистограмма байтовой интенсивности соответствует логнормальному распределению”. Однако никакого обоснования этого утверждения не было приведено. Более того, в указанной гистограмме наблюдается пик в области малых интенсивностей, не согласующийся с логнормальным законом. В работе [4] исследовались статистические характеристики Интернет-трафика в магистральном канале при трех значениях времени агрегирования: 1 мс, 10 мс и 100 мс. Как отмечали авторы данной работы, полученные ими графики плотностей распределения вероятностей величины интенсивности трафика не удалось аппроксимировать каким-либо из известных распределений. В настоящем исследовании на основе анализа измерений Интернет-трафика в магистральном канале, взятых с того же сайта [5], что и в работе [4], показано, что при агрегировании измерений трафика формируются статистические распределения, которые, в зависимости от периода наблюдения, с высокой точностью аппроксимируются логнормальным, либо двумя логнормальным распределениями. Список литературы [1]	I. Antoninou, V.V. Ivanov, Valery V. Ivanov, and P.V. Zrelov: On the Log-Normal Distribution of Network Traffic, Physica D 167 (2002) 72-85. [2]	I. Antoninou, V.V. Ivanov, Valery V. Ivanov, and P.V. Zrelov: Statistical Model of Network Traffic, “Физика элементарных частиц и атомного ядра” (ЭЧАЯ), 2004, Т.35, Вып.4 (984-1019). [3]	Ю.А. Крюков, Д.В. Чернягин: ARIMA – модель прогнозирования значений трафи- ка, Информационные технологии и вычислительные системы, 2011, №2, C. 41-49. [4]	Д.В. Симаков, А.А. Кучин: Анализ статистических характеристик Интернет- трафика в магистральном канале, //T-Comm: Телекоммуникации и транспорт. 2015, Том 9, №5, С. 31-35. [5]	MAWI Working Group Traffic Archive. URL: http://mawi.wide.ad.jp/mawi/
</Content>
<field id="content">
В работах [1, 2] анализировались статистические особенности потоков информации на входном шлюзе локальной сети среднего размера (250-300 компьютеров) и было показано, что при агрегировании измерений трафика формируется (начиная с некоторого порогового значения окна агрегирования: в нашем случае 1 с) статистическое распределение величины потока, которое не меняет своей формы при дальнейшем росте окна агрегирования (вплоть до 10 с). Было показано, что указанное распределение с высокой точностью аппроксимируется логнормальным распределением. Авторами работы [3] также проводилось агрегирование измерений трафика и был сделан вывод о том, что “гистограмма байтовой интенсивности соответствует логнормальному распределению”. Однако никакого обоснования этого утверждения не было приведено. Более того, в указанной гистограмме наблюдается пик в области малых интенсивностей, не согласующийся с логнормальным законом. В работе [4] исследовались статистические характеристики Интернет-трафика в магистральном канале при трех значениях времени агрегирования: 1 мс, 10 мс и 100 мс. Как отмечали авторы данной работы, полученные ими графики плотностей распределения вероятностей величины интенсивности трафика не удалось аппроксимировать каким-либо из известных распределений. В настоящем исследовании на основе анализа измерений Интернет-трафика в магистральном канале, взятых с того же сайта [5], что и в работе [4], показано, что при агрегировании измерений трафика формируются статистические распределения, которые, в зависимости от периода наблюдения, с высокой точностью аппроксимируются логнормальным, либо двумя логнормальным распределениями. Список литературы [1]	I. Antoninou, V.V. Ivanov, Valery V. Ivanov, and P.V. Zrelov: On the Log-Normal Distribution of Network Traffic, Physica D 167 (2002) 72-85. [2]	I. Antoninou, V.V. Ivanov, Valery V. Ivanov, and P.V. Zrelov: Statistical Model of Network Traffic, “Физика элементарных частиц и атомного ядра” (ЭЧАЯ), 2004, Т.35, Вып.4 (984-1019). [3]	Ю.А. Крюков, Д.В. Чернягин: ARIMA – модель прогнозирования значений трафи- ка, Информационные технологии и вычислительные системы, 2011, №2, C. 41-49. [4]	Д.В. Симаков, А.А. Кучин: Анализ статистических характеристик Интернет- трафика в магистральном канале, //T-Comm: Телекоммуникации и транспорт. 2015, Том 9, №5, С. 31-35. [5]	MAWI Working Group Traffic Archive. URL: http://mawi.wide.ad.jp/mawi/
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ivanov@jinr.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Petr</FirstName>
<FamilyName>Zrelov</FamilyName>
<Email>zrelov@jinr.ru</Email>
<Affiliation>LIT JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kua@uni-dubna.ru</Email>
<Affiliation>Alekseevich</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Tatarinov</FamilyName>
<Email>ivan-tatarinov@yandex.ru</Email>
<Affiliation>Kaspersky Lab</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Valeriy</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ivanov@grid2018.jinr.ru</Email>
<Affiliation>JINR / TECHNOCOMPLEKT</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Tatarinov</FamilyName>
<Email>ivan-tatarinov@yandex.ru</Email>
<Affiliation>Kaspersky Lab</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
</abstract>
<abstract>
<Id>332</Id>
<Title>
The activity of Russian Chapter of International Desktop Grid Federation
</Title>
<Content>
Results of activity of the Russian Chapter of International Desktop Grid Federation (IDGF) are considered. Including interaction with community of the russian volunteers (crunchers), start new and support of the existing projects of the volunteer distributed computing.
</Content>
<field id="content">
Results of activity of the Russian Chapter of International Desktop Grid Federation (IDGF) are considered. Including interaction with community of the russian volunteers (crunchers), start new and support of the existing projects of the volunteer distributed computing.
</field>
<field id="summary">
In this report the results of activity of the Russian Chapter of International Desktop Grid Federation (IDGF) for 2 years are summed up. Continuous interaction with community of the russian crunchers (volunteers) was carried out. Cooperation was conducted in several directions: • Search of scientific groups among the russian scientists interested in creation of new projects; • Carrying out sociological researches; • Development of the existing projects of the voluntary distributed calculations; • Joint publications and reports at scientific conferences; • Support of the crowdfunding project on creations of the specialized computing server; • Support of creation of a cluster with the improved relation the cost-performance. By results of interaction of several scientific groups and community of the russian crunchers it is possible to tell that the group of crunchers which, at the high level helps scientific groups in a number of researches was created. It is possible to call such phenomenon the term "citizen science". Interaction of members of the Russian Chapter of IDGF happens on a constant basis that is expressed in a large number of joint publications.
</field>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Kurochkin</FamilyName>
<Email>qurochkin@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Kurochkin</FamilyName>
<Email>qurochkin@gmail.com</Email>
<Affiliation>IITP RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>333</Id>
<Title>
Kubernetes testbed cluster for the Lightweight Sites project
</Title>
<Content>
The Worldwide LHC Computing Grid (WLCG) is a global collaboration of more than 170 computing centres in 42 countries and the number is expected to grow in the coming years. However, provisioning resources (compute, network, storage) at new sites to support WLCG workloads is still no straightforward task and often requires significant assistance from WLCG experts. Recently, the WLCG community has initiated steps towards reducing such overheads through the use of prefab Docker containers or OpenStack VM images, along with the adoption of popular tools like Puppet for configuration. In 2017, the Lightweight Sites project was initiated to construct shared community repositories providing such building blocks. These repositories are governed by a single Lightweight Site Specification Document which describes a modular way to define site components such as Batch Systems, Compute Elements, Worker Nodes, Networks etc. Implementation of the specification is based on a popular orchestration technology – Kubernetes. Here it is discussed the testbed cluster for deploying Lightweight grid sites. The research is mainly focused on the controlling lifecycle of containers for compute element, batch system and worker. Also some parameters for benchmarking and evaluation of the performance of different implementations were introduced.
</Content>
<field id="content">
The Worldwide LHC Computing Grid (WLCG) is a global collaboration of more than 170 computing centres in 42 countries and the number is expected to grow in the coming years. However, provisioning resources (compute, network, storage) at new sites to support WLCG workloads is still no straightforward task and often requires significant assistance from WLCG experts. Recently, the WLCG community has initiated steps towards reducing such overheads through the use of prefab Docker containers or OpenStack VM images, along with the adoption of popular tools like Puppet for configuration. In 2017, the Lightweight Sites project was initiated to construct shared community repositories providing such building blocks. These repositories are governed by a single Lightweight Site Specification Document which describes a modular way to define site components such as Batch Systems, Compute Elements, Worker Nodes, Networks etc. Implementation of the specification is based on a popular orchestration technology – Kubernetes. Here it is discussed the testbed cluster for deploying Lightweight grid sites. The research is mainly focused on the controlling lifecycle of containers for compute element, batch system and worker. Also some parameters for benchmarking and evaluation of the performance of different implementations were introduced.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>IULIIA</FirstName>
<FamilyName>GAVRILENKO</FamilyName>
<Email>yulya952@rambler.ru</Email>
<Affiliation>
Research Assistant, Plekhanov Russian University of Economics, Moscow, Russia
</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>IULIIA</FirstName>
<FamilyName>GAVRILENKO</FamilyName>
<Email>yulya952@rambler.ru</Email>
<Affiliation>
Research Assistant, Plekhanov Russian University of Economics, Moscow, Russia
</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
</abstract>
<abstract>
<Id>334</Id>
<Title>
Data gathering and wrangling for the monitoring of the Russian labour market
</Title>
<Content>
This project is devoted to monitoring and analyzing the labour market based on the publicly available data on job offers, CVs and companies gathered from open data projects and recruitment agencies. The relevance of project is that some work areas have already overcrowded, some are outdated or some may have a little need for new workers, and some new and growing industries will offer good jobs. The result obtained at the end will allow one to have a look on the labor market on different levels starting from the local one. This information is useful not only for school graduates, students and people who is just looking for a better job for themselves, but also for the employers. It is also can be useful for universities to estimate the relevance of the educational programs they offer. One of the key tasks is the collection of job offers data from open sources and recruitment agencies. Before writing parsing-scripts, need to analyze existing open sources of vacancies and identify the final list from which the vacancy data will be downloaded. No less important task is data pre-processing, where the main task is to remove duplicate job offers appear from different sources. Because sophisticated comparison of more than a million vacancies requires significant time, this step was realized using Apache Spark on a cluster. Also, this step involves using of machine learning algorithms. For the job offers, the vector representation is constructed using gensim word2vec, then the closest ones are selected. For the moment, more than a million of vacancies from Headhunter, Superjob, Trudvsem recruitment agencies have been already collected and processed.
</Content>
<field id="content">
This project is devoted to monitoring and analyzing the labour market based on the publicly available data on job offers, CVs and companies gathered from open data projects and recruitment agencies. The relevance of project is that some work areas have already overcrowded, some are outdated or some may have a little need for new workers, and some new and growing industries will offer good jobs. The result obtained at the end will allow one to have a look on the labor market on different levels starting from the local one. This information is useful not only for school graduates, students and people who is just looking for a better job for themselves, but also for the employers. It is also can be useful for universities to estimate the relevance of the educational programs they offer. One of the key tasks is the collection of job offers data from open sources and recruitment agencies. Before writing parsing-scripts, need to analyze existing open sources of vacancies and identify the final list from which the vacancy data will be downloaded. No less important task is data pre-processing, where the main task is to remove duplicate job offers appear from different sources. Because sophisticated comparison of more than a million vacancies requires significant time, this step was realized using Apache Spark on a cluster. Also, this step involves using of machine learning algorithms. For the job offers, the vector representation is constructed using gensim word2vec, then the closest ones are selected. For the moment, more than a million of vacancies from Headhunter, Superjob, Trudvsem recruitment agencies have been already collected and processed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Javad</FirstName>
<FamilyName>Javadzade</FamilyName>
<Email>jjavadzade@inbox.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Javad</FirstName>
<FamilyName>Javadzade</FamilyName>
<Email>jjavadzade@inbox.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Poster presentations</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
</abstract>
<abstract>
<Id>335</Id>
<Title>
THE GAME CHARACTER OF COLLABORATION IN VOLUNTEER COMPUTING COMMUNITY
</Title>
<Content>
The paper shows the emergence of a new form of online scientific collaboration, the collaborative networks of volunteer computing (VC) participants. And it examines what makes a collaborative VC-project successful and determines the formation of VC-community. We report on data from a statistic online study of volunteers’ activities and an online survey of VC-participants on several online forums, and discuss and analyze the emerging type of collaboration network of VC-volunteers using Brandeburger and Nallebuff`s (1996) notion – the “co-competition”. The results can be significant for optimizing VC-management for solving problems that require large computational resources.
</Content>
<field id="content">
The paper shows the emergence of a new form of online scientific collaboration, the collaborative networks of volunteer computing (VC) participants. And it examines what makes a collaborative VC-project successful and determines the formation of VC-community. We report on data from a statistic online study of volunteers’ activities and an online survey of VC-participants on several online forums, and discuss and analyze the emerging type of collaboration network of VC-volunteers using Brandeburger and Nallebuff`s (1996) notion – the “co-competition”. The results can be significant for optimizing VC-management for solving problems that require large computational resources.
</field>
<field id="summary">
INTRODUCTION The notion of “collaboration” is nowadays a central issue in many fields including business, science and even everyday speech. So, for example the result of the request in the Google browser gives greater than eight hundred millions links in 0,44 sec. We can say that this notion has turned into a buzzword. The usual definition of collaboration as it was done in Oxford dictionary is the action of working with some one to produce something. This notion is used in a variety of phrases to describe different phenomena. So, in this paper it will be used to glean knowledge that will deepen our understanding of an emerging type of scientific distributed computing and online communication. One of the forms of the distributed computing is the volunteer computing (VC), a specific form of online activity of volunteers (> 4,5 mln.), usually ordinary computer users which have no relation to scientific or professional computer activities, and participants do not receive compensation for their work. So, we use the notion of volunteer computing as a type of a distributed computing in which unskilled computer owners can donate their spare computer resources to perform a computation of one or more large-scale research projects. VC relies on the client-server architecture; and volunteers` desktops, notebooks, mobile phones can be connected to form equivalent of a huge and super-power supercomputer (greater than 24,032 Teraflops). The VC paradigm is divided into three main points: The idea David Gedye of using multiple integrated computers for distributed computing in scientific tasks (David Gedye, 1994); The idea of the network organization of communications (M. Castells, 1996; J. Arquila and D. Ronfeld, 2001); The idea of distributed network of computers (>1,200,000) implemented in a BOINC software/platform (David P. Anderson, 2004) As a whole VC-projects is based on two pillars: ➢	Technical task (computational): slicing a problem into thousands of tiny pieces which are then allocated (with a help of downloading project software) to a large number of individuals who volunteer their PC (notebooks, etc.) to scientific computing; ➢	Social task (participation): recruiting and retaining a large number of individuals to volunteer their computer resources, time and energy to the project, and facilitating their continuous contribution. And while the computational aspect of VC received much research attention; the participative aspect remains largely unexplored. In order to understand the phenomenon of VC we ought to look in side it. And we discover the two spaces. At first, it is computational VC-space. As it was mentioned earlier VC relies on the standard client-server architecture where clients (software programs) “talk” only to the project server and do not talk to one another directly. So, in order to participate in a VC project, a user must download the project software from the VC project website and install it on their computer. Once installed, the project software contacts the project website and is assigned a task. Upon solving the task, the result is sent back to the project server and a new task is downloaded if the user so desires. And the second, it is VC social structure. VC-participants are not a “sack” of volunteers. VC projects are by their nature decentralized and so, volunteer’s communication through online form a communal virtual place. Participants in VC projects can either work alone or join a team and work together on the same project. Also all VC-projects have their own websites that are used to host the project software, disseminate information about performing the project and host a ranking table of volunteers` activity. Also there are discussion forums and online groups in social media where they can discuss different aspects and technical problems of distributed computing. So, we can say that VC is a dispersed network of individuals and teams, project and team sites, forums. And the VC organizational framework consists of projects – typically academic-based research assignments (there are 57 projects, http://www.boincstats.com); individuals – Internet-connected low-powered computers = average person or organization (volunteers), not paid for their work, anonymous, working alone (just about 16,9%, survey May, 2018); VC teams – a long-term strategic alliance formed by volunteers, individual users of PC, in order to participate in solving large scientific computational task (just about 83,1%, survey in May, 2018 ); Internet resources – project websites with statistics in rank tables, discussion forums, etc The implementation of VC project directly depends on the number of participants, PC (other machines) and time of their work. So, the main question of the project managers – Why do volunteers decided to participates in VC-projects? Most sociological studies of the motives of volunteers' behavior were based on the representation of the community as a set of participants and as a result, the research focuses only on the motivation of individual participants [Holohan A. and Garg A., 2006; Nov O., et al., 2014; Andreev. A., 2014; Kurochkin et al., 2015]. The background of such view is based on an idea that VC participants provide something for others (processing data for scientific projects) at their own cost (time, energy, opportunity costs, use of PC resources). E.g., factors which need for self-oriented motivations. But we think that a motive of self-actualization is not enough to explain why millions of common people regular (day by day) participate in scientific computation. Our research indicates that the answer lies at the intersection of self-oriented motivation and the interactional and organizational possibilities emerging through the Internet. We suggest: online collaboration can capture people`s motivation better than only intrinsic motives. And the questions to be answered are: •	Is there any collaboration in volunteer computing? •	Does this collaboration affect a performance computing? For answering these questions some statistical approaches (mainly clustering, graph visualization, etc.) are applied on several networks. AIMS, OBJECTIVES AND METHODOLOGY It is obviously that the implementation of VC-project directly depends on the number of participants, PC (other machines) and time of their work , and so we will show that along the project managers, VC-community figures out collaboration ways to speed up and expand the capability of their computer resources. In the study as a metric (measuring the amount of computational time and resources/efforts donated by each volunteer/team), we use conditional points (crédits) that are charged by project managers in the team rating tables posted on the statistical website (www.boincstats.com). The object of the study was Russian VC-community (www.boinc.ru). The object of the study is a lot of participants in the BOINC.RU community. Out of 107,227 teams, uniting more than 4,5 million participants in the VC projects, there are 821 Russian teams. Of 57 active BOINC-projects using the BOINC platform, there are 11 Russian projects. The Data were collected in online survey, statistic analysis assembled in database; network of Boinc.ru community was visualized with a help of “Gephi” and “Force Atlas 2”. To conduct a statistical analysis of the behavior of Russian participants in the VC, we used the data obtained with the websites www.boincstatistic.com and www.boinc.ru. Content of the database were unique participant identifiers, participant names, unique project identifiers, project names, number of units (crédits) of participants/teams for the last week, month, year and all the time, participants' ownership of projects, unique team IDs, team names, team memberships. THE COLLABORATIVE NETWORK`S EFFECT As it was mentioned before participation in VC is voluntary and participants do not receive compensation for their work. So project managers/organizers in search of a mechanism to encourage participation in VC-projects use conditional points accrual mechanism; the number of these points («credits») depends on the provided capacities, the time of participation in projects, and other characteristics of the activity of volunteers and their teams. The availability of constant statistics for all projects, in addition to tracking various ratings, provokes various virtual competitions (“challenges”) between participants and teams. And many of the projects create an environment for the competitors by volume computations are done, both individually and in the team event. Thus modus operandi of VC – is spirit of competition. If volunteers are members of a team, they are simultaneously competing with the other teams on the project on the more immediate goal of racking up the most contributions and coming out on top of the table of statistics documenting contributions. This form of cooperation and competition demonstrates a new type of online scientific collaborative network. As an example, we can cite the Russian project SAT@home [Andreev A., 2013], which is actively supported by national volunteers. For the period of each of these competitions the project's productivity increased by about 7,4 teraflops with the average value in 4,3. Picture 1 The performance of the SAT@home project. The peaks correspond to the moments of the competition among teams of volunteers. In order to highlight traces of collaborative networks we will allocate links between the active volunteers in a whole Boinc.ru community. The background of looking for the active participants is based on the idea that only a few participants of virtual communities are the most effective in obtaining the most important results. For example, in the study of the OSS projects was showed that only a small subset is significant, 85-90%% of the code was written by about 15 developers [Weber S., 2004]. For determining an active group of participants in VC-projects in Boinc.ru community we have used the accrued credits, assumed that a person took part in the project if he collected at least 50,000 points in it. So it appeared – 200 volunteers, who participated in 2176 projects. To visualize and analyze links (simultaneous participation of two volunteers in a project) between volunteers we use the Gephi software and the Force Atlas 2 algorithm. Having constructed a connectivity table, where aij – the number of general projects for two volunteers we construct a graph. Picture 2 Visualization of links in group `active volunteers` (>50 000 credits, 2176 projects) Increasing the participation threshold in the project to 100 000 crédits, followed by up to one million crédits hardly changes the picture evaluation of the participation of the 200 most active volunteers. In spite increasing the threshold the form of graphs does not change. It means that almost all active users are connected with some projects. Picture 3. Distribution of active participants in the projects In spite increasing the threshold the distribution of the participants in the projects are very similar. And as can be seen from the distribution and height of the peaks, which corresponds to the number of participants in the projects, these active volunteers participate in almost the same projects. The most active users are not inclined to participate in projects "on trial", they thoroughly approach the choice of the project and actively participate in the accruals. If we look for the communication between participants of the largest team in Russian projects, (“Russia Team”) and construct a graph, we see the graph of the links (participation) of volunteers in 7 projects (orange cluster is dedicated to the collaborative community). Picture 4. Collaborative network The unification of volunteers into this group, without a provocation, shows that these are not only active, but also interested community members who follow the BOINC space, analyze the information and agree on the preference for new projects. Such behavior of the community members testifies, very likely, about their interaction with each other on third-party network resources. CONCLUSION The most active and productive participants in VC exemplify “co‐competition” (Brandenburger and Nalebuff, 1996) – collaboration within the network organization of a VC project. Our research indicates that the motivation of participation of millions of unskilled volunteers in VC-projects lies at the intersection of intrinsic motivation and the organizational possibilities emerging through the collaboration. In providing the means for channeling participants' motivations to compete and cooperate, VC-projects provide powerful insights into a new type of collaborative network VC community model – collaborative network REFERENCES David P. Anderson. Boinc: A system for public-resource computing and storage. In Proceedings of the Fifth IEEE/ACM International Workshop on Grid Computing. IEEE (2004), 4-10. Доступно: https://boinc.berkeley.edu/grid_paper_04.pdf (дата обращения 13.06.2018) Holohan, A. et al. Collaboration Online: The Example of Distributed Computing. // Journal of Computer-Mediated Communication. 2006, 10(4). Доступно: https://onlinelibrary.wiley.com/doi/full/10.1111/j.1083-6101.2005.tb00279.x (дата обращения 13.04.2018). Андреев А. Социология ГРИД. Троицкий вариант, № 166, 04 ноября 2014 года. с. 8. Доступно: http://trv-science.ru/2014/11/04/sotsiologiya-grid/ (дата обращения 13.04. 2018) Oded Nov et al. Scientists@Home: What Drives the Quantity and Quality of Online Citizen Science Participation? PLOS One, April 1 2014. Доступно: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0090375/ (дата обращения 13.04.2018) Якимец В. Н., Курочкин И. И. «Рейтинг проектов добровольных распределенных вычислений» // VII-я Международная конференция «Распределенные вычисления и грид-технологии в науке и образовании» (GRID 2016), 4-9 июля, 2016, г. Дубна, Россия. (Book of abstracts) Victor I. Tishchenko. Behavioral patterns of volunteer computing communities//BOINC-FAST 2017: International Conference BOINC: FAST 2017. Proceedings of the Third International Conference BOINC-based High Performance Computing: Fundamental Research and Development (BOINC:FAST 2017) Petrozavodsk, Russia, August 28 - September 01, 2017. Edited by Evgeny Ivahsko, Alexander Rumyantsev//CEUR-WS. – 2017. – P. 55-60/ Доступно: https://ceur-ws.org)/ (дата обращения 13.04.2018) Kurochkin I. I., Posypkin M. A., Andreev A. A., Vatutin E. I., Zaikin O. S., Putilina E. V., Manzuk M. O. The activity of Russian chapter of international desktop grid federation // Distributed Computing and Grid-Technologies in Science and Education 2016. С. 36. Тищенко В. И., Прочко А. Л. Виртуальное сообщество российских участников добровольных распределенных вычислений на платформе BOINC // Сборник презентаций и статей докладов НСКФ’2017// Доступно: http://2017.nscf.ru/prezentacii (дата обращения: 13.04.2018)
</field>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Tishchenko</FamilyName>
<Email>vtichenko@mail.ru</Email>
<Affiliation>RCF "Computer Science and Control" RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Victor</FirstName>
<FamilyName>Tishchenko</FamilyName>
<Email>vtichenko@mail.ru</Email>
<Affiliation>RCF "Computer Science and Control" RAS</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
7. Desktop grid technologies and volunteer computing
</Track>
</abstract>
<abstract>
<Id>336</Id>
<Title>
Checking foreign counterparty companies using Big Data
</Title>
<Content>
The project aims to create a database of companies and company data and an automated analytical system based on this data. The development of the system will allow credit institutions to obtain information about the links between companies, to carry out a policy of "Know your customer" - to identify the final beneficiaries, to assess risks, to identify relationships between customers. For the moment, there are some projects like OpenCorporates having global databases of companies collected from a large number of jurisdictions. But at the same they don’t cover neither all the national registries, nor other useful data sources (courts, customs, press, etc.). Also the existing services have rather sketchy abilities on searching for relations between companies, which are not always direct. The project we present is about to overcome main of these deficiencies. Number of companies worldwide is more than 150 millions. Having company information from many sources, there is no other reasonable way to process it using Big Data technologies. In the research we use such technologies along with machine learning and graph databases.
</Content>
<field id="content">
The project aims to create a database of companies and company data and an automated analytical system based on this data. The development of the system will allow credit institutions to obtain information about the links between companies, to carry out a policy of "Know your customer" - to identify the final beneficiaries, to assess risks, to identify relationships between customers. For the moment, there are some projects like OpenCorporates having global databases of companies collected from a large number of jurisdictions. But at the same they don’t cover neither all the national registries, nor other useful data sources (courts, customs, press, etc.). Also the existing services have rather sketchy abilities on searching for relations between companies, which are not always direct. The project we present is about to overcome main of these deficiencies. Number of companies worldwide is more than 150 millions. Having company information from many sources, there is no other reasonable way to process it using Big Data technologies. In the research we use such technologies along with machine learning and graph databases.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Lazar</FirstName>
<FamilyName>Badalov</FamilyName>
<Email>badalov.la@rea.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Kadochnikov</FamilyName>
<Email>kadivas@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>11.	Big data Analytics, Machine learning</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>337</Id>
<Title>DIRAC services for scientific communities</Title>
<Content>
The software framework developed by the DIRAC Project provides all the necessary components for building distributed computing systems for large-scale scientific collaborations. It supports both workload and data management tasks, therefore providing an integral solution for computing models of various scientific communities. DIRAC is used by several large High Energy Physics experiments, most notably by the LHCb Collaboration at CERN. The DIRAC services provided by several grid infrastructures are giving access to the framework to researchers from other scientific domains with applications largely differing in their scale and properties. The DIRAC development plans are strongly influenced by these new communities aiming to satisfy their specific needs. In this contribution we will present recent DIRAC evolution for enhancing services provided by grid infrastructure projects, in particular those provided by the EOSC-Hub project for the users of the European Grid Infrastructure.
</Content>
<field id="content">
The software framework developed by the DIRAC Project provides all the necessary components for building distributed computing systems for large-scale scientific collaborations. It supports both workload and data management tasks, therefore providing an integral solution for computing models of various scientific communities. DIRAC is used by several large High Energy Physics experiments, most notably by the LHCb Collaboration at CERN. The DIRAC services provided by several grid infrastructures are giving access to the framework to researchers from other scientific domains with applications largely differing in their scale and properties. The DIRAC development plans are strongly influenced by these new communities aiming to satisfy their specific needs. In this contribution we will present recent DIRAC evolution for enhancing services provided by grid infrastructure projects, in particular those provided by the EOSC-Hub project for the users of the European Grid Infrastructure.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrei</FirstName>
<FamilyName>Tsaregorodtsev</FamilyName>
<Email>atsareg@in2p3.fr</Email>
<Affiliation>CPPM-IN2P3-CNRS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrei</FirstName>
<FamilyName>Tsaregorodtsev</FamilyName>
<Email>atsareg@in2p3.fr</Email>
<Affiliation>CPPM-IN2P3-CNRS</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
3. Middleware and services for production-quality infrastructures
</Track>
</abstract>
<abstract>
<Id>338</Id>
<Title>
SALSA - Scalable Adaptive Large Structures Analysis
</Title>
<Content>
Data environments are growing exponentially and the complexity of data analysis is becoming critical issue. The goal of SALSA project is to provide tools to make connection between human and computer to understand and learn from each other. Analysis of diﬀerent parameters in N-dimensional space should be made easy and intuitive. Task distribution system has to be adaptive to the enviroment where analysis is done and has provide easy access and interactivity to the user. SALSA contains distribution network system that can constructed at level of clusters, nodes, processes and threads and will be able to build any tree strucure. User interface is implemented as web service that can connect to SALSA network and distribute tasks to workers. Web application is using latest web techonlogies like ReacJS, WebSockets to provide interactivity and dynamism. JavaScript ROOT (JSROOT) package is used as analysis interface. EOS storage support with JSROOT is included to provide prossibility to browse ﬁles and view results on web browser. Users can create, delete, start and stop tasks. The web application has several templates for diﬀerent types of user tasks that makes it possible to quickly create new task and submit it to the SALSA network.
</Content>
<field id="content">
Data environments are growing exponentially and the complexity of data analysis is becoming critical issue. The goal of SALSA project is to provide tools to make connection between human and computer to understand and learn from each other. Analysis of diﬀerent parameters in N-dimensional space should be made easy and intuitive. Task distribution system has to be adaptive to the enviroment where analysis is done and has provide easy access and interactivity to the user. SALSA contains distribution network system that can constructed at level of clusters, nodes, processes and threads and will be able to build any tree strucure. User interface is implemented as web service that can connect to SALSA network and distribute tasks to workers. Web application is using latest web techonlogies like ReacJS, WebSockets to provide interactivity and dynamism. JavaScript ROOT (JSROOT) package is used as analysis interface. EOS storage support with JSROOT is included to provide prossibility to browse ﬁles and view results on web browser. Users can create, delete, start and stop tasks. The web application has several templates for diﬀerent types of user tasks that makes it possible to quickly create new task and submit it to the SALSA network.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Branislav</FirstName>
<FamilyName>Beke</FamilyName>
<Email>beke.n35t@gmail.com</Email>
<Affiliation>SPSEKE, Kosice, Slovakia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>Butenko</FamilyName>
<Email>gohas94@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Fedor</FirstName>
<FamilyName>Matej</FamilyName>
<Email>matej.fedor.mf@gmail.com</Email>
<Affiliation>SPSEKE, Kosice, Slovakia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Martin</FirstName>
<FamilyName>Vala</FamilyName>
<Email>mvala@saske.sk</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional reports</ContributionType>
<Track>
1. Technologies, architectures, models of distributed computing systems
</Track>
<Track>
4. Scientific, industry and business applications in distributed computing systems
</Track>
</abstract>
<abstract>
<Id>339</Id>
<Title>
Development of the Geometry Database for the CBM Experiment and first adjustment for use in NICA project
</Title>
<Content>
This paper is dedicated to the current state of the Geometry Database (Geometry DB) for the CBM experiment and first result of usage the DB for NICA project. Geometry DB is an information system that supports the CBM geometry. The main aims of Geometry DB are to provide the storage the CBM geometry, the convenient tools for managing the geometry modules, assembling various versions of the CBM setup as a combination of geometry modules and additional files, providing support of various versions of the CBM setup. The development takes into account the specifics of the workflow for simulation of particles transport through the setup. The details of detectors geometry encapsulated in files of ROOT format. Such approach leads to possibility to use the results in different physical experiments. Both Graphical User Interface (GUI) and Application Programming Interface (API) are available for members of the CBM collaboration.
</Content>
<field id="content">
This paper is dedicated to the current state of the Geometry Database (Geometry DB) for the CBM experiment and first result of usage the DB for NICA project. Geometry DB is an information system that supports the CBM geometry. The main aims of Geometry DB are to provide the storage the CBM geometry, the convenient tools for managing the geometry modules, assembling various versions of the CBM setup as a combination of geometry modules and additional files, providing support of various versions of the CBM setup. The development takes into account the specifics of the workflow for simulation of particles transport through the setup. The details of detectors geometry encapsulated in files of ROOT format. Such approach leads to possibility to use the results in different physical experiments. Both Graphical User Interface (GUI) and Application Programming Interface (API) are available for members of the CBM collaboration.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>alexand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Irina</FirstName>
<FamilyName>Filozova</FamilyName>
<Email>fia@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Volker</FirstName>
<FamilyName>Friese</FamilyName>
<Email>v.friese@gsi.de</Email>
<Affiliation>GSI Darmstadt</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Ivanov</FamilyName>
<Email>ivanov@jinr.ru</Email>
<Affiliation>JINR, LIT</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Rogachevskiy</FamilyName>
<Email>rogachevsky@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Elena</FirstName>
<FamilyName>Akishina</FamilyName>
<Email>aep@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Evgeny</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>aleksand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Irina</FirstName>
<FamilyName>Filozova</FamilyName>
<Email>fia@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
10. Databases, Distributed Storage systems, Datalakes
</Track>
</abstract>
<abstract>
<Id>340</Id>
<Title>Performance measurements for the WLCG Cost Model</Title>
<Content>
High energy physics community needs metrics that allow to characterize the resource usage of the experiments workloads detailed enough so that the impact of changes in the infrastructure or the workload implementations can be quantified with a precision high enough to guide design decisions towards improved efficiencies. This model has to express the resource utilization of the workloads in terms of fundamental capabilities that computing systems provide, such as storage, memory, network, computational operations, latency, bandwidths etc. To allow sites and user communities use this model to improve also their cost efficiency, an approach to map these capabilities to local costs is highly desirable. This can’t be achieved at a global level, since the conditions at different grid sites are too different, but the model should be constructed in such a way that this mapping on a local level can be done easily, following given examples. Decisions on the evolution of workloads, workflows and infrastructures impact the quantity and quality of human resources required to build and operate the system. It is important that a cost and performance model at the system level takes these adequately into account to allow to optimize the global infrastructure cost into a constrained budget. In this report there are presented methods and results of grid sites benchmarking with typical HEP tasks. Comparative analysis and correlation studies of these results against the data from accounting portals (Rebus, etc.) are discussed.
</Content>
<field id="content">
High energy physics community needs metrics that allow to characterize the resource usage of the experiments workloads detailed enough so that the impact of changes in the infrastructure or the workload implementations can be quantified with a precision high enough to guide design decisions towards improved efficiencies. This model has to express the resource utilization of the workloads in terms of fundamental capabilities that computing systems provide, such as storage, memory, network, computational operations, latency, bandwidths etc. To allow sites and user communities use this model to improve also their cost efficiency, an approach to map these capabilities to local costs is highly desirable. This can’t be achieved at a global level, since the conditions at different grid sites are too different, but the model should be constructed in such a way that this mapping on a local level can be done easily, following given examples. Decisions on the evolution of workloads, workflows and infrastructures impact the quantity and quality of human resources required to build and operate the system. It is important that a cost and performance model at the system level takes these adequately into account to allow to optimize the global infrastructure cost into a constrained budget. In this report there are presented methods and results of grid sites benchmarking with typical HEP tasks. Comparative analysis and correlation studies of these results against the data from accounting portals (Rebus, etc.) are discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Victoria</FirstName>
<FamilyName>Matskovskaya</FamilyName>
<Email>viktoriyams@mail.ru</Email>
<Affiliation/>
</PrimaryAuthor>
<Speaker>
<FirstName>Victoria</FirstName>
<FamilyName>Matskovskaya</FamilyName>
<Email>viktoriyams@mail.ru</Email>
<Affiliation/>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>341</Id>
<Title>
The concept of proactive protection in a distributed computing system
</Title>
<Content>
The paper considers modern problems of information security, the main stages of the implementation of attacks. The stages of implementing the concept of proactive protection, a technique for describing possible attacks are described, an example of a prototype of a proactive defense system is described. Keywords: information security, proactive protection, information systems.
</Content>
<field id="content">
The paper considers modern problems of information security, the main stages of the implementation of attacks. The stages of implementing the concept of proactive protection, a technique for describing possible attacks are described, an example of a prototype of a proactive defense system is described. Keywords: information security, proactive protection, information systems.
</field>
<field id="summary"/>
<PrimaryAuthor>
    <FirstName>Anatoly</FirstName>
    <FamilyName>Minzov</FamilyName>
    <Email>minzovas@mpei.ru</Email>
    <Affiliation>MPEI</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
    <FirstName>Pavel</FirstName>
    <FamilyName>Osipov</FamilyName>
    <Email>osipov.pavel92@gmail.com</Email>
    <Affiliation>State University "Dubna"</Affiliation>
</PrimaryAuthor>
<Speaker>
    <FirstName>Pavel</FirstName>
    <FamilyName>Osipov</FamilyName>
    <Email>osipov.pavel92@gmail.com</Email>
    <Affiliation>State University "Dubna"</Affiliation>
</Speaker>
    <ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>342</Id>
<Title>
Orthogonality-based classification of diagonal Latin squares of order 10
</Title>
<Content>
The search for pairs of orthogonal diagonal Latin squares (ODLS) is a hard-combinatorial problem [1]. According to the Euler-Parker approach, a set of diagonal transversals is constructed for a given DLS of order N. If a subset of N non-overlapping transversals is found, then an orthogonal mate for the DLS can be easily constructed. According to some estimations, only 1 DLS of order 10 out of 32 millions has an orthogonal mate. Authors of the volunteer computing pro-ject Gerasim@home and SAT@home maintain the collection of pairs of ODLS of order 10. It contains more than 580 000 canonical forms (isotopy classes) of DLS of order 10 as for June 2018. DLSs from the collection can be classified by the number of their orthogo-nal mates. According to this classification, about 550 000 of DLSs are bachelor – i.e. each of them has exactly one orthogonal mate. About 7 500 of DLSs are line-2 – i.e. that each of them has exactly two orthogonal mates. There are also 63 line-3, 283 fours, 2 fives, 9 sixes, 1 sevens, 7 eights and 1 ten (see Fig. 1). This classification can be expanded. Table 1 contains examples of DLSs of order 10 that are part of structures depicted in Figure 1. These DLSs were constructed during several computational experiments: random search for DLSs with consequent attempt to construct their orthogonal mates; comprehensive search for DLSs that are symmetric according to some plane; comprehensive search for general symmetric DLSs; random search for partially symmetric DLSs. The found combinatorial structures are new and were not published before. Due to their simplicity they allow a trivial classification based on a vector of de-grees of vertices which is sorted in ascending order. In fact, in this case a degree of a vertex is the number of ODLS for the chosen DLS. The research was partially supported by Russian Foundation for Basic Re-search (grants 16-07-00155-a, 17-07-00317-a, 18-07-00628-a, 18-37-00094-mol-a) and by Council for Grants of the President of the Russian Federation (stipend SP-1829.2016.5). Authors thank citerra [Russia Team] from the internet portal BOINC.ru for his help in the development and implementation of some algorithms. Also authors thank all the volunteers of SAT@home and Gerasim@home for their participation. Bibliography 1. Colbourn C.J., Dinitz J.H. Handbook of Combinatorial Designs. Second Edi-tion. ChapmanandHall, 2006. 984 p.
</Content>
<field id="content">
The search for pairs of orthogonal diagonal Latin squares (ODLS) is a hard-combinatorial problem [1]. According to the Euler-Parker approach, a set of diagonal transversals is constructed for a given DLS of order N. If a subset of N non-overlapping transversals is found, then an orthogonal mate for the DLS can be easily constructed. According to some estimations, only 1 DLS of order 10 out of 32 millions has an orthogonal mate. Authors of the volunteer computing pro-ject Gerasim@home and SAT@home maintain the collection of pairs of ODLS of order 10. It contains more than 580 000 canonical forms (isotopy classes) of DLS of order 10 as for June 2018. DLSs from the collection can be classified by the number of their orthogo-nal mates. According to this classification, about 550 000 of DLSs are bachelor – i.e. each of them has exactly one orthogonal mate. About 7 500 of DLSs are line-2 – i.e. that each of them has exactly two orthogonal mates. There are also 63 line-3, 283 fours, 2 fives, 9 sixes, 1 sevens, 7 eights and 1 ten (see Fig. 1). This classification can be expanded. Table 1 contains examples of DLSs of order 10 that are part of structures depicted in Figure 1. These DLSs were constructed during several computational experiments: random search for DLSs with consequent attempt to construct their orthogonal mates; comprehensive search for DLSs that are symmetric according to some plane; comprehensive search for general symmetric DLSs; random search for partially symmetric DLSs. The found combinatorial structures are new and were not published before. Due to their simplicity they allow a trivial classification based on a vector of de-grees of vertices which is sorted in ascending order. In fact, in this case a degree of a vertex is the number of ODLS for the chosen DLS. The research was partially supported by Russian Foundation for Basic Re-search (grants 16-07-00155-a, 17-07-00317-a, 18-07-00628-a, 18-37-00094-mol-a) and by Council for Grants of the President of the Russian Federation (stipend SP-1829.2016.5). Authors thank citerra [Russia Team] from the internet portal BOINC.ru for his help in the development and implementation of some algorithms. Also authors thank all the volunteers of SAT@home and Gerasim@home for their participation. Bibliography 1. Colbourn C.J., Dinitz J.H. Handbook of Combinatorial Designs. Second Edi-tion. ChapmanandHall, 2006. 984 p.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Eduard</FirstName>
<FamilyName>Vatutin</FamilyName>
<Email>evatutin@rambler.ru</Email>
<Affiliation>Southwest State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Zaikin</FamilyName>
<Email>zaikin.icc@gmail.com</Email>
<Affiliation>
Institute for System Dynamics and Control Theory of Siberian Branch of Russian Academy of Sciences
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Natalia</FirstName>
<FamilyName>Nikitina</FamilyName>
<Email>nikitina@krc.karelia.ru</Email>
<Affiliation>
Institute of Applied Mathematical Research, Karelian Research Center RAS
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vitaly</FirstName>
<FamilyName>Titov</FamilyName>
<Email>titov@grid2018.jinr.ru</Email>
<Affiliation>Southwest State University, Kursk, Russia</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Stepan</FirstName>
<FamilyName>Kochemazov</FamilyName>
<Email>kochemazov@grid2018.jinr.ru</Email>
<Affiliation>
Matrosov Institute for System Dynamics and Control Theory SB RAS, Irkutsk, Russia
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Maxim</FirstName>
<FamilyName>Manzyuk</FamilyName>
<Email>manzyuk@grid2018.jinr.ru</Email>
<Affiliation>BOINC.ru, Moscow, Russia</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Eduard</FirstName>
<FamilyName>Vatutin</FamilyName>
<Email>evatutin@rambler.ru</Email>
<Affiliation>Southwest State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
</AbstractBook>

